{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPYi9Diz2dlcx2mdhmTMxEO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b680e161abf443de8538f72967097d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_edec48abf3be40cc89f7f43026dcf1b5",
              "IPY_MODEL_558785316867421bbf03c31ef36789a3",
              "IPY_MODEL_608eb08f0d3340d08ef0a6457c388424"
            ],
            "layout": "IPY_MODEL_1f149fc3f992410493f44e90d71a40da"
          }
        },
        "edec48abf3be40cc89f7f43026dcf1b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42c9e059f3b64a528fcd6e15e463e46d",
            "placeholder": "​",
            "style": "IPY_MODEL_9a572bbc1ed34d1892199e3f609afb84",
            "value": "100%"
          }
        },
        "558785316867421bbf03c31ef36789a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2e3562531ea4b6c995e5c9d597bef12",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20b10f32281845869252426b96c50c31",
            "value": 3
          }
        },
        "608eb08f0d3340d08ef0a6457c388424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3ecaea3bb304184b79dfec67518fe18",
            "placeholder": "​",
            "style": "IPY_MODEL_a08777c1f21e40cdbcf6bcedf1e274e0",
            "value": " 3/3 [01:00&lt;00:00, 19.44s/it]"
          }
        },
        "1f149fc3f992410493f44e90d71a40da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42c9e059f3b64a528fcd6e15e463e46d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a572bbc1ed34d1892199e3f609afb84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2e3562531ea4b6c995e5c9d597bef12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20b10f32281845869252426b96c50c31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3ecaea3bb304184b79dfec67518fe18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a08777c1f21e40cdbcf6bcedf1e274e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14f2acba9ef546b9bd37f0479186a89f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61aa11b39e9841c0b40f1df018a5e305",
              "IPY_MODEL_31648da1b3e4432db77ccc78e1076d41",
              "IPY_MODEL_b057a745a72b4774ac1e1fd564bf430d"
            ],
            "layout": "IPY_MODEL_8c12a766ed0b4611a1248106fcb5ab38"
          }
        },
        "61aa11b39e9841c0b40f1df018a5e305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e35fc05828f4e738fc552bd7905aec0",
            "placeholder": "​",
            "style": "IPY_MODEL_7afff0fb02c14fca9d981d6847e3ae64",
            "value": " 67%"
          }
        },
        "31648da1b3e4432db77ccc78e1076d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b71b9a60d9094699855dd44174d91c1f",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0906df6b57374138ac4956669c369b5d",
            "value": 2
          }
        },
        "b057a745a72b4774ac1e1fd564bf430d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daaa57208d274f9ca417b591c70a85bc",
            "placeholder": "​",
            "style": "IPY_MODEL_ec4f3251f2794056b1ece84fe2be03ce",
            "value": " 2/3 [00:26&lt;00:13, 13.24s/it]"
          }
        },
        "8c12a766ed0b4611a1248106fcb5ab38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e35fc05828f4e738fc552bd7905aec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7afff0fb02c14fca9d981d6847e3ae64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b71b9a60d9094699855dd44174d91c1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0906df6b57374138ac4956669c369b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "daaa57208d274f9ca417b591c70a85bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec4f3251f2794056b1ece84fe2be03ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vidorc/PyTorch-for-Deep-Learning-A-Comprehensive-Study-Guide/blob/main/04_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "XnEIXr7VwR7G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huNduSx1FNya",
        "outputId": "24fa1dda-ad81-43ff-c208-056a4110ed59"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n",
            "0.21.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None\n",
        ")"
      ],
      "metadata": {
        "id": "7A9J8xeTFlJJ"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data) , len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5FnVBQcGvUG",
        "outputId": "7d3f1d0c-b417-4704-b891-e295bc019e89"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image , label = train_data[0]\n",
        "print(image.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3glmcdZ4G9pz",
        "outputId": "8b8d7fd9-61ae-4848-853c-14dfd4892077"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_name = train_data.classes\n",
        "print(class_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZmVITwNHBEJ",
        "outputId": "5779fa9a-b86f-4154-98e9-b2086bf62ee4"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_index = train_data.class_to_idx\n",
        "print(class_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvGrv8XAHMl0",
        "outputId": "60d00a18-c079-40bd-f9bc-0d3d4c8f45a0"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'T-shirt/top': 0, 'Trouser': 1, 'Pullover': 2, 'Dress': 3, 'Coat': 4, 'Sandal': 5, 'Shirt': 6, 'Sneaker': 7, 'Bag': 8, 'Ankle boot': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzqH0xDtHQcc",
        "outputId": "2de5722e-be09-45a8-8586-cc90dc5246a3"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9, 0, 0,  ..., 3, 0, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"image shape : {image.shape}\")\n",
        "print(f\"label : {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1i29wPoHTdI",
        "outputId": "ad0ad332-1996-479f-8b47-009b89437846"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image shape : torch.Size([1, 28, 28])\n",
            "label : 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "image , label = train_data[0]\n",
        "print(f\"image shape : {image.shape}\")\n",
        "print(f\"label : {label}\")\n",
        "plt.imshow(image.squeeze(),cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "GFAsGTgNHWXg",
        "outputId": "06b13e71-dadb-4eab-91d7-ab721fb6a3fa"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image shape : torch.Size([1, 28, 28])\n",
            "label : 9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAINpJREFUeJzt3Xts1fX9x/HXaaGHQtvDSulNylUQIxc3hFpRfioV6BIjQiZe/oDNS2TFDJnTsKjoXFLHks24MUy2BWYi3hKBaJQFi5Q5Lg6EIJkjgChgabnMnlN6p/3+/iB2Vq6fj+f03ZbnI/km9Jzvi+/HL9/25bfn9N1QEASBAADoZEnWCwAAXJ4oIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJjoZb2Ab2tra1NlZaXS09MVCoWslwMAcBQEgWpra5Wfn6+kpPPf53S5AqqsrFRBQYH1MgAA39Hhw4c1aNCg8z7f5b4Fl56ebr0EAEAcXOzrecIKaNmyZRo6dKj69OmjwsJCffTRR5eU49tuANAzXOzreUIK6PXXX9eiRYu0ZMkSffzxxxo/frymT5+uY8eOJeJwAIDuKEiASZMmBaWlpe0ft7a2Bvn5+UFZWdlFs9FoNJDExsbGxtbNt2g0esGv93G/A2pubtaOHTtUXFzc/lhSUpKKi4u1ZcuWs/ZvampSLBbrsAEAer64F9CJEyfU2tqqnJycDo/n5OSoqqrqrP3LysoUiUTaN94BBwCXB/N3wS1evFjRaLR9O3z4sPWSAACdIO4/B5SVlaXk5GRVV1d3eLy6ulq5ubln7R8OhxUOh+O9DABAFxf3O6CUlBRNmDBB5eXl7Y+1tbWpvLxcRUVF8T4cAKCbSsgkhEWLFmnu3Lm67rrrNGnSJL3wwguqq6vTj3/840QcDgDQDSWkgObMmaPjx4/r6aefVlVVla699lqtW7furDcmAAAuX6EgCALrRXxTLBZTJBKxXgYA4DuKRqPKyMg47/Pm74IDAFyeKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgIle1gsAupJQKOScCYIgASs5W3p6unPmxhtv9DrWe++955Vz5XO+k5OTnTOnT592znR1PufOV6Kuce6AAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmGAYKfANSUnu/0/W2trqnLnyyiudMw888IBzpqGhwTkjSXV1dc6ZxsZG58xHH33knOnMwaI+Az99riGf43TmeXAdABsEgdra2i66H3dAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATDCMFPgG16GLkt8w0ltvvdU5U1xc7Jw5cuSIc0aSwuGwc6Zv377Omdtuu80585e//MU5U11d7ZyRzgzVdOVzPfhIS0vzyl3KkNBvq6+v9zrWxXAHBAAwQQEBAEzEvYCeeeYZhUKhDtvo0aPjfRgAQDeXkNeArrnmGr3//vv/O0gvXmoCAHSUkGbo1auXcnNzE/FXAwB6iIS8BrRv3z7l5+dr+PDhuu+++3To0KHz7tvU1KRYLNZhAwD0fHEvoMLCQq1cuVLr1q3T8uXLdfDgQd10002qra095/5lZWWKRCLtW0FBQbyXBADoguJeQCUlJfrRj36kcePGafr06Xr33XdVU1OjN95445z7L168WNFotH07fPhwvJcEAOiCEv7ugP79+2vUqFHav3//OZ8Ph8NeP/QGAOjeEv5zQKdOndKBAweUl5eX6EMBALqRuBfQY489poqKCn3++efavHmz7rzzTiUnJ+uee+6J96EAAN1Y3L8Fd+TIEd1zzz06efKkBg4cqBtvvFFbt27VwIED430oAEA3FvcCeu211+L9VwKdprm5uVOOM3HiROfM0KFDnTM+w1UlKSnJ/Zsjf//7350z3//+950zS5cudc5s377dOSNJn3zyiXPm008/dc5MmjTJOeNzDUnS5s2bnTNbtmxx2j8Igkv6kRpmwQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADCR8F9IB1gIhUJeuSAInDO33Xabc+a6665zzpzv19pfSL9+/ZwzkjRq1KhOyfzrX/9yzpzvl1teSFpamnNGkoqKipwzs2bNcs60tLQ4Z3zOnSQ98MADzpmmpian/U+fPq1//OMfF92POyAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgIlQ4DP+N4FisZgikYj1MpAgvlOqO4vPp8PWrVudM0OHDnXO+PA936dPn3bONDc3ex3LVWNjo3Omra3N61gff/yxc8ZnWrfP+Z4xY4ZzRpKGDx/unLniiiu8jhWNRpWRkXHe57kDAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYKKX9QJweelis2/j4quvvnLO5OXlOWcaGhqcM+Fw2DkjSb16uX9pSEtLc874DBZNTU11zvgOI73pppucMzfccINzJinJ/V4gOzvbOSNJ69at88olAndAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATDCMFPiO+vbt65zxGT7pk6mvr3fOSFI0GnXOnDx50jkzdOhQ54zPQNtQKOSckfzOuc/10Nra6pzxHbBaUFDglUsE7oAAACYoIACACecC2rRpk26//Xbl5+crFAppzZo1HZ4PgkBPP/208vLylJqaquLiYu3bty9e6wUA9BDOBVRXV6fx48dr2bJl53x+6dKlevHFF/XSSy9p27Zt6tevn6ZPn+71i6cAAD2X85sQSkpKVFJScs7ngiDQCy+8oCeffFJ33HGHJOnll19WTk6O1qxZo7vvvvu7rRYA0GPE9TWggwcPqqqqSsXFxe2PRSIRFRYWasuWLefMNDU1KRaLddgAAD1fXAuoqqpKkpSTk9Ph8ZycnPbnvq2srEyRSKR960pvEQQAJI75u+AWL16saDTavh0+fNh6SQCAThDXAsrNzZUkVVdXd3i8urq6/blvC4fDysjI6LABAHq+uBbQsGHDlJubq/Ly8vbHYrGYtm3bpqKiongeCgDQzTm/C+7UqVPav39/+8cHDx7Url27lJmZqcGDB2vhwoX69a9/rZEjR2rYsGF66qmnlJ+fr5kzZ8Zz3QCAbs65gLZv365bbrml/eNFixZJkubOnauVK1fq8ccfV11dnR566CHV1NToxhtv1Lp169SnT5/4rRoA0O2FAp/JfgkUi8UUiUSsl4EE8RkK6TMQ0me4oySlpaU5Z3bu3Omc8TkPDQ0NzplwOOyckaTKykrnzLdf+70UN9xwg3PGZ+ipz4BQSUpJSXHO1NbWOmd8vub5vmHL5xq///77nfZvbW3Vzp07FY1GL/i6vvm74AAAlycKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAnnX8cAfBc+w9eTk5OdM77TsOfMmeOcOd9v+72Q48ePO2dSU1OdM21tbc4ZSerXr59zpqCgwDnT3NzsnPGZ8N3S0uKckaRevdy/RPr8Ow0YMMA5s2zZMueMJF177bXOGZ/zcCm4AwIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCYaToVD5DDX0GVvras2ePc6apqck507t3b+dMZw5lzc7Ods40NjY6Z06ePOmc8Tl3ffr0cc5IfkNZv/rqK+fMkSNHnDP33nuvc0aSfvvb3zpntm7d6nWsi+EOCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgInLehhpKBTyyvkMhUxKcu96n/W1tLQ4Z9ra2pwzvk6fPt1px/Lx7rvvOmfq6uqcMw0NDc6ZlJQU50wQBM4ZSTp+/LhzxufzwmdIqM817quzPp98zt24ceOcM5IUjUa9conAHRAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATPWYYqc8wv9bWVq9jdfWBml3ZlClTnDOzZ892zkyePNk5I0n19fXOmZMnTzpnfAaL9url/unqe437nAefz8FwOOyc8Rlg6juU1ec8+PC5Hk6dOuV1rFmzZjln3n77ba9jXQx3QAAAExQQAMCEcwFt2rRJt99+u/Lz8xUKhbRmzZoOz8+bN0+hUKjDNmPGjHitFwDQQzgXUF1dncaPH69ly5add58ZM2bo6NGj7durr776nRYJAOh5nF/VLCkpUUlJyQX3CYfDys3N9V4UAKDnS8hrQBs3blR2drauuuoqzZ8//4LvEmpqalIsFuuwAQB6vrgX0IwZM/Tyyy+rvLxcv/nNb1RRUaGSkpLzvh20rKxMkUikfSsoKIj3kgAAXVDcfw7o7rvvbv/z2LFjNW7cOI0YMUIbN27U1KlTz9p/8eLFWrRoUfvHsViMEgKAy0DC34Y9fPhwZWVlaf/+/ed8PhwOKyMjo8MGAOj5El5AR44c0cmTJ5WXl5foQwEAuhHnb8GdOnWqw93MwYMHtWvXLmVmZiozM1PPPvusZs+erdzcXB04cECPP/64rrzySk2fPj2uCwcAdG/OBbR9+3bdcsst7R9//frN3LlztXz5cu3evVt/+9vfVFNTo/z8fE2bNk3PPfec18wnAEDPFQp8p/QlSCwWUyQSsV5G3GVmZjpn8vPznTMjR47slONIfkMNR40a5ZxpampyziQl+X13uaWlxTmTmprqnKmsrHTO9O7d2znjM+RSkgYMGOCcaW5uds707dvXObN582bnTFpamnNG8hue29bW5pyJRqPOGZ/rQZKqq6udM1dffbXXsaLR6AVf12cWHADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADARNx/JbeV66+/3jnz3HPPeR1r4MCBzpn+/fs7Z1pbW50zycnJzpmamhrnjCSdPn3aOVNbW+uc8ZmyHAqFnDOS1NDQ4Jzxmc581113OWe2b9/unElPT3fOSH4TyIcOHep1LFdjx451zvieh8OHDztn6uvrnTM+E9V9J3wPGTLEK5cI3AEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAw0WWHkSYlJTkNlHzxxRedj5GXl+eckfyGhPpkfIYa+khJSfHK+fw3+Qz79BGJRLxyPoMan3/+eeeMz3mYP3++c6aystI5I0mNjY3OmfLycufMZ5995pwZOXKkc2bAgAHOGclvEG7v3r2dM0lJ7vcCLS0tzhlJOn78uFcuEbgDAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYCIUBEFgvYhvisViikQiuu+++5yGZPoMhDxw4IBzRpLS0tI6JRMOh50zPnyGJ0p+Az8PHz7snPEZqDlw4EDnjOQ3FDI3N9c5M3PmTOdMnz59nDNDhw51zkh+1+uECRM6JePzb+QzVNT3WL7DfV25DGv+Jp/P9+uvv95p/7a2Nn355ZeKRqPKyMg4737cAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADDRy3oB53P8+HGnoXk+Qy7T09OdM5LU1NTknPFZn89ASJ9BiBcaFngh//3vf50zX3zxhXPG5zw0NDQ4ZySpsbHROXP69GnnzOrVq50zn3zyiXPGdxhpZmamc8Zn4GdNTY1zpqWlxTnj828knRmq6cpn2KfPcXyHkfp8jRg1apTT/qdPn9aXX3550f24AwIAmKCAAAAmnAqorKxMEydOVHp6urKzszVz5kzt3bu3wz6NjY0qLS3VgAEDlJaWptmzZ6u6ujquiwYAdH9OBVRRUaHS0lJt3bpV69evV0tLi6ZNm6a6urr2fR599FG9/fbbevPNN1VRUaHKykrNmjUr7gsHAHRvTm9CWLduXYePV65cqezsbO3YsUNTpkxRNBrVX//6V61atUq33nqrJGnFihW6+uqrtXXrVuffqgcA6Lm+02tA0WhU0v/eMbNjxw61tLSouLi4fZ/Ro0dr8ODB2rJlyzn/jqamJsVisQ4bAKDn8y6gtrY2LVy4UJMnT9aYMWMkSVVVVUpJSVH//v077JuTk6Oqqqpz/j1lZWWKRCLtW0FBge+SAADdiHcBlZaWas+ePXrttde+0wIWL16saDTavvn8vAwAoPvx+kHUBQsW6J133tGmTZs0aNCg9sdzc3PV3NysmpqaDndB1dXVys3NPeffFQ6HFQ6HfZYBAOjGnO6AgiDQggULtHr1am3YsEHDhg3r8PyECRPUu3dvlZeXtz+2d+9eHTp0SEVFRfFZMQCgR3C6AyotLdWqVau0du1apaent7+uE4lElJqaqkgkovvvv1+LFi1SZmamMjIy9Mgjj6ioqIh3wAEAOnAqoOXLl0uSbr755g6Pr1ixQvPmzZMk/f73v1dSUpJmz56tpqYmTZ8+XX/605/islgAQM8RCoIgsF7EN8ViMUUiEY0dO1bJycmXnPvzn//sfKwTJ044ZySpX79+zpkBAwY4Z3wGNZ46dco54zM8UZJ69XJ/CdFn6GLfvn2dMz4DTCW/c5GU5P5eHp9Pu2+/u/RSfPOHxF34DHP96quvnDM+r//6fN76DDCV/IaY+hwrNTXVOXO+19UvxmeI6SuvvOK0f1NTk/74xz8qGo1ecNgxs+AAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACa8fiNqZ/jkk0+c9n/rrbecj/GTn/zEOSNJlZWVzpnPPvvMOdPY2Oic8ZkC7TsN22eCb0pKinPGZSr615qampwzktTa2uqc8ZlsXV9f75w5evSoc8Z32L3PefCZjt5Z13hzc7NzRvKbSO+T8Zmg7TOpW9JZv0j0UlRXVzvtf6nnmzsgAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJkKB77TCBInFYopEIp1yrJKSEq/cY4895pzJzs52zpw4ccI54zMI0WfwpOQ3JNRnGKnPkEuftUlSKBRyzvh8CvkMgPXJ+Jxv32P5nDsfPsdxHab5Xfic87a2NudMbm6uc0aSdu/e7Zy56667vI4VjUaVkZFx3ue5AwIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCiyw4jDYVCTkMHfYb5daZbbrnFOVNWVuac8Rl66jv8NSnJ/f9ffIaE+gwj9R2w6uPYsWPOGZ9Puy+//NI54/t5cerUKeeM7wBYVz7nrqWlxetY9fX1zhmfz4v169c7Zz799FPnjCRt3rzZK+eDYaQAgC6JAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACAiS47jBSdZ/To0V65rKws50xNTY1zZtCgQc6Zzz//3Dkj+Q2tPHDggNexgJ6OYaQAgC6JAgIAmHAqoLKyMk2cOFHp6enKzs7WzJkztXfv3g773Hzzze2/y+fr7eGHH47rogEA3Z9TAVVUVKi0tFRbt27V+vXr1dLSomnTpqmurq7Dfg8++KCOHj3avi1dujSuiwYAdH9Ov2py3bp1HT5euXKlsrOztWPHDk2ZMqX98b59+yo3Nzc+KwQA9Ejf6TWgaDQqScrMzOzw+CuvvKKsrCyNGTNGixcvvuCvtW1qalIsFuuwAQB6Pqc7oG9qa2vTwoULNXnyZI0ZM6b98XvvvVdDhgxRfn6+du/erSeeeEJ79+7VW2+9dc6/p6ysTM8++6zvMgAA3ZT3zwHNnz9f7733nj788MML/pzGhg0bNHXqVO3fv18jRow46/mmpiY1NTW1fxyLxVRQUOCzJHji54D+h58DAuLnYj8H5HUHtGDBAr3zzjvatGnTRb84FBYWStJ5CygcDiscDvssAwDQjTkVUBAEeuSRR7R69Wpt3LhRw4YNu2hm165dkqS8vDyvBQIAeianAiotLdWqVau0du1apaenq6qqSpIUiUSUmpqqAwcOaNWqVfrhD3+oAQMGaPfu3Xr00Uc1ZcoUjRs3LiH/AQCA7smpgJYvXy7pzA+bftOKFSs0b948paSk6P3339cLL7yguro6FRQUaPbs2XryySfjtmAAQM/g/C24CykoKFBFRcV3WhAA4PLANGwAQEIwDRsA0CVRQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAw0eUKKAgC6yUAAOLgYl/Pu1wB1dbWWi8BABAHF/t6Hgq62C1HW1ubKisrlZ6erlAo1OG5WCymgoICHT58WBkZGUYrtMd5OIPzcAbn4QzOwxld4TwEQaDa2lrl5+crKen89zm9OnFNlyQpKUmDBg264D4ZGRmX9QX2Nc7DGZyHMzgPZ3AezrA+D5FI5KL7dLlvwQEALg8UEADARLcqoHA4rCVLligcDlsvxRTn4QzOwxmchzM4D2d0p/PQ5d6EAAC4PHSrOyAAQM9BAQEATFBAAAATFBAAwES3KaBly5Zp6NCh6tOnjwoLC/XRRx9ZL6nTPfPMMwqFQh220aNHWy8r4TZt2qTbb79d+fn5CoVCWrNmTYfngyDQ008/rby8PKWmpqq4uFj79u2zWWwCXew8zJs376zrY8aMGTaLTZCysjJNnDhR6enpys7O1syZM7V3794O+zQ2Nqq0tFQDBgxQWlqaZs+ererqaqMVJ8alnIebb775rOvh4YcfNlrxuXWLAnr99de1aNEiLVmyRB9//LHGjx+v6dOn69ixY9ZL63TXXHONjh492r59+OGH1ktKuLq6Oo0fP17Lli075/NLly7Viy++qJdeeknbtm1Tv379NH36dDU2NnbyShPrYudBkmbMmNHh+nj11Vc7cYWJV1FRodLSUm3dulXr169XS0uLpk2bprq6uvZ9Hn30Ub399tt68803VVFRocrKSs2aNctw1fF3KedBkh588MEO18PSpUuNVnweQTcwadKkoLS0tP3j1tbWID8/PygrKzNcVedbsmRJMH78eOtlmJIUrF69uv3jtra2IDc3N/jtb3/b/lhNTU0QDoeDV1991WCFnePb5yEIgmDu3LnBHXfcYbIeK8eOHQskBRUVFUEQnPm37927d/Dmm2+27/Ppp58GkoItW7ZYLTPhvn0egiAI/u///i/42c9+ZreoS9Dl74Cam5u1Y8cOFRcXtz+WlJSk4uJibdmyxXBlNvbt26f8/HwNHz5c9913nw4dOmS9JFMHDx5UVVVVh+sjEomosLDwsrw+Nm7cqOzsbF111VWaP3++Tp48ab2khIpGo5KkzMxMSdKOHTvU0tLS4XoYPXq0Bg8e3KOvh2+fh6+98sorysrK0pgxY7R48WLV19dbLO+8utww0m87ceKEWltblZOT0+HxnJwc/ec//zFalY3CwkKtXLlSV111lY4ePapnn31WN910k/bs2aP09HTr5ZmoqqqSpHNeH18/d7mYMWOGZs2apWHDhunAgQP65S9/qZKSEm3ZskXJycnWy4u7trY2LVy4UJMnT9aYMWMknbkeUlJS1L9//w779uTr4VznQZLuvfdeDRkyRPn5+dq9e7eeeOIJ7d27V2+99Zbhajvq8gWE/ykpKWn/87hx41RYWKghQ4bojTfe0P3332+4MnQFd999d/ufx44dq3HjxmnEiBHauHGjpk6dariyxCgtLdWePXsui9dBL+R85+Ghhx5q//PYsWOVl5enqVOn6sCBAxoxYkRnL/Ocuvy34LKyspScnHzWu1iqq6uVm5trtKquoX///ho1apT2799vvRQzX18DXB9nGz58uLKysnrk9bFgwQK98847+uCDDzr8+pbc3Fw1Nzerpqamw/499Xo433k4l8LCQknqUtdDly+glJQUTZgwQeXl5e2PtbW1qby8XEVFRYYrs3fq1CkdOHBAeXl51ksxM2zYMOXm5na4PmKxmLZt23bZXx9HjhzRyZMne9T1EQSBFixYoNWrV2vDhg0aNmxYh+cnTJig3r17d7ge9u7dq0OHDvWo6+Fi5+Fcdu3aJUld63qwfhfEpXjttdeCcDgcrFy5Mvj3v/8dPPTQQ0H//v2Dqqoq66V1qp///OfBxo0bg4MHDwb//Oc/g+Li4iArKys4duyY9dISqra2Nti5c2ewc+fOQFLwu9/9Lti5c2fwxRdfBEEQBM8//3zQv3//YO3atcHu3buDO+64Ixg2bFjQ0NBgvPL4utB5qK2tDR577LFgy5YtwcGDB4P3338/+MEPfhCMHDkyaGxstF563MyfPz+IRCLBxo0bg6NHj7Zv9fX17fs8/PDDweDBg4MNGzYE27dvD4qKioKioiLDVcffxc7D/v37g1/96lfB9u3bg4MHDwZr164Nhg8fHkyZMsV45R11iwIKgiD4wx/+EAwePDhISUkJJk2aFGzdutV6SZ1uzpw5QV5eXpCSkhJcccUVwZw5c4L9+/dbLyvhPvjgg0DSWdvcuXODIDjzVuynnnoqyMnJCcLhcDB16tRg7969totOgAudh/r6+mDatGnBwIEDg969ewdDhgwJHnzwwR73P2nn+u+XFKxYsaJ9n4aGhuCnP/1p8L3vfS/o27dvcOeddwZHjx61W3QCXOw8HDp0KJgyZUqQmZkZhMPh4Morrwx+8YtfBNFo1Hbh38KvYwAAmOjyrwEBAHomCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJv4fq+TKSY6M9H8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9,9))\n",
        "rows , cols = 4,4\n",
        "for i in range(1,rows*cols+1):\n",
        "    random_idx = torch.randint(0,len(train_data),(1,)).item()\n",
        "    print(random_idx)\n",
        "    image , label = train_data[random_idx]\n",
        "    fig.add_subplot(rows,cols,i)\n",
        "    plt.imshow(image.squeeze(),cmap='gray')\n",
        "    plt.title(class_name[label])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WhgoZqf6IguE",
        "outputId": "484ba67a-9db9-4a04-8c99-5ce14efe6b89"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37542\n",
            "46067\n",
            "46876\n",
            "46414\n",
            "10026\n",
            "27335\n",
            "38620\n",
            "11924\n",
            "14950\n",
            "57113\n",
            "31378\n",
            "29014\n",
            "47210\n",
            "18954\n",
            "18231\n",
            "47572\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x900 with 16 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAucAAAL3CAYAAAAp0etTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAA2ERJREFUeJzs3Xl4VFW2NvC3EpJKyBwgCYFMzPPQzMioDCKCKDi3AiJOgW7E27S0A62tItgiCih6r4IiiNotqLSgGBCUQQQFAQUBw0wSAmQghASS/f3Bl2oqe204RRJyUnl/z1OPZmWfOqeq1jm1U9Ra26GUUiAiIiIiokrnU9kHQEREREREF3ByTkRERERkE5ycExERERHZBCfnREREREQ2wck5EREREZFNcHJORERERGQTnJwTEREREdkEJ+dERERERDbByTkRERERkU1wcn4VjRo1CsHBwZcd16dPH/Tp06fc9tunTx+0atWq3O6PqKwcDgfGjRt32XHz58+Hw+HA/v37K/6giIi8HOchVQMn55fx+uuvw+FwoEuXLpV9KFXSCy+8gKVLl1b2YdBVtH37dowYMQIJCQkICAhAvXr10L9/f8yaNavC9818o6uh5I/Gi29RUVHo27cvli9fXtmHR16G85CyqYrvC5ycX8bChQuRmJiITZs2Ye/evZV9OFVOVTwp6MqtX78eHTt2xLZt2zB27FjMnj0b999/P3x8fPDqq696fH/33HMP8vPzkZCQYGk8842upmeffRYLFizAe++9h0mTJuH48eO44YYbsGzZsso+NPIinIeUTVV8X6hR2QdgZ6mpqVi/fj0++eQTPPjgg1i4cCGmTJlS2YdFZFvPP/88wsLC8MMPPyA8PNztdxkZGR7fn6+vL3x9fS85RimFs2fPIjAw0OP7JyqLQYMGoWPHjq6fx4wZg+joaHzwwQe48cYbK/HIyFtwHlI98ZPzS1i4cCEiIiIwePBgjBgxAgsXLtTG7N+/Hw6HA//85z/x1ltvoWHDhnA6nejUqRN++OGHy+5j69atqFOnDvr06YPTp08bxxUUFGDKlClo1KgRnE4n4uLiMGnSJBQUFFh+PFu2bEH37t0RGBiIpKQkzJ07VxuTkZHheoMJCAhA27Zt8e6772rj8vLy8NhjjyEuLg5OpxNNmzbFP//5TyilXGMcDgfy8vLw7rvvuv7pd9SoUZaPl6qeffv2oWXLltrEHACioqK02NKlS9GqVSs4nU60bNkSK1ascPu99J3zxMRE3Hjjjfjyyy/RsWNHBAYG4s0332S+UaULDw9HYGAgatT47+de//znP9G9e3fUqlULgYGB6NChA/71r39p2+bn5+NPf/oTateujZCQEAwdOhRHjhyBw+HA3//+96v4KMhOOA+ppvMQRUbNmjVTY8aMUUoptXbtWgVAbdq0yW1MamqqAqDat2+vGjVqpKZNm6amT5+uateurerXr68KCwtdY0eOHKmCgoJcP2/atElFRESo/v37qzNnzrjivXv3Vr1793b9XFRUpAYMGKBq1qypJkyYoN588001btw4VaNGDXXTTTdd9nH07t1bxcbGqqioKDVu3Dj12muvqR49eigA6u2333aNO3PmjGrevLny8/NTjz76qHrttddUz549FQA1c+ZM17ji4mJ17bXXKofDoe6//341e/ZsNWTIEAVATZgwwTVuwYIFyul0qp49e6oFCxaoBQsWqPXr11/+iacqa8CAASokJERt3779kuMAqLZt26q6deuqf/zjH2rmzJmqQYMGqmbNmiozM9M1bt68eQqASk1NdcUSEhJUo0aNVEREhHr88cfV3Llz1erVq5lvdNWU5OXXX3+tjh8/rjIyMtSOHTvUgw8+qHx8fNRXX33lGlu/fn31yCOPqNmzZ6sZM2aozp07KwBq2bJlbvd52223KQDqnnvuUXPmzFG33Xabatu2rQKgpkyZcpUfIdkF5yHVcx7CybnB5s2bFQC1cuVKpdSFRKhfv77685//7Dau5KSoVauWOnnypCv+6aefKgDq888/d8UuPim+++47FRoaqgYPHqzOnj3rdp+lT4oFCxYoHx8f9e2337qNmzt3rgKg1q1bd8nH0rt3bwVAvfzyy65YQUGBateunYqKinKduDNnzlQA1Pvvv+8aV1hYqLp166aCg4NVTk6OUkqppUuXKgDqueeec9vPiBEjlMPhUHv37nXFgoKC1MiRIy95fOQ9vvrqK+Xr66t8fX1Vt27d1KRJk9SXX37p9uag1IXJub+/v1uubNu2TQFQs2bNcsVMk3MAasWKFdr+mW90NZTkZemb0+lU8+fPdxt78YRHqQvX1FatWqlrr73WFduyZYs2qVBKqVGjRnFyXo1xHnJBdZyH8GstBgsXLkR0dDT69u0L4MI/jdx+++1YvHgxioqKtPG33347IiIiXD/37NkTAPD7779rY1evXo2BAwfiuuuuwyeffAKn03nJY/n444/RvHlzNGvWDJmZma7btdde67q/y6lRowYefPBB18/+/v548MEHkZGRgS1btgAAvvjiC8TExODOO+90jfPz88Of/vQnnD59GmvWrHGN8/X1xZ/+9Ce3fTz22GNQSrFbQTXWv39/bNiwAUOHDsW2bdswffp0DBw4EPXq1cNnn33mNrZfv35o2LCh6+c2bdogNDRUPGdKS0pKwsCBA8v9+Ik8MWfOHKxcuRIrV67E+++/j759++L+++/HJ5984hpzcS3EqVOnkJ2djZ49e+LHH390xUu+zvXII4+43f/48eMr+BGQnXEeckF1nIdwci4oKirC4sWL0bdvX6SmpmLv3r3Yu3cvunTpgvT0dKSkpGjbxMfHu/1ccoKcOnXKLX727FkMHjwY7du3x0cffQR/f//LHs+ePXuwc+dO1KlTx+3WpEkTANYK7WJjYxEUFOQWK9m+5Pu8Bw4cQOPGjeHj454WzZs3d/2+5L+xsbEICQm55Diqnjp16oRPPvkEp06dwqZNmzB58mTk5uZixIgR+OWXX1zjSp8zwIXzpvQ5I0lKSirXYya6Ep07d0a/fv3Qr18/3H333fjPf/6DFi1aYNy4cSgsLAQALFu2DF27dkVAQAAiIyNRp04dvPHGG8jOznbdz4EDB+Dj46PldaNGja7q4yH74Dykes9D2K1FsGrVKhw7dgyLFy/G4sWLtd8vXLgQAwYMcIuZOkqoiwoTAMDpdOKGG27Ap59+ihUrVliq6C8uLkbr1q0xY8YM8fdxcXGXvQ+iq83f3x+dOnVCp06d0KRJE4wePRoff/yxq9OA1XNGws4sZEc+Pj7o27cvXn31VezZswcnT57E0KFD0atXL7z++uuoW7cu/Pz8MG/ePCxatKiyD5dsjPOQ6o2Tc8HChQsRFRWFOXPmaL/75JNPsGTJEsydO/eKJggOhwMLFy7ETTfdhFtvvRXLly+/7CpcDRs2xLZt23DdddfB4XB4vE8AOHr0KPLy8tz+av3tt98AXOh+AQAJCQn4+eefUVxc7PZX665du1y/L/nv119/jdzcXLe/WkuPK3m8RCXt5o4dO1ah+2G+UWU7f/48AOD06dP497//jYCAAHz55ZduXxuYN2+e2zYJCQkoLi5GamoqGjdu7Iqzp3X1xXlI9Z6H8GstpeTn5+OTTz7BjTfeiBEjRmi3cePGITc3V/v+rCf8/f3xySefoFOnThgyZAg2bdp0yfG33XYbjhw5gv/93/8VjzcvL++y+zx//jzefPNN18+FhYV48803UadOHXTo0AEAcMMNNyAtLQ0ffvih23azZs1CcHAwevfu7RpXVFSE2bNnu+3jlVdegcPhwKBBg1yxoKAgZGVlXfb4yDusXr1a/OT7iy++AAA0bdq0QvfPfKPKdO7cOXz11Vfw9/dH8+bN4evrC4fD4fb94P3792sLopTUT7z++utu8auxqi7ZD+chnIfwk/NSPvvsM+Tm5mLo0KHi77t27Yo6depg4cKFuP322694P4GBgVi2bBmuvfZaDBo0CGvWrEGrVq3Esffccw8++ugjPPTQQ1i9ejWuueYaFBUVYdeuXfjoo49c/Z4vJTY2FtOmTcP+/fvRpEkTfPjhh9i6dSveeust+Pn5AQAeeOABvPnmmxg1ahS2bNmCxMRE/Otf/8K6deswc+ZM11+nQ4YMQd++ffHEE09g//79aNu2Lb766it8+umnmDBhgluRX4cOHfD1119jxowZiI2NRVJSEpcg9mLjx4/HmTNncPPNN6NZs2YoLCzE+vXr8eGHHyIxMRGjR4+u0P0z3+hqWr58ueuTuoyMDCxatAh79uzB448/jtDQUAwePBgzZszA9ddfj7vuugsZGRmYM2cOGjVqhJ9//tl1Px06dMDw4cMxc+ZMnDhxAl27dsWaNWtcnypWxU/+6MpxHsJ5CFspljJkyBAVEBCg8vLyjGNGjRql/Pz8VGZmpquF0UsvvaSNQ6kWWKX7iyqlVGZmpmrRooWKiYlRe/bsUUrpLYyUutBKaNq0aaply5bK6XSqiIgI1aFDB/XMM8+o7OzsSz6m3r17q5YtW6rNmzerbt26qYCAAJWQkKBmz56tjU1PT1ejR49WtWvXVv7+/qp169Zq3rx52rjc3Fz16KOPqtjYWOXn56caN26sXnrpJVVcXOw2bteuXapXr14qMDBQAahy7YzIM8uXL1f33XefatasmQoODlb+/v6qUaNGavz48So9Pd01DoBKTk7Wtk9ISHDLEVMrxcGDB4v7Z77R1SC1UgwICFDt2rVTb7zxhtt18O2331aNGzdWTqdTNWvWTM2bN09NmTJFlX77zcvLU8nJySoyMlIFBwerYcOGqd27dysA6sUXX7zaD5EqEechnIc4lLJQfUVERERX1datW9G+fXu8//77uPvuuyv7cIjoKuF3zomIiCpZfn6+Fps5cyZ8fHzQq1evSjgiIqos/M45ERFRJZs+fTq2bNmCvn37okaNGli+fDmWL1+OBx54gG3qiKoZfq2FiIiokq1cuRLPPPMMfvnlF5w+fRrx8fG455578MQTT6BGDX6ORlSdcHJORERERGQT/M45EREREZFNcHJORERERGQTFfZFtjlz5uCll15CWloa2rZti1mzZqFz586X3a64uBhHjx5FSEgIF16gcqWUQm5uLmJjY92WBfYE85rshnlN3oh5Td7Icl5XRPP0xYsXK39/f/XOO++onTt3qrFjx6rw8HC3RUhMDh06pC3uwBtv5Xk7dOgQ85o3r7sxr3nzxhvzmjdvvF0uryukILRLly7o1KkTZs+eDeDCX6FxcXEYP348Hn/88Utum52djfDw8PI+JCKXrKwshIWFebwd89q6mjVrivHhw4drsby8PC2WlZUlbh8VFaXFTp8+LY5dtmzZJY7Q+zCvyRsxr8kbXS6vy/1rLYWFhdiyZQsmT57sivn4+KBfv37YsGGDNr6goAAFBQWun3Nzc8v7kIjcXMk/UzKvL5CeO+nve9Nz7O/vr8UKCwu1mKl1nLS9n5+fOLa6YV6TN2Jekze6XF6Xe0FoZmYmioqKEB0d7RaPjo5GWlqaNn7q1KkICwtz3bjYAtkR85q8EfOavBHzmqq6Su/WMnnyZGRnZ7tuhw4dquxDIioz5jV5I+Y1eSPmNdlNuX+tpXbt2vD19UV6erpbPD09HTExMdp4p9MJp9NZ3odBVK68Oa+tflXlUvHSBg8eLMYjIiK0mPS1FOk5BYDWrVtrsebNm4tjlyxZcqlDLFeePId24s15TdUX85qqunL/5Nzf3x8dOnRASkqKK1ZcXIyUlBR069atvHdHdFUwr8kbMa/JGzGvqaqrkD7nEydOxMiRI9GxY0d07twZM2fORF5eHkaPHl0RuyO6KpjX5I2Y1+SNmNdUlVXI5Pz222/H8ePH8fTTTyMtLQ3t2rXDihUrtOIMoqqEeU3eiHlN3oh5TVVZhfQ5L4ucnJwr6mlKZFV2djZCQ0Ov6j7tnNcV8X3p2267TYzHx8drMalPuen7n5585/yaa6651CGWKzt855x5Td6IeU3e6HJ5XendWoiIiIiI6IIK+VoLVYymTZtqsTp16ohj8/PztZhpsRZpERhpbFFRkbh9cXGxpZhpXz4++t+IUgyQP6EMCQkRx/70009uPyulcObMGXFsdebJJ7zSX/rSqp8dO3YUt1+/fr0W++tf/6rFpE/IAeDo0aNa7B//+Ic49rHHHtNiBw4c0GIrV64Ut8/OzhbjEpv9AyQRkS2ZFt+x4zU0NjZWjEsdf0xznq1bt17RvvnJORERERGRTXByTkRERERkE5ycExERERHZBCfnREREREQ2wYLQSmYqfJSKC+69914tVrduXXH7goICLWZqOScVf9asWVOLScWcgLwku8nZs2e1WI0aehoePnxY3F4qGjEVYrz22mtuP587dw5ff/21lcOsVtq0aaPF2rdvL46VXuvz589rsby8PHF7qVB01apVWkwq3ASAr776Sovt379fHNuoUSMtFhcXp8X+/Oc/i9sfP35ci5mKR/fu3SvGq5OLC708Ke4yFYhJ7Fg0RmRXZW3xKm0vxUzvwdL7RVJSkjh29+7dWsz0PmJVQkKCGJd63QcFBVm+X6nNpqkt4m+//eb2s1JKbNhRGj85JyIiIiKyCU7OiYiIiIhsgpNzIiIiIiKb4OSciIiIiMgmODknIiIiIrIJdmupZJ5UTksdWKTuJwDEauDSVcMlAgICtJivr68WO3nypLh9rVq1tJipC42/v7+lfZmel3Pnzmkxp9Mpjt23b5/bz0VFReK46uK2224T4y1atNBiqamp4thTp05pMSkvpNcJkKvcpUr/lJQUcXupM0zt2rXFsadPn7Z0XEePHhW3Dw8P12IjR44Ux/773//WYle6bLM3k851T85LKVelnDAxdZySjkHqYmW6rkmPS+pC5UlnGhPp2ijFTMfqyfMlnZuBgYGW71Mau2vXLnGsdL5S5Sprd6SmTZtqsTp16ohjz5w5o8VMuSIZPny45bFSNzoph6X3AEDOVWluA+hzNKvPKT85JyIiIiKyCU7OiYiIiIhsgpNzIiIiIiKb4OSciIiIiMgmWBBahUjFTKZiKqk4wVSwIBUpSWPr1asnbi8VDUnFUIBcOCUVLpmWA5bGSoVbgF6IYbpPbxQTE6PFmjdvLo7duXOnFjMVrkmvn1T0ZSpUlgpKpYLinJwccXupmMhUjCadG9JxhYSEiNtLY3///XdxbLt27bRYdSsItVLo5Enx54ABA7TY4MGDtdj+/fvF7TMzM7VY69atxbFSgZdUVG+6hkrF+tI5ZCrS9KRQ1Or9mp5rq0WegLz8uvR8m5Yjb9SokRZbt26dOHbp0qVinK5cWQs6rd6nKX+la7OpgUNwcLAWi4qK0mJjxowRt5euzZGRkeJYaX4indtSkSogny+m863088WCUCIiIiKiKoaTcyIiIiIim+DknIiIiIjIJjg5JyIiIiKyCRaEViFSgZpUoAfIRW6m4gipoFIqWjCtrudJMZJ0vNL9mvYlHWtsbKw4tnQxR3UqCG3WrJkWy8rKEsd6supmdna2FvMkV6TXTzou08psUl6Zio+l11taDddUpJebm6vFTMWj0vFKRVIVUaBV1b3wwgtiXCqylAo3pdVZAfn1NxV4devWTYtJq8l6cr2VctV0XkjH6sn1yup1FZAL36TnGgB2796txaTn21SU/cgjj2ix3r17i2NXrVrl9rNSSjwHyX5M12CpoDgvL08cK51DUkHo4cOHxe2l1adN+5LODanhhWl76dw2XRuuFD85JyIiIiKyCU7OiYiIiIhsgpNzIiIiIiKb4OSciIiIiMgmODknIiIiIrIJdmupQoKCgrSYaSlcqZrYVPkuVVRLnSZMSzRL1f+mymWpql8aa+oeIHX7MFVUS8t3VxfS82TiSaeHmjVrajEpL6TOQoCcr1JXDNMy6VIHC1OuSKTnxZTX0vNi6vYhdQqQOt4cP378codYJdWoUcPtmiG9pgDw9ddfa7E5c+aIY6XOOsOHD9diDz/8sLh93bp1tdjRo0fFsVIODRo0SIvt3LlT3F7Ka6kLkKkDi3S9trocuIm0HDogd9bYv3+/ODYhIUGL9ezZ0/IxSY/BdG43btxY23br1q3iWLo6rHackjqdAEBSUpIWM10DpfeMOnXqaLGTJ0+K28fExGgx09xAujafOHFCi5lyVTq3Te95pc83pZSxu5HbPi47goiIiIiIrgpOzomIiIiIbIKTcyIiIiIim+DknIiIiIjIJlgQWoVIy9uaCoykog1p6WtALnKTCiZMxaehoaFazFTwIMWlQgpTQaJUaGYqxKjOpGKwY8eOiWOlop0DBw6IY6UiSak4xrScs9XiTVPRUK1atSzt3xSXigxNhdLSWKkoG5DzVSqm89aC0NLndZ8+fcRx0dHRWmzJkiXi2A8//FCLSXnVsGFDcXvptUpMTBTHzpgxQ4tJ17VrrrlG3F5a5l66NpuKPKVrq1SMB8h5LT3WU6dOidv/+uuvWsxUwNuiRQstJhXJeXK9l4rKpWMwPVdkP6biY6lQ3nQNlM4BU7G+RJofmQpVrV6HTYXOnjQxKN1wo7i42FKzCn5yTkRERERkE5ycExERERHZBCfnREREREQ2wck5EREREZFNcHJORERERGQT7NZSyawuxQzI1etS9wxA7jThSfW7dL+min4pLlVOA3IHhKysLC1m6oohdWYx7as6k7qa7Nu3Txzbtm1bLWaqZpc6m/j5+WkxU6cJiZRrUpW/aaypA4zUFULq1vHtt9+K27dp08byvqQclpaPry5Gjx4txsePH2/5PuLj47VYWlqa5e2lXA0PDxfHjhkzRov94x//0GKmzlDNmjXTYtI12ES6hprOoYMHD2qx9PR0LZaTkyNuL3VxMXWxkXJ4z549WszUyUt6f2vSpIk4tnTXMFMnMrp6pByUXlNTpxLpfDFd26VrqDRnadCggbh9TEyMpf0DQGRkpBaTutbl5eWJ20uk90FAP7etzvn4yTkRERERkU1wck5EREREZBMeT87Xrl2LIUOGIDY2Fg6HA0uXLnX7vVIKTz/9NOrWrYvAwED069dP/GcwIjthXpO3YU6TN2JeU3Xg8eQ8Ly8Pbdu2xZw5c8TfT58+Ha+99hrmzp2L77//HkFBQRg4cCBXcSRbY16Tt2FOkzdiXlN14HFB6KBBgzBo0CDxd0opzJw5E08++SRuuukmAMB7772H6OhoLF26FHfccUfZjraKs1pcYSIV55gKQrOzs7WYtMQ4IBdpfvfdd1rMtCS7tC9TgZEUDwwM1GKm5W1jY2O12E8//SSO9YS35bVUDGkqEEtKStJi0hLfQNmLtKTtpRw25ZpUkGkaK70Zb926VYuZlp7OyMjQYqbzTSqIMhU1Xy1XM6fDw8Pdrm9Dhw4Vx40cOdLyfUrXBel6aXr9pddq//794tgOHTposTvvvFOLbdu2Tdx+1apVWqxz585a7Pfffxe3l67tJ06cEMcOGTJEi61bt06Lma7BUpGd9FgBICUlRYtJeW0qXpXOd6nwD9AfV0FBAV5//XVtnLddq72B6Q+fw4cPa7GEhARxrFTsLV1XT58+LW4vNYaQriEAsGvXLi0mFTWbmmhIcatjK6UgNDU1FWlpaejXr58rFhYWhi5dumDDhg3luSuiq4Z5Td6GOU3eiHlN3qJcWymW/OUTHR3tFo+Ojja2wCooKHD7NMz06R5RZWFek7e5kpwGmNdkb8xr8haV3q1l6tSpCAsLc93i4uIq+5CIyox5Td6IeU3eiHlNdlOuk/OSJvClF0RIT08XG8QDwOTJk5Gdne26HTp0qDwPiajMmNfkba4kpwHmNdkb85q8Rbl+rSUpKQkxMTFISUlBu3btAFz456Hvv/8eDz/8sLiN0+k0ri7mbcpaECr9U5tpZS6p+NO0L6kgrkWLFpaP68iRI1rs/Pnz4lhp1T6peFEqSAWAG2+8UYv9+OOPlzvEMrF7Xksrrvn46H93mwrnpKIZU3GLdL+erAYqbS/ty1RgZMp3ibRimyeFPNI5VKdOHXGsVORmKkaygyvJacCc13fccYdbfPny5ZaPxZQ/ZT1/pOudlH+AvHruddddp8VMX42QVuSVVjjdsmWLuP17772nxUzFs1KhspSrBw4cELe///77tZhUKA1YX+XUdG2RVl00nW+tWrVy+zk/P9/Svi9W3nld3Vmdn5j+mJGKP00FwVJjAqlw01SUv379ei1mamAgzU+k67WpqF9a0df03lT6fouLi8V5UGkeT85Pnz6NvXv3un5OTU3F1q1bERkZifj4eEyYMAHPPfccGjdujKSkJDz11FOIjY3FsGHDPN0V0VXDvCZvw5wmb8S8purA48n55s2b0bdvX9fPEydOBHDhr/z58+dj0qRJyMvLwwMPPICsrCz06NEDK1assPwXOFFlYF6Tt2FOkzdiXlN14PHkvE+fPpf8pw6Hw4Fnn30Wzz77bJkOjOhqYl6Tt2FOkzdiXlN1UOndWoiIiIiI6AJOzomIiIiIbKJcu7VURWXtoFJRpOOSqtelJWsBICIiQouZKo+lyuXQ0FAtZqpGLr3gw6X2JZGqt7t06WJ5+z179lge642kpb+lXJEqzAH5tTaRqt+l/DF167Ha7cW0CEhUVJSlYzLtS+oqYTpW6X5NHR2kJaXDw8PFsd4oMTHR7Tu97777ruVtTddbqVOD1OnD1L1BYvresdVl5seOHStuv3LlSi3222+/aTHT9fpvf/ubFjNdQ6VcGzhwoBbr3LmzuL20UubFC/BczGp3JNM5KHVMkjq4AHrHGakDDXnG6vzG1DHJ6vamc1DqBGfal/T+JF1vpbkNIOeayalTp7RYZGSkFqtXr564/cUFySXy8vLEsaU7fBUVFeH48eOXPUZ+ck5EREREZBOcnBMRERER2QQn50RERERENsHJORERERGRTVT7gtCrWfzpyTLnUuGSVFxRs2ZNcXurS5cDcuGctC/TkupSgY+pOEISFxdnaf8mpgKj6kIqmpFea1Pho1SIIxX5AvIy51LRmGk5b6nwTDovYmNjxe2lYjjTOSAVqUm5bipmkwo6Y2JixLE///yzFpNeF1PRkqlYt6qoXbu22+uwe/duy9uaisk8uV5KPCl8k3JAKkg8fPiwuH2PHj20mLQcuKnQ+ddff9VipZezLyEVRWdmZmqxb775RtxeOjdNr4HV88WTgkLTvkq/XnZozFDVWX0OTeOsnoMNGjQQ49L1PiwsTBwrNTGQ3rNMx+RJsXjpIk1AfgwHDhwQtz9x4oSl+wT0x2Cah5XGT86JiIiIiGyCk3MiIiIiIpvg5JyIiIiIyCY4OSciIiIisolqXxB6NXlS4NK4cWMtJhU3mAovpaIJ02pvUtFPWVdn86TA5ODBg1rMVDgnPQfSCpnVibSSoFR0YnpOjx07psWkYjbAeg6biiyl10/KP09WLDTtSyIdv6l4VSrSNBVFS8+tJ8VQUkFfVVKjRg2351Eq7jIxFf9KueLJdcmTglBprPT6S7kKABkZGVrMaq4D8vXeVDwqFalJ54XpfPek0FYqyJMegyfXe9P5Wvq1repF0nZQ1hXQpddKyqv69euL2+fm5moxU+GktEKntPq3KS+kYzXNDaTC/l9++UWLmQrApaJs04rQqampbj9bff75yTkRERERkU1wck5EREREZBOcnBMRERER2QQn50RERERENsHJORERERGRTbBbSwXxZJlwSf/+/bWYVPkcEhIibi91CrBaJQ/IFdGmZdKDg4O1mLR0uek+pG4Zpq4YUneadu3aiWM///xzMe5trHYwMb0mUkcG0xLDUl55skS3dFxSzPT6S51ppOMH5HPQagyQu62YKu2l45Kel4CAAHH7qq6goMDtefSkI4ipM5BEev7Lo1OIdA2S8sqU1xJPumJI3W086Vgl5aqpO5d0vTe9XtK+pGuDJ11wTM9L6eMqa8cwb+VJBxar56En23fu3FmLmbprSXMD03vLjh07LN2vqbNVy5YttZjpPe+nn34S46U1atRIjEvvT1lZWeLY0nnMbi1ERERERFUMJ+dERERERDbByTkRERERkU1wck5EREREZBNVviDUk8IjTwp0rDIt/W0qeijt5ptvFuPSUrDSfZqKfqRiItMxWS3oNBWzSXFTgYhUZCUVV5w5c0bcXireMy2bW11I54AnRZ7SWFOhsTRWKh42FbNZLZQ2FQlK23tS+CYVuJmK/KTny/QcSueA9FyZCpSqutzcXLfH68nS657kivT8m7Yva+GidL+m65rV4l+p8NO0f0+WKbd6DQDk58VUKOtJAbVEOgarRbmm99bqzpN5jNUmFKbXtEmTJlpMut6eOHFC3D4xMVGLmQonpetwUlKSFqtfv77l7fft22d5rPS4Tp06JW4vXRtM1/bS+1JKWZof8pNzIiIiIiKb4OSciIiIiMgmODknIiIiIrIJTs6JiIiIiGyCk3MiIiIiIpuwdTn0xRXopgplKW6qPLZaZW7al1T5bKqIl/Tt21eLtWnTRhx7+PBhLSYtUx8RESFuL3U7MXXgkLoKSJ0CTFXWUkWz6TmsWbOmFpMqn03HKnUliIyMFMdWF1ZfP1NXEum8MOW1tC/pvDB1CZDiUlcG07FKeW2qkpdyRXpeTF0hpO1N3T5iY2O12OnTp7WYJ8u/VyVHjx51ex0eeOABcdzUqVO1mCnXrHaaMHU+kPLa9PpZ7WDiSacS6fX35Bpskp6ersU86eAhjTU9L9JzID1XpnNIOt88ec+0AyvzkMttdzkV0UkOAEJDQ7WYdK2SuqoA8nHl5eVpsaZNm4rbS+/tubm54lipu5aUa6ZrqHS/0jkIyK+NtC+pkx0AnDx5UosdO3ZMHFv6HLL6WvOTcyIiIiIim+DknIiIiIjIJjg5JyIiIiKyCU7OiYiIiIhswrYFoQ6H44oLMawWEpUH0zLjN954oxZr1KiRFpMKCwCgdu3aWkwqxPDkecnJyRHj0tK70nPoSVGuqehHKvqIi4vTYp4UbkkFLtWJ1SJNU9GWJzkkLXF89uxZy/cpFdRJBZ1paWni9lKBkXRMprgnS7pLuWo6h6QclIpXr+a16WrasWOH22szd+5ccZxUEGp6ThMSEsT9lGYqEJNeP9NYq4W6pryWCh+joqK0WH5+vrh9RkaGFpOOHwBiYmK0mFSsbyq886RQ0SpTUW5BQYEWMz2HpcdK21Y203MnPaayFnmaio+DgoK0WIMGDcSx4eHhWkzKC1Mxo7S99H4jzU0Az15/6doYHR2txUzvDdKcSTpXAOsF2L///rsYlx7vRx99JI5t2LCh28/FxcU4cuTIZffNT86JiIiIiGyCk3MiIiIiIpvg5JyIiIiIyCY4OSciIiIisgnbFoQqpdwKB2rVqiWOa968uRbzZLUyqcBDKjoD5OIEqQjBdB9SIYNUNATIBZW7du3SYlIxJwDUrVvX0n0CclGrFDMVqHiyCpx0H1JRrOk1kIr8pP1XJ9JzLZ0D0uqsAHDo0CEtJhUzAtZXI/WkGEra3rSSolR4ZiocM60capWUq6ZCIqtFUqbXoKr76quv3H7evn27OK579+5abP369eJYKYel64Ip16Rru+n1K+uqldL20nvW3XffLW6/detWy/t68skntdjAgQO1mFRkCsjPoamg03QdtkoqtJUKyAG9MNguBaFWrmVSrknFlIC8onWdOnW0mOn6ZbWoHpCv4/Xr19dipkLl48ePa7GwsDAtlp2dLW4vFTVL9wkAnTp10mLS+4BpziXli2luIOWgNA/xZLV507lS+tpgtSkAPzknIiIiIrIJTs6JiIiIiGyCk3MiIiIiIpvwaHI+depUdOrUCSEhIYiKisKwYcOwe/dutzFnz55FcnIyatWqheDgYAwfPhzp6enletBE5Yl5Td6IeU3eiHlN1YFHk/M1a9YgOTkZGzduxMqVK3Hu3DkMGDDArfDp0Ucfxeeff46PP/4Ya9aswdGjR3HLLbeU+4ETlRfmNXkj5jV5I+Y1VQcedWtZsWKF28/z589HVFQUtmzZgl69eiE7Oxtvv/02Fi1ahGuvvRYAMG/ePDRv3hwbN25E165dr/hAGzduLMalJZ5NHRHK2pUkMzNTi5mqnE+fPq3FpIpuaRxgvVOBtGwzIFcpmyq6pS40nhyrVJEtdYsBgNDQUC129OhRLWZ6DaWK7LJ2FKjMvC4PUjcB6XkKDg4Wt5c6a5i6I0mV+lIOmjomSXklVa+bXlOry6wD8jkgndumJbmlvJa6BQHyY5C6tZQ1Vz1RmXk9adIkMf7oo49qMVO3lmXLlmmxNm3aaDGpywIgd2YxdWUp65L2VjtoSJ2RAPl8MR1r6a4mpn2ZOrBIz4sn56t0DpqOVbo2md5fv/32W0v3WZl5HRERIcala6vpWiW9LgcPHtRi0vXHxPScSscgvbea8l+63klzHiknAXkeYHr+pblYvXr1tJg0hwCA1NRULWbqOiZdr6XnKiYmRtxees/csGGDOLb03PX8+fOW/hWnTN85L2mfU9IaaMuWLTh37hz69evnGtOsWTPEx8cbD5zIbpjX5I2Y1+SNmNfkja64z3lxcTEmTJiAa665Bq1atQJwoY+3v7+/1t8zOjpa7PENXPiE7+JP+Ux/gRFdDcxr8kbMa/JGzGvyVlf8yXlycjJ27NiBxYsXl+kApk6dirCwMNctLi6uTPdHVBbMa/JGzGvyRsxr8lZXNDkfN24cli1bhtWrV7utNhUTE4PCwkLtO6jp6enG7+5MnjwZ2dnZrpvpO3lEFY15Td6IeU3eiHlN3syjr7UopTB+/HgsWbIE33zzDZKSktx+36FDB/j5+SElJQXDhw8HAOzevRsHDx5Et27dxPt0Op1iMUvt2rXdihx69uwpbi99sd5U3CAVaOXm5mqxoKAgcXtpOV5TkaXVJc1NRXpS0YhUCGFajloqJjEVmEhFE6biT4l0DJ4UzkhFJ1JxiOm4pKIRQF/mt7i4WCweu5p5XRGkvJIKXkyvyZo1a7RYhw4dxLFSoajV4hrTWOlcsbrEMWAuZpOKpKTjMl0vpLFS4RYANGrUSItJj+Fq5QRw9fP64udxx44d4hipePizzz4Tx0rXQNN7hUQqKDQVzkmvlbR/T5a5P3DggBYbMGCAuP2vv/6qxUwFkdL70++//67FTK+T9FhN55BEOl9Nhc7SZFhqrAAA3333naX9V+b1ukWLFmI8NjZWi5mWjpcKKqX3QNP7tTSPMY2VckXKYdP7rZQX0vu96bFK7xemY734D6xLHdfOnTvF7UtqDy4WEhIijpUaG0jvo6bHFR8fr8XmzJkjji1d8C7tR+LR5Dw5ORmLFi3Cp59+ipCQENf3t8LCwhAYGIiwsDCMGTMGEydORGRkJEJDQzF+/Hh069at0jtaEJkwr8kbMa/JGzGvqTrwaHL+xhtvAAD69OnjFp83bx5GjRoFAHjllVfg4+OD4cOHo6CgAAMHDsTrr79eLgdLVBGY1+SNmNfkjZjXVB14/LWWywkICMCcOXOMH/ET2Q3zmrwR85q8EfOaqoMy9TknIiIiIqLyw8k5EREREZFNXPEiRBWtY8eObhXgN954ozjul19+0WKmBQSkKmGp28uRI0fE7aVqYlNXE6nKV+pUYvonOql6W6qyl7rNAHKVtnT8gFy57EmVtcS0zLnUgUDqImLq1iHdr6mDQlRUlDbOtNR3VSY9J1JFvakryYkTJ7SYqduK1O3CVOkvkc4L6VhNr6mpel4idV+Qtr948ZGLSXlp6hQgdYGQcvjs2bPi9lVdjRo13PLL9Dp9//33Wqxz587i2P3792sx6bUydWuRXn/TNUzKaylmuq5J55Z0bR43bpy4vdRVQurqAcjdOqSYaZlzU8caiXQdkLY3LZO+du1aLfbMM89Y3r/dmK6L0pzDdF2UrguedEeLjo7WYqXf60qcOnVKi0mPwXRedOzYUYvVrFlTix09elTcXjoHpc42gHzN/+2337SYqbOQ9HybHpd0bkr3azpXpGOVOrhIx6WUsvR685NzIiIiIiKb4OSciIiIiMgmODknIiIiIrIJTs6JiIiIiGzCtgWhK1ascPvZVIgzbNgwLdakSRNxrFSMKBVolaw4VppURGD6Yr9UZCctcSwVbpri4eHhWkwqDgHkZZNNhbLSc7BgwQItdtttt4nbS4WupqWnpeI/ialQVipmkQpUAP35Mh1TVSflmnQOmIpjpO1NhTBSDkvFMabzVbpf6T6lnAKAY8eOaTHTEs0SKX9MRV5SMZNpWXqpiFwqtN6zZ8/lDrFKsroktZRr0tLzJtL11vT6SzlkGivlpfT6S8cPyDkknRfr1q0Tt5euTaZif1O+liblHyAXmpoKeKXz7ddff9Vi27dvF7c3vedIShfVWs2pilS60LlevXriOOm9+fjx4+LYhIQELSYVn5teZ0+WqZfeG62+B5tkZmZqscjISHGslGum+ZVUKNu8eXMtZsop6XwxFeVKxZ9SvpneM6UCcNPrVfo6YDWv+ck5EREREZFNcHJORERERGQTnJwTEREREdkEJ+dERERERDZh24LQ0v79739bjjdr1kwce+edd2qxBg0aaLHGjRuL20vFFaYiAKnARioiMK3aKMWzsrK0mGnFwqeeekqLmYqRrHrllVfEuHRcpkJZq4VTphVCpefVVOgZFxenbbtp0yZxbFVmdSVKU4GZxFQQKhV6elJkKZEKlEwFpdLrbyqwsXq+mR6rFDet8CkVGEn7kq43gLzScVViWtG1tJSUFC22atUqcaxUOCflhWklTKmo3lQ8LuWr9Pp5UlR+6NAhLWYqKPZWnqxGarrmV6bGjRu75YZp1daDBw9qMdN7u/R+KeWK6RooMV2XTA0nSpOK8gG5+FJ6XKZVlqXX1HS+SudW6fdwwFzk6ckK5tL5Lj2Hnry3WJ0LsiCUiIiIiKiK4eSciIiIiMgmODknIiIiIrIJTs6JiIiIiGyCk3MiIiIiIpuoMt1aTNXEUveGXbt2iWOnTJliaV+mKuumTZtqsbp164pja9eurcWkjg6mpWyPHj2qxXbv3i2OvVoeeughMZ6enq7FpCV+Abl6W6r0NlXFSxXZpuV8MzIyxLi3kboTSctMm5Yel2zdulWMt2vXTotJeW2qnJdef6nK3bS91NnFVCUv3YfUPaJWrVri9lJXBRPpGGJjY8t0n9WFqXvB/v37r+6BULmzYwcWTxw/ftztmjF8+HBxnHQNNHUwkbqSeNKxTLqGmcZK9yt1gTF1MZI6o0hjnU6nuL3EdKzS9frMmTNaTJrzAZ49h3l5eVpMegym9xar3akA/frGbi1ERERERFUMJ+dERERERDbByTkRERERkU1wck5EREREZBMOZfXb6VdJTk4OwsLCKvswyItlZ2cblxCuKBWZ11KBT3h4uBYzFceYincl/fv312LXXXedFjty5IjlfcXExGgx07EePnxYiwUHB4tjpcKpkJAQLSYVHQHAe++9p8U8WSJaKiyvyMutt+U1EWCvvJYK0gG5+DsiIkIcKz0WqcjRtEy9FDcVKEqNNKR9SUWqgHy9y83N1WKm67V0DTQ195CKak1jy0p6vqTGAJ48L6dOnRLHbtq0SYxfLq/5yTkRERERkU1wck5EREREZBOcnBMRERER2QQn50RERERENsHJORERERGRTejlsURUpUiV4xkZGRWyr5UrV2qxH3/8UYu1atVK3D4yMlKLnTx5UouZuqJI1fPSUsyA3JVgw4YNWmzXrl3i9mVls0ZYRFRGZ8+eFeO///77VT4S8nb85JyIiIiIyCY4OSciIiIisglOzomIiIiIbMJ23znn9zSpolVGjnlzXkurw5lWVpO+By59v1wad6m41bGmlfS8AfOavBHzmrzR5XLMdpNzaWlYovKUm5t71Zcc9+a8lpYtXrduXSUcSfXGvCZvxLwmb3S5vHYom/2JWFxcjKNHjyIkJAS5ubmIi4vDoUOHEBoaWtmHVm5ycnL4uCqBUgq5ubmIjY2Fj8/V/UYX87rqsvvjYl5XLLu//lfK7o+LeV2x7P76Xym7Py6reW27T859fHxQv359AIDD4QAAhIaG2vJJLis+rqvvan8CU4J5XfXZ+XExryseH9fVx7yueHxcV5+VvGZBKBERERGRTXByTkRERERkE7aenDudTkyZMgVOp7OyD6Vc8XFVb976PPFxVW/e+jzxcVVv3vo88XHZm+0KQomIiIiIqitbf3JORERERFSdcHJORFXW/Pnz4XA4sH//fo+3HTVqFBITE8v9mIiIyB2v1Z7h5LycOBwOS7dvvvmmsg+VqEy2b9+OESNGICEhAQEBAahXrx769++PWbNmVfahEVW4ffv24cEHH0SDBg0QEBCA0NBQXHPNNXj11VeRn59fIftctGgRZs6cWSH3Td6L1+qqy3Z9zquqBQsWuP383nvvYeXKlVq8efPmV/OwiMrV+vXr0bdvX8THx2Ps2LGIiYnBoUOHsHHjRrz66qsYP358ZR8iUYX5z3/+g1tvvRVOpxP33nsvWrVqhcLCQnz33Xf4y1/+gp07d+Ktt94q9/0uWrQIO3bswIQJE8r9vsk78Vpdtdn6k/M5c+YgMTERAQEB6NKlCzZt2lTZh2T0xz/+0e3WpEkTAMCHH36ISZMm4Z577kFwcDCio6Nd2yilMHnyZNStWxeBgYHo168f9uzZU1kPwZKpU6eiU6dOCA4ORlRUFIYNG4bdu3e7jTl79iySk5NRq1YtBAcHY/jw4UhPT6+kI7aXqpTTkueffx41a9ZEw4YN8frrr2Ps2LFo3749vvzyS6xfvx7Ahbx++umnq2Reh4SEMK+vQFXPawBYu3YthgwZgtjYWDgcDixdutTt97///jtuueUWFBYWIj8/Hzt37kSfPn2QnJyMDz74AL/88gtatmxZOQdvwLwum6qc1956rQaqSV4rm1q8eLHy9/dX77zzjtq5c6caO3asCg8PV+np6ZV9aJYkJycrAOqJJ55Qn3zyiQKgWrZsqVq2bKk2b96sevbsqfz8/JS/v79aunSpWrVqlYqPj1c+Pj7K6XSqNm3aqPnz57vd5+rVqxUAtXr1ard4amqqAqDmzZvnih07dkyNGjVK1atXT/n7+6uYmBg1dOhQlZqa6rbtF198oXr06KFq1qypgoOD1Q033KB27NjhNmbkyJEqKChI7d27V9WuXVsFBASoa6+9Vm3dulXdcMMNKj4+Xp0+fdo1/qGHHlJxcXEqJSVFbd68WXXt2lV17969XJ7Xqqyq57RSSjVt2lS1adPGLa+XLFniNubFF19UgYGBqnXr1ioiIkL5+PgoPz8/9eqrr2r3l5CQoAYPHqy+/fZb1alTJ+V0OlVSUpJ69913tbE7duxQffv2VQEBAapevXrqH//4h3r77bcVALe8Xrp0qbrhhhtU3bp1lb+/v2rQoIF69tln1fnz593ub+TIkSohIUEppdTAgQPVvHnz1I4dO5jXHvKGvFbqwrXwUnndpUsXBUC9+OKLatu2bWro0KEqKSlJ5efna/d17tw59eyzz6oGDRoof39/lZCQoCZPnqzOnj3rNs5Krvbu3VsBcLuV5O3lMK+vXFXPa2+9VitVPfLatpPzzp07q+TkZNfPRUVFKjY2Vk2dOrUSj8q6ksl5iZLJeUxMjKpTp44aN26cCg0NVSNHjlRnzpxRzZs3V35+fsrX11eNHDlS9ezZUwFQM2fOdN2HJ5Pz7t27q7CwMPXkk0+q//u//1MvvPCC6tu3r1qzZo1rzHvvvaccDoe6/vrr1axZs9S0adNUYmKiCg8PdzuBRo4cqZxOp2rYsKEaOXKkmjt3rnrvvfeUUkplZGQoAK77zcrKUn5+furjjz92bf/rr78qAGrDhg3l8dRWWVU9p5VSasCAASokJERt375dKaW0C35xcbGKiYlRcXFxatSoUeqVV15R06dPVw6HQwFQs2fPdru/hIQE1bRpUxUdHa3+9re/qdmzZ6s//OEPyuFwuP2ReOzYMVWnTh0VERGh/v73v6uXXnpJNW7cWLVp00a74A8bNkzddttt6qWXXlJvvPGGuvXWWxUA9T//8z9u+y59wb8Y89o6b8jr0qS89vHxUZGRka5YVlaWcjqd6oMPPtC2HzlypAKgRowYoebMmaPuvfdeBUANGzbMbZyVXP3qq69Uu3btVO3atdWCBQvUggULtEmWVcxr66p6XleXa7VS3pnXtpycFxQUKF9fX+0CdO+996qhQ4dWzkF5yDQ5B6Dmzp2r9u3bpwCon376Sc2cOVMBUO+//77q1auX+tOf/qQKCwtVt27dVHBwsMrJyVFKWZ+cnzp1SgFQL730kvH4cnNzVXh4uBo7dqxbPC0tTYWFhbnFS95oHn/8ce1+9uzZowC4LgApKSkKgDp16pTbuPj4eDVjxozLPm/eyhtyWqkLEwVfX1/l6+urunXrpgCoKVOmqMLCQqWUcuV16Qtgr169VHx8vGrQoIFbPCEhQQFQa9eudcUyMjKU0+lUjz32mCs2YcIEBUB9//33buPCwsK0C/6ZM2e0437wwQdVzZo13T65vNQFn3ltjbfkdWmlJzJbt25VAFSfPn3cxpVcry9WMvb+++93i//P//yPAqBWrVrlilnN1cGDB1v+tPxSmNfWeENeV5drtVLemde2/M55ZmYmioqK3L6fDQDR0dFIS0urpKMqH06nE6NHj3Y9jujoaHzxxReIiYnBnXfe6XqMfn5++NOf/oTTp09jzZo1Hu0jMDAQ/v7++Oabb3Dq1ClxzMqVK5GVlYU777wTmZmZrpuvry+6dOmC1atXa9s8/PDDbj8XFxdjwoQJuOaaa9CqVSsAQFpaGvz9/REeHu421hteu7Lwlpzu378/NmzYgKFDh2Lbtm0AgGeeeQb16tXDZ5995nosCQkJrm2ys7MRHh6OsLAw/P7778jOzna7zxYtWqBnz56un+vUqYOmTZvi999/d8W++OILdO3aFZ07d3Ybd/fdd2vHGBgY6Pr/3NxcZGZmomfPnjhz5gx27dp12cfIvLbOW/L6clJTUwEAtWrVcotLj/OLL74AAEycONEt/thjjwG4UFRaoqy56gnmtXXekNfV4VoNeG9es1vLVVavXj34+/u7xQ4cOIDGjRvDx8f9b6WSzi4HDhzwaB9OpxPTpk3DY489hujoaHTt2hU33ngj7r33XsTExACAq+jj2muvFe8jNDTU7ecaNWqgfv36brHk5GTs2LED3333nUfHR1Vbp06d8Mknn6CwsBBOpxPDhw/Hf/7zH4wYMQLz5s0DAGzatAmzZs3Chg0bcObMGbfts7OzERYW5vo5Pj5e20dERITbH5YHDhxAly5dtHFNmzbVYjt37sSTTz6JVatWIScnR9v35TCvqbSgoCAAwOnTpy879sCBA/Dx8UGjRo3c4jExMQgPD3e7npc1Vz3BvK5+vP1aDXhvXttycl67dm34+vpqlbXp6emuyWVVVfKXYsnjkB5ju3btxG0dDocYLyoq0mITJkzAkCFDsHTpUnz55Zd46qmnMHXqVKxatQrt27dHcXExgAstIKXntEYN99RwOp1ufzyMGzcOy5Ytw9q1a90m7TExMSgsLERWVpbbX63e8NqVhTfmdMkfmX/84x9x4403YvTo0fjhhx8AALfddhuaN2+OGTNmIC4uDo8//jh8fX2xdetWV+6V8PX1Fe9fKeXxMWVlZaF3794IDQ3Fs88+i4YNGyIgIAA//vgj/vrXv2r7Lo157RlvzGtJw4YNAQC//PKLW/xKrtclypqrnmBee8bb8tobr9WAd+e1Lb/W4u/vjw4dOiAlJcUVKy4uRkpKCrp161aJR1Z+kpKSEBMTg5SUFCQkJGDPnj3IysrC999/73qMJf+sU/LPThEREQAuJPXFTJ+sN2zYEI899hi++uor7NixA4WFhXj55ZddvwOAqKgo9OvXT7v16dNHvE+lFMaNG4clS5Zg1apVSEpKcvt9hw4d4Ofn5/ba7d69GwcPHvSa1+5KeHtOd+zYEQCQn5+PkJAQFBYW4rPPPsODDz6IHj164LfffivTCm8l50hppdtnffPNNzhx4gTmz5+PP//5z7jxxhvRr18/17ljwry+Mt6e1yWSkpIQGBiIQ4cOYcOGDQCAnJwct+t1iYSEBBQXF2v5mp6ejqysLNf13JNcvdxE34R5fWW8Oa+r+rUaqB55bcvJOXDh+3r/+7//i3fffRe//vorHn74YeTl5WH06NGVfWge2bp1K7Zu3QrgQt/Ns2fP4uDBg3A4HJgwYQKee+45xMfHIy0tDf369UNsbCyGDRuG8+fPY9asWQgODkbv3r0BXEh6X19frF271m0fr7/+utvPZ86cwdmzZ91iDRs2REhICAoKCgAAAwcORGhoKF544QWcO3dOO+7jx4+Ljyc5ORnvv/8+Fi1ahJCQEKSlpSEtLc21Ml5YWBjGjBmDiRMnYvXq1diyZQtGjx6Nbt26oWvXrp4/gV7EG3J69erVyM3Ndcvr1NRU/N///R8AoFmzZq4/7L766its377d9XWqjRs3XvF+b7jhBmzcuNGtz/Dx48excOFCt3Eln+xc/ElOYWGhdo6Uxry+ct6Q18CFr6yUzuutW7e6rtfjxo0DANx+++345ptvcO+997qu18CFlUNfffVV3HDDDQCgreg5Y8YMAMDgwYMBeJarQUFBV/Q1F+b1lavqee2t12qgmuR1pZWiWjBr1iwVHx+v/P39VefOndXGjRsr+5AsK+nWIt1GjhyplLrQyuipp55SUVFRyuFwKIfDoe677z41a9YsV2/bi1spKqXUHXfcoWrUqKEmTpyo5syZowYNGqQ6dOjg1q3lp59+UpGRkeqhhx5Sr732mnr99ddV//79FQD1r3/9y3VfCxcuVD4+PqpVq1bqueeeU2+++aZ64oknVLt27dxaSJX0OVdKGR/TxW0c8/Pz1SOPPKIiIiJUzZo11c0336yOHTtWMU90FVOVc1oppVq2bKnq1q0r5kBwcLA6deqU+vXXX5Wvr6+qUaOGqlGjhmrYsKGKj49Xbdu21ar1S3rnlta7d2/Vu3dv189Hjx5VtWrVumx7rszMTBUREaESEhLUyy+/rGbMmKHat2/v2vfFnY4u7gDAvC6bqp7XSv23G9alrte33XabKx4XF6eee+45NWfOHHX33Xcrf39/9cADDyil/tvh6rbbblNz5sxx/XxxK0VPcnX69OkKgHr00UfVokWL1GeffWbpMTGvy6Yq57W3XquVqh55bevJeVVWupWiUheSuGXLluL49PR0NXr0aFW7dm3l7++vWrdu7ZZoJY4fP66GDx+uatasqSIiItSDDz6oduzY4ZaYmZmZKjk5WTVr1kwFBQWpsLAw1aVLF/XRRx9p97d69Wo1cOBAFRYWpgICAlTDhg3VqFGj1ObNm11jLp6cU/W2fPlydd9996lmzZqp4OBg5e/vrxo1aqTGjx/vtjjHZ599ptq0aaMCAgJUYmKimjZtmnrnnXeu+IKvlFI///yz6t2792UXtli3bp3q2rWrCgwMVLGxsWrSpEnqyy+/vOwFn8iK3377TY0dO1YlJiYqf39/FRISoq655ho1a9YsV/u3c+fOqWeeeUYlJSUpPz8/FRcXJy5CZDVXT58+re666y4VHh6uAOuLEFH1xWt11eZQ6gq+yU9EREREROXOtt85JyIiIiKqbjg5JyIiIiKyCU7OiYiIiIhsgpNzIiIiIiKb4OSciIiIiMgmKmxyPmfOHCQmJiIgIABdunRxa0hPVFUxr8kbMa/JGzGvqaqqkFaKH374Ie69917MnTsXXbp0wcyZM/Hxxx9j9+7diIqKuuS2xcXFOHr0KEJCQq54yWIiiVIKubm5iI2NhY+P53+XMq/JjpjX5I2Y1+SNLOd1RTRP79y5s9sKk0VFRSo2NlZNnTr1stseOnTIuPoTb7yVx+3QoUPMa9687sa85s0bb8xr3rzxdrm8roFyVlhYiC1btmDy5MmumI+PD/r164cNGzZcdvuQkJDyPiTbSE1N1WInT54UxxYXF2uxoKAgLbZ3715x+/DwcC3m5+cnjs3Ly9NiERERWuznn38Wtx89erQYt6sryTHmNdkd87ride/eXYz36tVLi9WsWVOLOZ1OcfucnBwtdvjwYXHse++9p8WUF68lyLwmb3S5HCv3yXlmZiaKiooQHR3tFo+OjsauXbu08QUFBSgoKHD9nJubW96HZBuhoaFarLCwUBwrTc6Dg4O1mDRhN401Tc6tbi+92VRFV/LPlMxrsjvmdcWrUUN+ywwICLjiGCC/D/j7+4tjpdfZmyfnzGvyRpfL60rv1jJ16lSEhYW5bnFxcZV9SERlxrwmb8S8Jm/EvCa7KffJee3ateHr64v09HS3eHp6OmJiYrTxkydPRnZ2tut26NCh8j4kojJjXpM3Yl6TN2JeU1VX7l9r8ff3R4cOHZCSkoJhw4YBuPAVjZSUFIwbN04b73Q6jd/DsxtP/jmxadOmWiwyMlKLnTlzRtze19dXi0nPU5s2bcTtjx07psVM/0xat25dLSb9s16jRo3E7asDb85rqr68Oa/L+vWPI0eOaDHpugzIX0OUOjGYvoZ44sQJy/t66623tFiPHj202Lp168TtqwNvzmuqHsp9cg4AEydOxMiRI9GxY0d07twZM2fORF5eXpUrHCS6GPOavBHzmrwR85qqsgqZnN9+++04fvw4nn76aaSlpaFdu3ZYsWKFVpxBVJUwr8kbMa/JGzGvqSqrkEWIyiInJwdhYWGVfRiisn6tRaoSN7XLsvq1FumfUwH5ay2mf7aT4tLXWkydZTp06CDG7So7O1vsnFOR7JzX5B2Y1+689Wst9evX12Le/LUW5jV5o8vldaV3ayEiIiIiogs4OSciIiIisokK+c65t/Lkn0TfeecdLXb06FEtZmrZJP2TrLQwkKkDi9QFxvTPpNLXVaTHatoXEVF5kxZNO3funOXtpevVxQvNXGzUqFFaTPq6n/R1QUD+uoq0rwMHDojbS9d204JF0krT33zzjRbzZNE56Ss4gPlrk0RUsfjJORERERGRTXByTkRERERkE5ycExERERHZBCfnREREREQ2wYLQCtK9e3cttnfvXi0WGRlp+T5NRTsSqRjKVCB0/vx5SzGpvy4RUUWQij896V1uKv6UJCQkaLHs7GwtFh4eLm4fEhKixaQ+2aZjzc/P12LSNdgU3759uzjWKhZ+EtkLPzknIiIiIrIJTs6JiIiIiGyCk3MiIiIiIpvg5JyIiIiIyCY4OSciIiIisgl2aymjDh06iPETJ05oManKXuo+AAC+vr5aTOoUUFRUJG5vilsdW6OGnhqmTgXS0tV5eXmW909EZIWp24lEui7NmjVLHDtkyBAtdujQIS0WGxsrbh8YGKjFFi1apMWkDjAAcOutt2oxUyev33//XYs5nU4ttmbNGnH7v/3tb1ps3bp14liJJx1ziOjK8JNzIiIiIiKb4OSciIiIiMgmODknIiIiIrIJTs6JiIiIiGyCBaFl1LlzZzHu5+enxaTlpCMiIsTt/f39tZhUuOnjI/99FRoaKsYl0rGalo6WSMVILAglorKQiuKla6CpcFIqcqxTp4449tixY1pMuoZlZGSI20v3u2vXLi32888/i9vfeeedWuzUqVPi2LNnz2ox6Rper149cfvPPvtMi40ePdryWGlfhYWF4vZEdGX4yTkRERERkU1wck5EREREZBOcnBMRERER2QQn50RERERENsGC0DK66aabxLjV1UBzcnLE7aUV52rWrGn5uKQVPouLi8Wx0upupkJTiekxEBFdKaurHI8ZM0aMBwQEaLH09HTL+5cK3aViTEAu/rz++uu1WJ8+fcTtpWvw/v37xbFSQaZUPGsq0jx58qQWGzt2rDhWKghl8SdRxeMn50RERERENsHJORERERGRTXByTkRERERkE5ycExERERHZBCfnREREREQ2wW4tZRQXFyfGz507p8U86YAiVeRHRERoMamjAACcOHFCi0kdZAC5s4vUWcbU6cB0v1R5PMk1qVOEFLua/vCHP4hxqWPRd999Z/l+pbw2kZ4D6VwBrJ8DISEhYjw3N9fycZG7O++8U4zn5+drMVMHGOn1k2L+/v7i9tL1PigoSIs1btxY3F66tppyVXofkB5XXl6euL00NiYmRhxrlel6Y+oQRkSXxk/OiYiIiIhsgpNzIiIiIiKb4OSciIiIiMgmODknIiIiIrIJFoSWUWJiohjPzs7WYlIxm1RIBMhLT0vLJs+cOVPc/vHHH9dihw4dEsdKhUfSsW7evFncnuznahZiSfljKiiViuTuu+8+LWYqUDt48KAWa926tTj27bff1mKeLD0uFX+aCj/r1aunxV577TUtlpWVJW6/Z88eLfavf/1LHLt3714xXh14kmvSkvamglBT8WRppgL806dPWxp74MABcXvpMdSpU0ccKx2rVFBsKl6VhIWFifE+ffposW+++cby/RKVN1OhdFmbGKSkpGixd999Vxz73nvvlWlfVvCTcyIiIiIim+DknIiIiIjIJjg5JyIiIiKyCU7OiYiIiIhsgpNzIiIiIiKbYLcWD1jtagIAGRkZlu7TVGEcFRWlxR555BEt9uabb4rbS91aPFl6XOp0sHPnTnF7qlxWO1hUVJW7J9ufOXNGi0mdifz8/MTtT548qcVq1aoljn311Ve12HPPPafFjhw5Im4vnRfNmjWzvK/o6GgttnjxYnH7yMhILXbNNdeIY6tzt5amTZtqscDAQHGslJemDibSfUjXQFMXJKkLkbQvKdcBoKCgQIuZOgvl5ORoMemxhoSEiNtLnWVM51uPHj20GLu10NXiSccsyXXXXSfGlyxZosUyMzO1mNRJDAA++eQTLSadV4B+HVFKWeqmxk/OiYiIiIhsgpNzIiIiIiKb8HhyvnbtWgwZMgSxsbFwOBxYunSp2++VUnj66adRt25dBAYGol+/fuICG0R2wrwmb8OcJm/EvKbqwOPJeV5eHtq2bYs5c+aIv58+fTpee+01zJ07F99//z2CgoIwcOBAnD17tswHS1RRmNfkbZjT5I2Y11QdeFwQOmjQIAwaNEj8nVIKM2fOxJNPPombbroJwIVlTqOjo7F06VLccccdZTvaStahQwfLY8+dO6fFpCKApKQkcXup6OaNN96wvH9PWC0e3L59e4Xs3w6qcl5bLcgsa+Fnebj22mu12NChQ7WYVGAJALfddpsWW7t2rThWKiZ6/vnntZipwG3r1q1a7E9/+pM49uDBg5b21aRJE3H7evXqaTFT8alVVTmnTVq3bq3FnE6nOFYqsjQVxUuka7jpHJKul/n5+VrMVDQmkQpSAcDHR/9MTYpJxw/Ix2oqUOvdu7cWk4qqrRS4lRdvzGuSScWfpqLuJ598Uovdf//94th169ZpsezsbC3Wv39/cftp06ZpseTkZHFs6XOzUgpCU1NTkZaWhn79+rliYWFh6NKlCzZs2FCeuyK6apjX5G2Y0+SNmNfkLcq1lWJaWhoAvYVYdHS063elFRQUuH3CIbWJIqpMzGvyNleS0wDzmuyNeU3eotK7tUydOhVhYWGuW1xcXGUfElGZMa/JGzGvyRsxr8luynVyHhMTAwBIT093i6enp7t+V9rkyZORnZ3tuh06dKg8D4mozJjX5G2uJKcB5jXZG/OavEW5fq0lKSkJMTExSElJQbt27QBc+Oeh77//Hg8//LC4jdPpNBb02E3Hjh0tj5UKGYqKirSYqbhh4MCBlvZjWqFUIhUNAXKBkFTZXl2/s1eReW1atdOTsVKRm7S6oLS6IgCEh4drMVOhslQQ9+GHH4pjJVLRjnT8I0eOFLeXCnGkYkpALqg7fvy4FuvcubO4fZcuXbTYF198IY6VVn4cNmyYFjOdr9JzYBpbOg+upND3SnIaqPzrdadOnbSYqbhKut6ZrrfSCplSzFRkKRV/Sq+paf/Sayi9XwDWV9Q1Xe+tXi8AcwGzXVXVvLZa5GtiOgeuZqGuxOrq1SZSA4B3331XHLtjxw4tduDAAXGstCKw9D749ttvi9tPmjRJjEtKzwWtPn6PJ+enT592Wz46NTUVW7duRWRkJOLj4zFhwgQ899xzaNy4MZKSkvDUU08hNjZWfKMisgvmNXkb5jR5I+Y1VQceT843b96Mvn37un6eOHEigAufdM2fPx+TJk1CXl4eHnjgAWRlZaFHjx5YsWKF+Nc+kV0wr8nbMKfJGzGvqTrweHLep0+fS34s73A48Oyzz+LZZ58t04ERXU3Ma/I2zGnyRsxrqg4qvVsLERERERFdwMk5EREREZFNlGu3Fm+XmJioxUzV0FKlfXBwsBb79ttvxe1N1fOlnTlzxtI4wNztQ4rXrl1bi+3atcvyvsjs4ufb9JpI/2xr6vQgddaRcrVbt27i9rm5uZbuEwBatmypxVq1aqXFGjRoIG4vPa4XX3xRiz3yyCPi9pMnT9Zips4yzZs312JS9f2+ffvE7aXWawMGDBDHStX/UreVU6dOidtLXUBM3VpCQkLcflZKia+hN6pbt64WkzpjAfK1uVatWuLYixegKSHlqmlfUmcgqduGqQOLxNStQzou6XyNiooSt8/KytJipvcxqbtRdeZJdy3TV2+kXJHy4mp2WvEk16RuP6YuRp50Zvnss8+0WOvWrbWYaR6Sl5enxUzvmb1799ZiM2bM0GKedGUpb/zknIiIiIjIJjg5JyIiIiKyCU7OiYiIiIhsgpNzIiIiIiKbYEGoB6Slo0+cOCGOlYo5pEUQ3nrrrbIfmEAqcPKkmOX06dPleThk4EnBjKlIUyIVsuzcuVMc+8MPP2gxqWgJkIskb731Vi1mKjB66aWXtFidOnW02O+//y5uP3jwYC22bNkyceyECRO0mFSo2qVLF3F76RhMha7S45WKNE1LhJcu8gTkJeGlfXmSQ1VdbGysFjMVz0uvybFjx8Sx6enpWkwqNDYV3kpFclLxqJ+fn7i99BqartdSY4GMjAwtduTIEXF76XwzPS4pL6Ojo7WY9Px5o/I416wWBUvXOgAYPny4FpNyAgBefvllLfb9999rMU+KT03Fn5JHH31Ui0mFlwCwY8cOLSbldVhYmLi9NL8yPS+33HKLFluyZIk4tqyuNGf4yTkRERERkU1wck5EREREZBOcnBMRERER2QQn50RERERENsGCUA/Ex8drMWlVKkAu/JKK6SqqCCE7O9vyWKmYKS0trTwPh/4/h8PhVuhlKsSRCrFMRVs333yzFqtXr54WM+XE1KlTtVhERIQ49ptvvtFiUtHR0KFDxe2lxyAVqE2cOFHc/qmnntJiffr0EcdKhVdHjx7VYqbXQFoNVTpXTPfRqFEjLWYqSHz33Xe12Keffmp5X9WFdA02FeU3btxYi5mut9LKre3bt9dihw4dEreXijelglRPivJNhYNS4Zu0kufmzZvF7adMmaLFfv75Z3GstMKitEqrtxaElr5em0ivtWk12bi4OC32+uuvazGpqQMgX8NMzQKefvppLbZ7924tJuUEAISHh2uxESNGaLE//elP4vbSe84999wjjpWKRxMSErSY6Xxv1qyZFjOtir1p0yYxbif85JyIiIiIyCY4OSciIiIisglOzomIiIiIbIKTcyIiIiIim+DknIiIiIjIJtitxQPSkvaRkZHiWKlbi1RlfebMmbIfmECq3pYq7wG50vzXX38t92OiC0v5WlnO19SZRSJ1FZG6L+zcuVPcXspBU2eXNWvWWBpr6nDQunVrLSYtJ/23v/1N3L5r165azNS9xGoHCan7BSAvtW7qoCCd7y+88IIWM3VgkZiew9Lnq9Wc8gZSFyGpUwkgL9198uRJcax0vvn6+mox6VoJmLv4WCW9ftL+Pdl+7dq14lgpr0ydRaRjkK43W7duvcwRVk2ln1fTeeZJByWp49TKlSu12GuvvSZu36NHDy02fPhwcWxiYqIWk7pIPfjgg+L20rmVmpqqxd566y1x+8OHD2sx0zV09erVWkzqDNS9e3dx+71792qxffv2iWNbtWqlxWrWrKnFrrvuOnH7+vXrazGpCw8AjB492u3n4uJisTtUafzknIiIiIjIJjg5JyIiIiKyCU7OiYiIiIhsgpNzIiIiIiKbYEGoB6TiAlMRQH5+vhYzFd1UBGmJW9OxSgVCBw8eLPdjIqB58+ZuRVbSEuMAkJKSosVycnLEsdu2bdNiUtHPb7/9Jm6/ePFiMS4JCwvTYh06dNBi0hLRgFy4FBQUpMVMBamfffaZFpOKMQF5qXdpOWjpXAXkAnBTQdgPP/ygxTwp/pQKDU1FZtWl+FMSGBhoeaz0nGZmZopjY2JitFhRUZEWMxXpSnHp9TMVeXry+p87d06LSYVz0jgTU0GrdAzt2rXTYgsXLrS8r6rm4vOtTp064hjpGrZ//35x7HfffafF7r//fi3Wv39/cfuOHTtqsbS0NHHsv/71Ly0mFXlKxe+AXFQdEhKixaQCSwC4/vrrLe/rp59+shQzFXlKpGYDgPyeKb1ndenSRdxeOgapUBYAmjZt6vbz+fPnsWnTJnHsxfjJORERERGRTXByTkRERERkE5ycExERERHZBCfnREREREQ2wck5EREREZFNsFuLBzZs2KDFevbsKY6VOiqYKv0rglRVbuoWIy1fLlVpm0iPqzp3lLiUXr16uXUXkSrvAeCOO+7QYqZOE9LrJ3U7+ctf/iJu/8wzz2ix5s2bi2Olrg7Scsz16tUTt//999+1mNVlmwFg6NChWkzqlADIxyp1vJG6sgBAXl6eGJdER0drMalbw65du8Ttjxw5osWaNWsmjn322Wfdfj5//rzY3aeqi42N1WJSrpuuq2fPntViplyROvtkZWVpMVO3lYq43kkdXADg5MmTWkzqNlO6S0QJqZOXJ48rKSlJHOuNkpKS3F6HcePGieOk7huRkZHiWOl1kZZzN3XbWb58uRYzdQqROr5I12vp+gXI55bUrcXUQUV6XKYuRFJ3pIYNG2oxU3cu6byQnitAngtJnaC+/fZbcXvpWJs0aSKOLf0+YHr8pfGTcyIiIiIim+DknIiIiIjIJjg5JyIiIiKyCU7OiYiIiIhsggWhHpCWtPdkOWdT0U1FkJYkDw0Ntbx9YWFheR4O/X9vvvmmpXEDBgzQYtKy2QBw8803azGpGM5UEPz8889rMVOBUe3atbWYVPwpFaQCQIMGDbTYn//8Zy0mFS0BQM2aNbWYv7+/OPbnn3/WYlKRn6nAzVQoKpHuV1rSfPPmzeL2GRkZWsxUUFa6+MpqgVFVU79+fS0mFWKZCielZcrvvfdecayUr1LxcEUVhEqPSyp+BeTXWypeNhUvSoWGpuOXrhmmYm9vFBoa6vaaSzkJyM+pqVmEVDwpXatM13vpffz+++8Xx/76669aTHoPMu1LKlSXCicTExPF7aVCZalIFJCfQ+m8NL03SEzzM+kcks5t6X0UANq3b6/Fdu7cKY49evTopQ7RiJ+cExERERHZBCfnREREREQ2wck5EREREZFNcHJORERERGQTLAj1wHfffafFTMUJUoGNVKBUUTwp/pQKqqTVtqjsWrVq5VZ4Iq22BgBr1qzRYl999ZU4dvr06Zb2bSqcCw8P12Km1c6klTulFdvatm1r6ZhMx2U6r6QCJdO+pJUQt2/fbmkcAOzYsUOLHThwQBxb1nObq+zqpFWOpefEVLQlFVmaCvqkgl6paMy0OqF0XJ4U6krngKmYzc/PT4tJxcthYWHi9tLKuaZ9Sc+htJqqt9q2bZvbzw888IA4Tirg79Chgzi2R48eWkwqhjQVhP/2229a7NNPPxXHSoWanTp10mKm94abbrrJ0ljT9Voq6DQV8EvFx7Vq1dJiUk4C8vlmelzS8UrnduPGjcXtpWLx5557Thx7pfjJORERERGRTXByTkRERERkE5ycExERERHZhEeT86lTp6JTp04ICQlBVFQUhg0bht27d7uNOXv2LJKTk1GrVi0EBwdj+PDhSE9PL9eDJipPzGvyRsxr8kbMa6oOPJqcr1mzBsnJydi4cSNWrlyJc+fOYcCAAW6rkz366KP4/PPP8fHHH2PNmjU4evQobrnllnI/cKLywrwmb8S8Jm/EvKbqwKHK0BLg+PHjiIqKwpo1a9CrVy9kZ2ejTp06WLRoEUaMGAEA2LVrF5o3b44NGzaga9eul73PnJwcY5W5HZmePqmrg1SN3KhRo3I/JkDuPmDqaiBVKc+ePVuLPfXUU+L2Va3TRHZ29iW72VRkXjscDrfnq2/fvuJ4aTllUwedwsJCLbZlyxYtVvrTpRJ2fq2qM1MHjdJKXr/KzOuKMHLkSC32v//7v1ps/fr14vZJSUlazNRtReoCJHVFMb0mnnRbkUgdKAICAsSx+fn5lu5TOn5A7gIivV8A8ntW06ZNtZjUAaa8eFteS6KiorRYbGysOFbK61atWlnel9Sdy/T6/fLLL1rMarcgQH5vqVmz5mWO8L+kY5Xe7wC5s4upE5f03Ernm9TdCwDmzp0rxiWlrwNWr9dl+s55SZuckpN9y5YtOHfuHPr16+ca06xZM8THx2PDhg3ifRQUFCAnJ8ftRlSZmNfkjZjX5I2Y1+SNrnhyXlxcjAkTJuCaa65x/dWWlpYGf39/7a+d6OhopKWlifczdepUhIWFuW5xcXFXekhEZca8Jm/EvCZvxLwmb3XFk/Pk5GTs2LEDixcvLtMBTJ48GdnZ2a7boUOHynR/RGXBvCZvxLwmb8S8Jm91RV8UGzduHJYtW4a1a9e6rbgWExODwsJCZGVluf3Vmp6ejpiYGPG+nE6n8XuARFcT85q8EfOavBHzmryZR5NzpRTGjx+PJUuW4JtvvtEKEzp06AA/Pz+kpKRg+PDhAC4UoR08eBDdunUrv6O2kb1794pxqcBCKoSIjo4Wty9r2ydPivzKuvR0VXc181op5fZ8p6SkiOOkuK+vrzhWWr6+efPmWuzhhx8Wt5eWWM7NzRXHSkXFUq6a8i8zM1OL1atXz9J+ACAwMFCLmQrvpGWipbGmwi+pQEh6rkzHJT0GUzGUtL3p07sPP/zQ7WellFiQVdWv19JzJS1zbjovpBw05aVpSfHSTMuBm+JWeVJQKj1eaXupmNMUN01MpeI7af+mwraK+O52Vc9rSUZGhqUYAGzdulWLLVmypLwPicrJlTZc8GhynpycjEWLFuHTTz9FSEiI6/tbYWFhCAwMRFhYGMaMGYOJEyciMjISoaGhGD9+PLp162apQpqoMjCvyRsxr8kbMa+pOvBocv7GG28AAPr06eMWnzdvHkaNGgUAeOWVV+Dj44Phw4ejoKAAAwcOxOuvv14uB0tUEZjX5I2Y1+SNmNdUHXj8tZbLCQgIwJw5czBnzpwrPiiiq4l5Td6IeU3eiHlN1UHZvihHRERERETlhpNzIiIiIiKbqLg1d6sJU0W8VP0vxUydIsrarUWqqDf9c6DUFcC0dDRVHmmJbwD48ccfLcVY0U9VSXBwsBbzpIuUJ9cwqbuWdG03dVCRrq3S+Voe7fqkYzB1rJGUrKR5MVNXFavdVtq1ayfG165da/m4iOi/+Mk5EREREZFNcHJORERERGQTnJwTEREREdkEJ+dERERERDbBgtAyql+/vhjPysrSYlIhj5+fX3kfEgC5wMlUTOXJ0s9ERFeDdA2TrlUhISHi9tL1zlQkKu1LYiq8LGuRpsRUwC/d79mzZ7VYQkKCuP3333+vxRo2bCiOlZoYSM0KoqKixO2J6Mrwk3MiIiIiIpvg5JyIiIiIyCY4OSciIiIisglOzomIiIiIbIIFoWUkFX4CcuHS1VyJc8+ePVpMWhkOkI+rsLCw3I+JiMiqiIgILXbkyBEtZlpl+T//+Y8WkwonAWDcuHFabOvWrVrMVDhqtbDfVOTpycqn0sqjUpFoaGiouH2/fv202Pr168WxMTExWkx6b6tVq5a4PRFdGX5yTkRERERkE5ycExERERHZBCfnREREREQ2wck5EREREZFNcHJORERERGQT7NZSRqdOnRLjUqeB8+fPa7G6deuW+zEBcgcWT0gV+Z7sy9SVgIjIisaNG2sx6boUGBgobi91Zhk/frw4VurWEhcXp8Xy8/PF7aXuVtL13nRdlbqtSDEAqFmzphYLDw/XYvPnzxe3l45r+/bt4tjExEQxbuWYiOjK8ZNzIiIiIiKb4OSciIiIiMgmODknIiIiIrIJTs6JiIiIiGyCBaFlZCq8jIyM1GL+/v5arHXr1uL2y5YtK9NxScVEpqWnpbgnBaFEROVNKsiUlq4/d+6cuP2PP/5oeV9SQePs2bO1WK9evcTtpcLJ/fv3azFPrqvSYwWAtLQ0LfbYY49pscWLF1ve16xZs8T49ddfr8WkAtwWLVpY3hcRXR5nYERERERENsHJORERERGRTXByTkRERERkE5ycExERERHZBCfnREREREQ2wW4tZbRo0SIx3r59ey2WmZmpxVauXFnuxwQA2dnZWszUKSA3N1eL7dixw/K+lFLWD4yIyIKOHTtqMak7ltPpFLc/e/as5X3l5+drsTFjxlje3io/Pz8xHhISosWkazhg7uJSFlu3bhXjjRs31mJhYWFa7NixY+V9SETVGj85JyIiIiKyCU7OiYiIiIhsgpNzIiIiIiKbsN13zqva95cLCgrE+JkzZ7SY9L3Givj+IADk5eVpMdP3HaXVRE2r7kmq2mtWGcdb1Z4jqnqqQ15L+6tK55bpWO36uKRjKC4urvRj8MZ9UvVyuRyz3eRcKk60sxkzZlT2IYj69+9f2YdgW7m5uWJRU0Xvk6giVde8Nn1AYkfnz58X46dOnbrKR2KN9IGSFKtI1TWvybtdLq8dymZ/IhYXF+Po0aMICQlBbm4u4uLicOjQIYSGhlb2oZWbnJwcPq5KoJRCbm4uYmNjjZ1rKgrzuuqy++NiXlcsu7/+V8ruj4t5XbHs/vpfKbs/Lqt5bbtPzn18fFC/fn0A/22bFRoaassnuaz4uK6+q/0JTAnmddVn58fFvK54fFxXH/O64vFxXX1W8poFoURERERENsHJORERERGRTdh6cu50OjFlyhTjCnBVFR9X9eatzxMfV/Xmrc8TH1f15q3PEx+XvdmuIJSIiIiIqLqy9SfnRERERETVCSfnNuRwOPD3v//d9fP8+fPhcDiwf//+SjsmIjvZv38/HA4H/vnPf1b2oVA1x+s1eTuHw4Fx48Zddhxzv/xwcl4OShKy5BYQEIAmTZpg3LhxSE9Pr+zDI7oi27dvx4gRI5CQkICAgADUq1cP/fv3x6xZsyr70IiuGK/XRP9Vmdf5F154AUuXLq3w/VRFtp6cz5kzB4mJiQgICECXLl2wadOmyj6kS3r22WexYMECzJ49G927d8cbb7yBBg0aoG7dunA4HFoSKqXw9NNPo27duggMDES/fv2wZ8+eyjl4i6ZOnYpOnTohJCQEUVFRGDZsGHbv3u025uzZs0hOTkatWrUQHByM4cOH803v/6sqOb1+/Xp07NgR27Ztw9ixYzF79mzcf//98PHxwbRp0zBkyBDExsYyr5nXAKpOXl+M1+sLmNdmVTGvL7Z27dpLXqvXrVuH9u3bY+nSpTh69CiSkpJw8803w8fHB6+++qrH+7vnnnuQn5+PhIQES+OvdHJeLfJa2dTixYuVv7+/euedd9TOnTvV2LFjVXh4uEpPT6/sQ9PMmzdPAVA//PCDW/zmm29WANSjjz6qAKglS5a4/f7FF19UYWFhaunSpWrbtm1q6NChKikpSQFQU6ZM0e4/NTW14h/MZQwcOFDNmzdP7dixQ23dulXdcMMNqn79+ur06dOuMQ899JCKi4tTKSkpavPmzapr166qe/fulXjU9lCVcvqGG25QderUUadOndJ+t2jRIvXEE0+oTz75xKO8zs/PL7fjS01NVQDUSy+9VC73x7y+clUpr5Xi9Zp5bU1Vy2vJF198cclrddOmTZXD4VDvv/++dq2++HECUMnJyeVyTMXFxerMmTNKKaWCgoLUyJEjPb4PKa/j4+O9Kq9tOznv3LmzWzIUFRWp2NhYNXXq1Eo8KpnpYr9s2TIFQD3//PMKgGrZsqXrd8XFxSomJkZ16NBBJSQkKKWUysrKUk6n0/LFfs6cOapFixbK399f1a1bVz3yyCNuk6nk5GQVFBSk8vLytGO+4447VHR0tDp//rwr9sUXX6gePXqomjVrquDgYHXDDTeoHTt2uG03cuRIFRQUpPbu3asGDRqkgoKCFAC1Zs0a12Pw8/NTH3/8sWubX3/9VQFQGzZssPR8equqlNNNmzZVffr0uew4AGrQoEFqyZIlqmXLlsrf31/VqFFDjRkzxjWmJK/nzJmjRo8eraKiopS/v79q0aKFevvtt93ur6CgQD311FPqD3/4gwoNDVU1a9ZUPXr0UKtWrXIbJ03Oi4uL1dixY5Wfn5/697//7YovWLBA/eEPf1ABAQEqIiJC3X777ergwYNu99e7d2/VsmVLtXnzZtWzZ08VGBjIvLaoKuW1UrxeM6+tqWp5fTmlJ+fFxcXK19dXNWzY0BUryekPPvhA2zY5OdntOt+iRQu1fPlyt3FS7ickJKjBgwerFStWqA4dOiin06leeeUVBUC7XclEXSmlMjIyvC6vbfm1lsLCQmzZsgX9+vVzxXx8fNCvXz9s2LChEo/MM/v27QMA1KpVS/tdamoq0tLSEBsb64qFhYWhS5culu7773//O5KTkxEbG4uXX34Zw4cPx5tvvokBAwbg3LlzAIDbb78deXl5+M9//uO27ZkzZ/D5559jxIgR8PX1BQAsWLAAgwcPRnBwMKZNm4annnoKv/zyC3r06KEVd5w/fx4DBw5EVFQU/vrXvwIAIiMjAQBbtmzBuXPn3F67Zs2aIT4+vkq9duWtquV0QkICtmzZgh07dlx27K+//opHHnkEd9xxByZNmoTz589j4cKFOHHiBIALed2+fXv89a9/xddff41x48bh1VdfRaNGjTBmzBjMnDnTdV85OTn4v//7P/Tp0wfTpk3D3//+dxw/fhwDBw7E1q1bjcdQVFSEUaNG4b333sOSJUtwyy23AACef/553HvvvWjcuDFmzJiBCRMmICUlBb169UJWVpbbfZw4cQKDBg1Cu3bt8Le//Q0A8/pyqlpeXwqv1xcwr70rr01SU1NRVFSEtLQ013W+JKelx/jdd9+5rvPTp0/H2bNnMXz4cNd1/lJ2796NO++8E/3798err76Kdu3aYcGCBXA6nejZsycWLFiABQsW4MEHH7yix5KdnQ3Ay/K6sv86kBw5ckQBUOvXr3eL/+Uvf1GdO3eupKMyK/lr8euvv1bHjx9Xhw4dUosXL1a1atVSgYGB6vDhw9onMevWrVMA1G233eb6JEYppW699dbLfhKTkZGh/P391YABA1RRUZFr3OzZsxUA9c477yilLvxlXK9ePTV8+HC34/3oo48UALV27VqllFK5ubkqPDxcjR071m1cWlqaCgsLc4uPHDlSAVCPP/64KioqUoMHD1bXXHON6/cLFy5U/v7+2nPUqVMnNWnSJIvPqPepajn91VdfKV9fX+Xr66u6deumJk2apL788ktVWFjoNg6AqlGjhtq7d69S6r95DUDNmjXLNS4pKUkFBASozMxMt+3vuOMOFRYW5vpnzvPnz6uCggK3MadOnVLR0dHqvvvuc8Uu/uT83Llz6vbbb1eBgYHqyy+/dI3Zv3+/8vX1Vc8//7zb/W3fvl3VqFHDLd67d28FQM2dO5d57YGqltdK8XrNvL68qpjXl4NSn5yX5HTp63zPnj3ViBEjtG39/f1d13mllNq2bZt2nTd9cg5ArVixQjumK/1ay8W8Na9t+cl5VdWvXz/UqVMHcXFxuOOOOxAcHIwlS5agXr165bqfr7/+GoWFhZgwYQJ8fP77Eo4dOxahoaGuT14cDgduvfVWfPHFFzh9+rRr3Icffoh69eqhR48eAICVK1ciKysLd955JzIzM103X19fdOnSBatXr9aO4eGHH0ZycjJ27NiBxYsXl+vjo8rXv39/bNiwAUOHDsW2bdswffp0DBw4EPXq1cNnn33mNrZt27Zo2LChWywkJAS///47gAuFdIcPH0ZsbCyUUm45NnDgQGRnZ+PHH38EAPj6+sLf3x8AUFxcjJMnT+L8+fPo2LGja8zFCgsLceutt2LZsmX44osvMGDAANfvPvnkExQXF+O2225z22dMTAwaN26s5bXT6cTo0aOZ19UEr9dEwOeff+52nf/222/x+eefa9f5fv36uV3n27Rpg9DQUNd1/lKSkpIwcODAcj92AF6b1zUq+wAktWvXhq+vr1ZZm56ejpiYmEo6qsubM2cOmjRpgho1aiA6OhpNmzZ1uxhfrORx5Ofnu8WtVBMfOHAAANC0aVO3uL+/Pxo0aOD6PXDhn0pnzpyJzz77DHfddRdOnz6NL774Ag8++CAcDgcAuDoOXHvtteL+QkND3X6uUaMGpk2bhmXLlmHt2rWoX7++2+MqLCxEVlYWwsPD3R6XnV+7ilYVc7pTp0745JNPUFhYiG3btmHJkiV45ZVXMGLECGzduhUtWrQAcOGxlSh5LMHBwTh16hQA4Pjx4zh37hx+//131KlTR9xXRkaG6//fffddvPzyy9i1a5frn/yBCxf40qZOnYrTp09j+fLl6NOnj9vv9uzZA6UUGjduLO7Tz8/P7ed69eph4sSJzGsPVMW8LsHrNfPapCrntVUlj6Nu3bpu1/nhw4fj6NGj2nU+Pj5eu4+IiAjXdf5SpGt3eRg3bpzX5rUtJ+f+/v7o0KEDUlJSMGzYMAAXPkVLSUmx1Ai/snTu3BkdO3a0NDYpKQkxMTE4cuSIK5aTk4Pvv/++XI+pa9euSExMxEcffYS77roLn3/+OfLz83H77be7xhQXFwO48D1GKXFr1NDTZOnSpfjmm2+0k65Dhw7w8/NDSkoKhg8fDuDC980OHjyIbt26ledDq1Kqak4DF469U6dO6NSpE5o0aYLRo0fj448/xpQpUwDAbUJTktdnz56FUgrAf78P2KNHD9c2pbVp0wYA8P7772PUqFEYNmwY/vKXvyAqKgq+vr6YOnWq6zvBFxs4cCBWrFiB6dOno0+fPggICHD9rri4GA6HA8uXL3d9V/diwcHBbj9nZWVhyZIlzGsPVOW85vWaeW1SlfPaqpKcTklJQbt27eDv74+mTZsiIyMDY8eOxdy5c92u89I1FIDrOn8pgYGB5XrsSimMHz/eq6/XtpycA8DEiRMxcuRIdOzYEZ07d8bMmTORl5eH0aNHV/ahWXb69Gns3bvX9fOpU6ewdetWREZGIj4+HhMmTMATTzyBWrVqYfv27XjqqacQGxuL1NTUS95vSQ/R3bt3o0GDBq54YWEhUlNT3YogAOC2227Dq6++ipycHHz44YdITExE165dXb8v+aeqqKgobdvSNm7ciPPnz2PRokUICQlBWloagAuFJIGBgQgLC8OYMWMwceJEREZGIjQ0FOPHj0e3bt3c9lkdeUNOl0xmtm/f7irQzMnJ0fJ68uTJOHToELZv344nn3wSDocDsbGxl82vf/3rX2jQoAE++eQT1yeFAIyT+q5du+Khhx7CjTfeiFtvvRVLlixxTU4aNmwIpRSSkpLQpEmTS+53z549yMrKwqpVq5jXHvKGvAZ4vWZeu/OGvC6d06mpqVpOP/fcc2jcuDGSkpJcOX3//fdj7ty5OHbsWIUe38XXeE8kJydj0aJF+PTTT703ryvx++6XNWvWLBUfH6/8/f1V586d1caNGyv7kESm1lyrV6++ZLugn376STkcDuXj46OcTqe67rrr1O7duy0XGF1//fWquLjYNe711193KzAqsWXLFgVAvfbaa8rpdGoFEdnZ2So0NFT17t1bK/gr2V8J6fEAUPPmzXONyc/PV4888oiKiIhQNWvWVDfffLM6duyYJ0+p16oqOb1q1Sq33Coxbdo0Yw6U5HVxcbEKCwtTAQEBrry+5ZZblL+/v9q+fbt2nxfn1y233KIaNGjgVji3ceNG5XA43ArxSrdSXLJkiapRo4a64447XNvu3btX+fr6qrvuukt7LMXFxW7Fqczrsqkqea0Ur9fMa+uqUl5LLpfTKSkp6sknn1TR0dFuOV1ynZ8xY4ZSytznPCEhwa2g81KtFCXR0dHqpptu8vhxVYe8tvXkvKowXewv9ssvvygfHx/Vvn17NXv2bPX000+rqKgo1bp1a7dJh1LKUt/cKVOmKABqwIABavbs2Wr8+PHK19dXderUSbxgN2rUSIWEhCgAasuWLdrvFy5cqHx8fFSrVq3Uc889p9588031xBNPqHbt2rmdlCV9c8m7tWzZUiUlJamJEyeqt956S82ePVvdddddytfXVyUmJrr6M1u9aKelpamEhARVs2ZN9ec//1m9+eabaurUqerWW29VERERrnHvvPOOAqCGDh2q3nzzTfX444+r8PBw1bJly0tOzpW60M/c4XCoBx54wBWbOnWqAqC6d++upk+frt544w01adIk1bhxY7dtS/qck/fj9ZrogvK+zns6Ob/hhhtUUFCQevnll9UHH3xQ5f74qUicnJcDKxd7pZR6//33VYMGDZS/v79q166d+vLLL9XIkSOv6GKv1IVWXM2aNVN+fn4qOjpaPfzww+KKjkop9cQTTygAqlGjRsbjW716tRo4cKDrU8+GDRuqUaNGqc2bN7vG8GJfPSxfvlzdd999qlmzZio4OFj5+/urRo0aqfHjx1taOa70RVsppdLT01VycrKKi4tTfn5+KiYmRl133XXqrbfeco0pLi5WL7zwgkpISFBOp1O1b99eLVu2TDtPTCuElnwa+T//8z+u2L///W/Vo0cPFRQUpIKCglSzZs1UcnKy2r17t2sMJ+fVB6/XRBeU93Xe08n5rl27VK9evVyLvpW1raI3cShl4dv8RERERERU4djnnIiIiIjIJjg5JyIiIiKyCU7OiYiIiIhsgpNzIiIiIiKb4OSciIiIiMgmKmxyPmfOHCQmJiIgIABdunTBpk2bKmpXRFcN85q8EfOavBHzmqqqCmml+OGHH+Lee+/F3Llz0aVLF8ycORMff/wxdu/ejaioqEtuW1xcjKNHjyIkJOSKl3YlkiilkJubi9jYWPj4eP53KfOa7Ih5Td6IeU3eyHJeV0Tz9M6dO7s1rC8qKlKxsbFq6tSpl9320KFDxqVZeeOtPG6HDh1iXvPmdTfmNW/eeGNe8+aNt8vldQ2Us8LCQmzZsgWTJ092xXx8fNCvXz9s2LDhstuHhISU9yFVqFq1aonxwYMHa7GcnBwtdvjwYcv7Onr0qBarUUN+Cf38/LRYcHCwOPaaa67RYuvWrdNi27Ztu9whVglXkmPVLa+p6mFel6/69etrMekaDFz4pLW8DR06VIx/9tln5b4vO2NeV55rr71Wi9WrV0+LnTt3Tty+ZcuWWmz+/Pni2H379nl2cFXc5XKs3CfnmZmZKCoqQnR0tFs8Ojoau3bt0sYXFBSgoKDA9XNubm55H1KFMv2zhL+/vxaTJsymybXVfZn27+vraykGAE6n0/JYb3Al/0xZ3fKaqh7mdfmSrq1X8ysO0vtFRSnr41IVuNA487rySPMTaW5jeo0CAgK0mCdzC+l+KzLXrqbL5XWld2uZOnUqwsLCXLe4uLjKPiSiMmNekzdiXpM3Yl6T3ZT7J+e1a9eGr68v0tPT3eLp6emIiYnRxk+ePBkTJ050/ZyTk1PpJ0arVq3EuPRVFdMn39JXSKSY6a/IU6dOabGL/7IvcebMGXH7sLAwLebJp/SDBg3SYqZ/Zt29e7cW++CDDyzvqyrwhrwmKs3OeV3WT83atWunxfLz88WxsbGxWuzDDz/UYnXq1BG3f+mll7TY8ePHtVjDhg3F7e+++24tZvpX0WHDhmmxRYsWabEHH3zQ8vam51V6f5LG2u3TTDvntR317t1bjL/88staTPpql+lfefr27avF+vXrJ45t3769FvMkr6TzpSK+bna1lPsn5/7+/ujQoQNSUlJcseLiYqSkpKBbt27aeKfTidDQULcbkd0wr8kbMa/JGzGvqaor90/OAWDixIkYOXIkOnbsiM6dO2PmzJnIy8vD6NGjK2J3RFcF85q8EfOavBHzmqqyCpmc33777Th+/DiefvpppKWloV27dlixYoVWnEFUlTCvyRsxr8kbMa+pKquQyTkAjBs3DuPGjauouyeqFMxr8kbMa/JGzGuqqiq9WwsREREREV1QYZ+cV2VDhgwR41Kl/2+//SaOlar/ExIStJjUlQWQK6KbNGmixU6cOCFuL3UKSEpKEsdKXWB27typxYKCgsTtpX8mlLq9AMDy5cu1mLdVWRNR2ZW1W8vXX3+txfbs2SOOla5BP/zwgxYzdVt59NFHLd2n6bp24MABLfbzzz+LY6UuIs8884yl/QPAwoULtZjULQYAioqKtBiXs/c+I0aMsDw2IyNDi2VmZopjmzdvrsV++eUXcezIkSO12Lvvvmv5uLxtzsBPzomIiIiIbIKTcyIiIiIim+DknIiIiIjIJjg5JyIiIiKyiWpfEFq/fn0tVqtWLXHs4cOHtZhU+AkAp0+f1mLZ2dmW9yUVdG7fvl0cK5GWKDaterZt2zYtVrNmTS2Wl5dnef+dOnUS41JBqLcVchBR2Vm9LsyaNUuMS0X1pgL8GjX0t0JpSfItW7aI20vFowEBAVrszJkz4vaSOnXqiPFz585psX379mkxf39/cfsOHTposb/+9a/i2GnTpmkxX19fLXb+/Hlxe7o6TAsr/eEPf9BiUgOH4OBgcfv9+/drsUaNGmmxdu3aidtLhaJRUVHi2DFjxmix4cOHa7GTJ0+K20+aNEmLScWrQNVoQsFPzomIiIiIbIKTcyIiIiIim+DknIiIiIjIJjg5JyIiIiKyCU7OiYiIiIhsotp3a5G6pZiqiXNycrSY1BEAkKvypYr6unXrittLXWRSU1O1mKnKWuoiY+r2YrWDgNTBBTB3gSEiqmjdunUT49LS86brpXQNlJapr127tri91K1E6m5lem+ROqCYukdI70NhYWFaTOo2A8jdLh566CFxrNStRXqs0nMFAEopMU5X7pprrtFid911lzi2sLBQixUUFGgxqXsJAAQGBmox6RySOhMBwG+//abFTF2IpA53ISEhWsw0Z5oyZYoWS05OFsfarTOLhJ+cExERERHZBCfnREREREQ2wck5EREREZFNcHJORERERGQT1b4gVCo4kAoTACApKUmLmZaHdTqdWkwqUJKWxwXkJXabNWumxUzH+vPPP1vaPyAfq1TMJC3bC8gFHgcOHBDHEhFdKekaFBkZKY7Nzs7WYqZroFTQKBUzmgrJpO2l6+K5c+fE7aUiS1NBp1SUmp6ersUaN24sbi8dg6nY3+oy5ywIvXpGjBihxY4cOSKOlQo1pdfK1BTixIkTWiw3N9fy9tK5mZmZKY61mmvHjx8Xt09MTNRiPXr0EMd+9913WszqNeBq4SfnREREREQ2wck5EREREZFNcHJORERERGQTnJwTEREREdlEtS8IlVabSktLE8dKK3N1795dHPvBBx9oMak4w7Qyl1SIkZWVJY6VSEU/pmKmGjX0NJAKPKSCWEA+ViKi8paQkKDFTMWIUrF/fn6+OFYqrJdiphVGg4KCtNjZs2e1mOl6LxV/SiuBAvJqoJ40C5CKYg8fPiyOla750krXLPwsf6YGDFKzCKn42USah5jOi4iICC127NgxLSatRArIK5WbikelJhTS3MR0DklF1Xfffbc4VioItVsO85NzIiIiIiKb4OSciIiIiMgmODknIiIiIrIJTs6JiIiIiGyCk3MiIiIiIpuo9t1apGpkUzXxrl27tFifPn3EsW+99ZYW8/X11WKm5aSl6n1pe6fTKW4fGBioxUwV2ampqVpM6nRgqh7/9ddftZjUUQCwvkQvEVFpf/jDHyyPlTpWRUVFiWOlziZSpwjTNVS6NkvXYKmjhClu6o4ljZXeR6T9A3IXGH9/f3Fsw4YNtZjUrcXUMcduHTCqkv79+4txqduKaR4hdXjzZB4hzYXCw8O1WEFBgbj9qVOntJh0XgLyPEDKS1NnGOk5CA0NFcdWBfzknIiIiIjIJjg5JyIiIiKyCU7OiYiIiIhsgpNzIiIiIiKbqPYFoTVr1tRipuKIjIwMLdamTRtx7LBhw7SYtMy9aYllqZBCKhI1kcaaCjpjYmK0mFSIIS2dDchFUqbCKxaEUllIy6QnJiaKY1u3bq3FFi9ebHlfZc1VqUiOBXJl06JFCy1mek6l108qhgOA2rVrazHpeu9Jobv0PiJd101jTe8NtWrV0mLHjx/XYqaC0szMTC1mel6aNWumxb766istxrwuf7179xbj0vutqfAxLy9Pi0lFolIDCAAIDg7WYlJBspT/gFz8aSqKlq6t0mM1FZ9KBdBSww9Azmup4Udl4ifnREREREQ2wck5EREREZFNcHJORERERGQTnJwTEREREdkEJ+dERERERDZR7bu1SEwdGaT4jh07xLFSlbBUfW/al1SRLHVgMVVJS9X3popu6X6lKm2pIwAgPwZT9b/UaSA9PV0cS9XX448/LsbvvvtuLXbo0CFxbMuWLbWYlGurV68Wt/ekM4uU76YltSVSt4t+/fqJY1NSUizfr7eJjY3VYqZOIVL3BtNYaUlyqTPL0aNHxe2l7lZSpwrT0uPSkuxSFy1A7rYiPVapAw0AHDx40PJxtWvXToyXxm4t5c/0fi29D0td5wA5ByVSVxdAzmurHWAAoHHjxlosNzdXHGvqkleaac4jXa9NYzt27KjF2K2FiIiIiIhEnJwTEREREdkEJ+dERERERDbh8eR87dq1GDJkCGJjY+FwOLB06VK33yul8PTTT6Nu3boIDAxEv379sGfPnvI6XqIKwbwmb8OcJm/EvKbqwOOC0Ly8PLRt2xb33XcfbrnlFu3306dPx2uvvYZ3330XSUlJeOqppzBw4ED88ssvYtFLZZMKFNPS0sSxUnHEypUrxbFSMY+poFIiFXJYLRIFgBo19Jd237594ljpPs6cOaPFvvvuO3F7qfjVVAxnOt7K5m15XVbS0vNA2Qu/6tSpo8XWrVunxaRCIgB48skntVh8fLw4VioI/frrr7XYZ599Jm7/6KOParH9+/eLY60Wf5quF1JBVadOncSxVgtCvTGnGzZsqMVMy4FLuWZa+lsqKpYKKhs0aCBun5GRocWka3BCQoK4vbR8unSfgHwOSgXJptfQavEqIC9zXtm8Ma+lYkjTNUUqcjQVPkrvt55cw6VjCAoK0mKm9wtpe+m8MPGkKF86LtNzKBWEvv/++5b3dTV4PDkfNGgQBg0aJP5OKYWZM2fiySefxE033QQAeO+99xAdHY2lS5fijjvuKNvRElUQ5jV5G+Y0eSPmNVUH5fqd89TUVKSlpbm1AAsLC0OXLl2wYcMGcZuCggLk5OS43YjshHlN3uZKchpgXpO9Ma/JW5Tr5Lzk6yDR0dFu8ejoaONXRaZOnYqwsDDXLS4urjwPiajMmNfkba4kpwHmNdkb85q8RaV3a5k8eTKys7NdN9OCIkRVCfOavBHzmrwR85rsplxXCC0pgkxPT0fdunVd8fT0dONKY06n0/LKUBVBKiIwFQ116dJFi7344ovi2IcffliLScUJppU0pRW/pCJNTwpBpBVKASAqKsrScR05ckTcXipmOnHihOV9HT58WBxrF3bPa6kYRyr68aTI05OiIWklu2eeeUYcO3r0aC328ssva7EHHnhA3P6hhx6yfFzS+SLlmmklztTUVC22atUqcezixYu12J133qnFLs6fyx3XtddeK441XXM8cSU5DVT+9VoqPjetMCytZHjy5ElxrHS9lAr4pf0D8vXatOqmRCr+NBXDSdd8aV+mc1hqgmC6XksFuHZWVfNaep5Nr5+UF6Zckd7HpfPCNA+RjsGT50kq1jbNWaSxUvGo6X3MamMLwFzYbSfl+sl5UlISYmJi3LoJ5OTk4Pvvv0e3bt3Kc1dEVw3zmrwNc5q8EfOavIXHn5yfPn0ae/fudf2cmpqKrVu3IjIyEvHx8ZgwYQKee+45NG7c2NXGKDY2FsOGDSvP4yYqV8xr8jbMafJGzGuqDjyenG/evBl9+/Z1/Txx4kQAwMiRIzF//nxMmjQJeXl5eOCBB5CVlYUePXpgxYoVtu0vSgQwr8n7MKfJGzGvqTrweHLep0+fS34f1eFw4Nlnn8Wzzz5bpgMjupqY1+RtmNPkjZjXVB1UercWIiIiIiK6oFy7tVRF0j91mZYylrqtSMtuA3L1sxQzLWdvqjIuzdRZRuoeYKrIlqqnc3NztdiOHTvE7e+55x4t9uuvv4pj69evr8V+/PFHcWx1Jr0mpop8q51VPOnAkpiYKMbnzZunxfr06aPFpG5FANC6dWstduzYMS0mdXUB5G4nBw4cEMdKXYSk59XUhUg6h0wdVKS41B3JtC+pa1Tbtm3FsdXFH/7wBy0m5bDpGnrx95JLmJ7/Tp06abH8/HwtZur2Ir3W0rGali6X4qbz1bQkeWmmY5Xy2jRWes9ISEjQYqZzkKyJjIzUYlL3EsCz6/jZs2e1mNTtxNRBxd/fX4udOnXKUgwAmjRposWkbjGAnGvSPMjUMUk6h0z7qgp97PnJORERERGRTXByTkRERERkE5ycExERERHZBCfnREREREQ2Ua0KQqWiK6n401SMKS3n3LRpU3FscHCwpX1JBRee8GSJaBOpmElakt1U9LFz504tZloOWiomqm4uLsjxZInmivLnP/9Zi5kKOqVioj179mixb775Rtz+73//uxa77777tNimTZvE7WNjY7VYdHS0OFbKV2npadNy1FIx1U8//SSOlQqPpILUwMBAcXvpPE5KShLHlr7mFBUVicWPVZ1UtCUVQ9auXVvc/j//+Y8WMxWI9erVS4tJ56Bp6XBTsb1V0utv2pf0niG9t5iu19I12FQoKxVre1KUTdZI1wrTe4CUa568X0jv7dIcAJDzUiq8lI4fkBtWmM4Vaax0vpvOC+k5ML2/SvkuPS85OTni9lcDPzknIiIiIrIJTs6JiIiIiGyCk3MiIiIiIpvg5JyIiIiIyCaqVUGotBqoVAhhKhCTVrI0FaNJRaVS0Y5pFTCJ1VVHTUyr00lFD1IxlqnoJCMjQ4tJxXSA+bmtTi4uUmnUqJE45tZbb9Vi0qqtANC+fXtL+zWtrNa4cWMt9tRTT4ljhw0bpsXuvPNOLWZaIVY632bOnKnFHn30UXH7BQsWaLE//vGP4ti0tDQtJhUIebLyqmk1SqnYXGIq4PZk1b+yFh9WFdKqidJrZbquSddradVPQF4h0bRStES6jkvvN6bX2ZNck54DadVPU5GmNFaKAXLxXceOHbXYxo0bxe3JGilXTKt/e1IkKRWgS2NN10DpHJCKP6XjB+S8Nl0DpWOQYtnZ2eL2pmOQSNfQqKgoLcaCUCIiIiIi4uSciIiIiMguODknIiIiIrIJTs6JiIiIiGyCk3MiIiIiIpuoVt1aatWqZWmcqUo6MzNTi7Vt21YcK3WlCAsL02JS5TUgVylLHQVMpCrr4OBgcezRo0ctHZe0vC0gH6upcrpOnTpivLpasmSJGJe65cyZM0ccu2PHDi3WvXt3LRYbGytuf/jwYS124403imPbtWunxdLT07WYaZl0qTtNs2bNtJip04R0DpiWng4PD9diUpW+qdtHWTt4SB2LTN06pPPN1PGo9PPtydLdVYl0vZSef1P3Gqm7UVZWljhW6mAhvVam67W0vamDhtXtTa+rFJfe29atWyduf/LkSS3WrVs3cax0bpk6TNGVk96bTdcaKQel93BAfs+WzgtTrknnlnRtN82ZpI5LpmugtC9pHmHqBCZ1gZHeAwD52h4TE6PF9u7dK25/NfCTcyIiIiIim+DknIiIiIjIJjg5JyIiIiKyCU7OiYiIiIhsoloVhEpL2UpFF6YCMangQLpP0/1KhRymQgxpmXtpe2nZaEAu5DEdq1SIIW1v2teJEye0WP369cWx3lq8ZtU///lPt9ehYcOG4ritW7dqsTvuuEMcKxUTSbliWmLetJyyJC0tTYtJBUKmx7Vv3z4tJhWzScWAgLz8ummJZSlfTQWBEk9yVdqXtPy6qXhRKpKyuny3qcCqqjMtX1+aqfBSWma8cePG4ljptZb2b3qupeu1NNZ0rNJ7jun1l45Ver8xPX8ZGRlarHbt2uJYaal0U2MAunJS4aPp/VZ6H5euy4DcBEB6vzY1m5DiZ86c0WJSkTEg55UnBaFSru3fv9/y9l27dhXHSueQdL2oTPzknIiIiIjIJjg5JyIiIiKyCU7OiYiIiIhsgpNzIiIiIiKb4OSciIiIiMgmqlW3FqtLb5s6Kpw6dUqLmSripW4nUqcJU2cYq50KTMtJSx04pP0D8hK3x48ft3xMpqXaJVKlt1QR7q1dXT7//HO313zo0KHiuHbt2mmx0su2l5C6Oki5bqr+9/f312Kminqp+l7KtV27donbS/l+7NgxLbZ7925xeykvPOlWYvW8AuTOGqauBla3N5HO45o1a4pj27Zt6/bz+fPnjUu1V2VWX2vTa3Lw4EEt1rFjR3GsdG2U8tq0L+k9Q3r9TTkhxU25KuWKdA1u1KiRuL10bnpyvfXkHCJrpOui6b1deq2krjoAkJubq8WkXJE6fgHy+4j0HmDqBCa9N3nSDS8qKsrSMQFyxxqpixIgPwfSPKgy8ZNzIiIiIiKb4OSciIiIiMgmODknIiIiIrIJTs6JiIiIiGyiWhWESkvkSsUFpoJQqaBOWkoXkJfIlcaaihukglKpQMxUNGYqJpFIy6dLBaGRkZHi9qZCRYlpSerqYs2aNW4/x8XFieMmTZqkxf74xz+KY1u3bl32AyvFkwIxKddMhXNS0Y9UIMSiM7Prr7/e7eezZ89Wm4JQKa9MBcHS9TYpKUkcK11vPclr6T1DOi5PioRNhXPS8yJdV03LkUvXC9Py69J56K3F+pVJKso3FfBLr+tPP/1keWxoaKjl45Jea+kabppvSPnjydzEVNAp2bNnjxaLjo62PLZOnTqW93U18JNzIiIiIiKb4OSciIiIiMgmODknIiIiIrIJTs6JiIiIiGyiWhWESoWPUiGNVPAAeLaK39GjR7WYVFwRFhYmbp+RkaHFpGIkU+GcNNa0Cpj0HEjbmwpJVqxYocVKr2JYQnoOpEIMT4pMvdH06dMtxUykldVatGghjpVWTaxbt6441uoqaqZzSCpys1p0BMgrOZqKjKX42bNntZipSE86LlMxnHS+SI/BVHgnrW5n2tfq1avFuLeRHr9UeGnKlSFDhmgxU9GXlBdWc9WT4zLlmlQ8atqXdM2XYqbnpV69emJcYjWvqWykwkdTYwrptc7JybE81mpBMSA30ZCaRUgrfwNAmzZttFhmZqY4ViKdLzExMeJYaUVgT843qSi3MvGTcyIiIiIim+DknIiIiIjIJjg5JyIiIiKyCU7OiYiIiIhswqPJ+dSpU9GpUyeEhIQgKioKw4YNw+7du93GnD17FsnJyahVqxaCg4MxfPjwal/cR/bGvCZvxLwmb8S8purAo24ta9asQXJyMjp16oTz58/jb3/7GwYMGIBffvkFQUFBAIBHH30U//nPf/Dxxx8jLCwM48aNwy233GKL5aXPnTunxaSK6KysLHF7aayp+n/v3r2WjunUqVNiXDoGqaNAXl6euL10rKbqb6lSXIqZugdIld6mLjLSa+DJEr0VoarntUTq9iPFAOCbb76p4KOhylDV81q6rkjdF0x53b9/fy3222+/iWOtLp9u6v4gkbqaSB1gALkrimlf0vMiXUNNy6SfOHFCi0ndnQD5mh8ZGSmOvVqqel5LpK4opvdriSmvpPuQ8sr0fi1tHxERocVMOXH69GktJuW6KS7Nb5o2bSpuv379ei1m6kIjdWsxHVdl8WhyXrpl3vz58xEVFYUtW7agV69eyM7Oxttvv41Fixbh2muvBQDMmzcPzZs3x8aNG9G1a9fyO3KicsK8Jm/EvCZvxLym6qBMfypkZ2cD+O9fTVu2bMG5c+fQr18/15hmzZohPj4eGzZsEO+joKAAOTk5bjeiysS8Jm/EvCZvxLwmb3TFk/Pi4mJMmDAB11xzDVq1agXgwkIa/v7+2kIl0dHR4iIbwIXvj4WFhblucXFxV3pIRGXGvCZvxLwmb8S8Jm91xZPz5ORk7NixA4sXLy7TAUyePBnZ2dmu26FDh8p0f0Rlwbwmb8S8Jm/EvCZv5dF3zkuMGzcOy5Ytw9q1a1G/fn1XPCYmBoWFhcjKynL7qzU9Pd245KrT6bxqBYFS0YO0b6nw0mT79u1iXCqEkJY+j42NFbeX/nKXjtVUxCAtcy4VYwLmotTSAgMDxXhwcLAWkx6/aWzNmjUt7b+iVdW8JrqUqprXUtGWFJMKNwG5yPPix38xqaBNui5K+wfkgj7puEzXa9P9SqRCz9DQUMv7kt4HTMWjUkGoJ0WxFamq5rVEep5NRZoS09Lz0vutVKjsyWOX8sd0rFJemQpdpbhUEFqrVq3LHeJlSeeG3QpCPToapRTGjRuHJUuWYNWqVUhKSnL7fYcOHeDn54eUlBRXbPfu3Th48CC6detWPkdMVM6Y1+SNmNfkjZjXVB149Ml5cnIyFi1ahE8//RQhISGu72+FhYUhMDAQYWFhGDNmDCZOnIjIyEiEhoZi/Pjx6NatGyukybaY1+SNmNfkjZjXVB14NDl/4403AAB9+vRxi8+bNw+jRo0CALzyyivw8fHB8OHDUVBQgIEDB+L1118vl4MlqgjMa/JGzGvyRsxrqg48mpxb+V5cQEAA5syZgzlz5lzxQRFdTcxr8kbMa/JGzGuqDuz1DXgiIiIiomrsirq1VFVS9bzUgUTqdALI1bx33HGHOPbw4cNa7MiRI1pMqtIGgDNnzmixrKwsS8cEyNXTderUEcc2atRIi0mdZUxLPL/yyiuWj0vqKmB6Doio+pK6Zkmfmpq6P0hLd5uWby9Z9v1y25s6lZi6ZZRm6gQmddAwLcludfn148ePi9tfc801Wiw+Pl4cK3XdYg/w8leykNLFpE4rAHDy5Ekt1rp1a8v7kuZBpn+NsNrN7sSJE+L2TZo00WJSBxYTqduLqbtbgwYNtFhGRoY4VrpmSB2XKhM/OSciIiIisglOzomIiIiIbIKTcyIiIiIim+DknIiIiIjIJqpVQahUYCMVf5oKHzdu3KjFxowZI46VCiKlpYNN+5KKQTxZCjc9PV2LSUVHgFxgIhWo7N69W9xeYloOOCcnR4uZCl+IqPqSCh+lYjbTtebtt9/WYi+++GLZD6yKk96zpk2bJo6V3kekZgFUNpmZmVpMKkgG5AYKPXr0EMdK7+PS3ESaWwByY4qQkBAtZirSNBVAS6zOb6RjAoAbbrhBi507d04cKxWA2w0/OSciIiIisglOzomIiIiIbIKTcyIiIiIim+DknIiIiIjIJqpVQai0OqVUhCCNM9m8eXOZjslbmVZZlVYpjY2N1WI//vhjuR8TEVUdUuHZqVOntJipQFG6rphIRXKmVRPLwrTCqCf7ku5DOn6peBYAEhMTLe/f6mqkVDbS6t+m51la1fytt94Sx951111arFatWlrMtKK3VHwaFhamxUyrfkqrbppyTSr+lJ4DU5HpF198ocV69+4tjpWKbb///ntxbGXhJ+dERERERDbByTkRERERkU1wck5EREREZBOcnBMRERER2QQn50RERERENlGturVInUIkpip3T9SooT+15XG/V4tUvS1Vbnuyvaf3QUTVl9Vlxk3XFE+W6L5a16Xy6ABT1vs4fvy4FjMt3y4tf37o0CEtJnXwAOTl40l34MABLebJ67xs2TLL8Xbt2mmxNm3aiNtHRERosbp162oxab4DAIWFhVqsZs2a4lgpL1NSUrTYxo0bxe0lXbt2FeNSFxlp/5WJn5wTEREREdkEJ+dERERERDbByTkRERERkU1wck5EREREZBPVqiBUIhUGOJ3OMt9vVSr+lJS1QMr0HEqFI9JyxERUvXXp0kWLSUWi0rLfgLnIzRs5HA7LY6WCPlORnlRUKy2p3q9fP3H7f//735aPqzpr2LChFouPjxfHHjx4UItJhZsAcOrUKS22detWSzFvIOUqIJ8DkZGRFX04HuEn50RERERENsHJORERERGRTXByTkRERERkE5ycExERERHZBCfnREREREQ2Ua26taSnp2sxqZpXqoYmz/z2229iPCkpSYtlZWVV8NEQUVWzbt06LSZ1D8nJyRG3//HHH8v9mOzKk24tc+fO1WLS8wrI3bX27dunxT799FPL+yfdl19+qcWaNm0qjk1LS9NiUlcWE6njUVm7s5UHKYelmCfHunr1ajG+Z88eLfbtt99avt+rgZ+cExERERHZBCfnREREREQ2wck5EREREZFN2O4750qpCrvvc+fOabHCwkItVtVX97QD03Noh+e7InPMTvuk6sXb8lq6Vpw5c0aLmVYBPHv2bLkfk1158jpI39nNz88Xx0rfOS8oKCjT/j3lbXltdX/S8wzI50VZ92UHFXFcpudKujbY4TW/mEPZ7JU6fPgw4uLiKvswyIsdOnQI9evXv6r7ZF5TRWNekzdiXpM3ulxe225yXlxcjKNHjyIkJAS5ubmIi4vDoUOHEBoaWtmHVm5ycnL4uCqBUgq5ubmIjY0VK9YrEvO66rL742JeVyy7v/5Xyu6Pi3ldsez++l8puz8uq3ltu6+1+Pj4uP6aKGmjExoaassnuaz4uK6+sLCwStkv87rqs/PjYl5XPD6uq495XfH4uK4+K3nNglAiIiIiIpvg5JyIiIiIyCZsPTl3Op2YMmUKnE5nZR9KueLjqt689Xni46revPV54uOq3rz1eeLjsjfbFYQSEREREVVXtv7knIiIiIioOuHk3IYcDgf+/ve/u36eP38+HA4H9u/fX2nHRGQyatQoBAcHX3Zcnz590KdPn3Lbb58+fdCqVatyuz+iizGviS5wOBwYN27cZcdxrlJ+ODkvByUJWXILCAhAkyZNMG7cOKSnp1f24RFpXn/9dTgcDnTp0qWyD6VKeuGFF7B06dLKPgwqhXldNszr6mf79u0YMWIEEhISEBAQgHr16qF///6YNWtWhe+b+WbGyXk5evbZZ7FgwQLMnj0b3bt3xxtvvIFu3bqJS04TVaaFCxciMTERmzZtwt69eyv7cKocvqnYE/O6bJjX1cv69evRsWNHbNu2DWPHjsXs2bNx//33w8fHB6+++qrH93fPPfcgPz8fCQkJlsYz38xsPTmfM2cOEhMTERAQgC5dumDTpk2VfUiXNGjQIPzxj3/E/fffj/nz52PEiBFITU1FvXr14HA4tCRUSuHpp59G3bp1ERgYiH79+mHPnj2Vc/AWTZ06FZ06dUJISAiioqIwbNgw/PTTT25jzp49i+TkZNSqVQvBwcEYPnw4/wXh/7NDTqempmL9+vWYMWMG6tSpg4ULF3q0/dq1azFkyBDExsbC4XDg4MGDbr/3lrzevXu325iL8zovLw+rV69mXv9/zGt7Yl6XjR3yuixK53TpOcjzzz+PGjVqICcnB88//zwWL16MP/7xj/jyyy+xfv16j/fn6+uLgIAA18JNEqUU8vPzPb7vi3ma11VxHmLbyfmHH36IiRMnYsqUKfjxxx/Rtm1bDBw4EBkZGZV9aJa1aNECAHDDDTcAAJ588km330+fPh0vvvgiioqK8P333yMoKAgDBw60fP+vv/46WrZsCafTidjYWCQnJyMrK8v1+3HjxiE4OFj85P7OO+9ETEwMioqKXLHly5ejZ8+eCAoKQkhICAYPHoydO3e6bTd79mxs374dH330EZo0aYJly5bhmmuuQV5enmvMo48+is8//xwff/wx1qxZg6NHj+KWW26x/Li8lV1yeuHChYiIiMDgwYMxYsQIcRKzf/9+OBwO/POf/8Rbb72Fhg0bwul0olOnTvjpp5/Qtm1bzJkzR7z/6dOn45VXXsGZM2fQqlUrOJ1ODBw4EMXFxdrYgoICTJkyBY0aNYLT6URcXBwmTZqEgoICy49ny5Yt6N69OwIDA5GUlIS5c+dqYzIyMjBmzBhER0cjICAAbdu2xbvvvuv6/Zo1a5CcnIxVq1bh+uuvx5dffolmzZqhSZMm+Oc//wmllCuvT548CQA4ePAgYmJi4HA4MGrUKMvH622Y18xrb2SXvC6LvLy8S+b0Dz/8gIKCArz11ltuc5CzZ88iKipKG7906VJX7rds2RIrVqxw+730nfPExETceOON+PLLL9GxY0cEBgbizTffhMPhQF5eHt59913XV4Kt5ltJXm/cuBErV67EuXPnMGDAAO+ahyib6ty5s0pOTnb9XFRUpGJjY9XUqVMr8ahk8+bNUwDUDz/84BZ/9dVXFQA1d+5cBUC1bNnS9bvi4mIVExOjOnTooBISEpRSSmVlZSmn06kAqClTpmj3n5qa6opNmTJFAVD9+vVTs2bNUuPGjVO+vr6qU6dOqrCwUCml1Nq1axUA9dFHH7kdV15engoKCnJ7ft977z3lcDjU9ddfr2bNmqWmTZumEhMTVXh4uNt+R44cqZxOp2rYsKEaOXKkeumllxQAtWbNGtdj8PPzUx9//LFrm19//VUBUBs2bLii59db2CWnmzVrpsaMGaOU+m+ObNq0yW1MamqqAqDat2+vGjVqpKZNm6amT5+uateurerXr+/KMQCqb9++KigoSCl1Ia9r1aqlAgMDVf/+/dWZM2dced28eXPVu3dv1z6KiorUgAEDVM2aNdWECRPUm2++qcaNG6dq1Kihbrrppss+jt69e6vY2FgVFRWlxo0bp1577TXVo0cPBUC9/fbbrnFnzpxRzZs3V35+furRRx9Vr732murZs6cCoGbOnOkaV1xcrK699lrlcDjUH//4RwVAde/eXQFQDz/8sCuvFyxYoJxOp+rQoYPrXF2/fv2VvhxVHvOaee2N7JLX5QWAWrJkievn4uJi5e/vr5xOp9q+fbtS6r9zkA8++EDbtm3btqpu3brqH//4h5o5c6Zq0KCBqlmzpsrMzHSNk+YqCQkJqlGjRioiIkI9/vjjau7cuWr16tWufOvZs6dasGCBWrBgwRXnW0ZGhtfNQ2w5OS8oKFC+vr5uiaSUUvfee68aOnRo5RzUJZQk5Ndff62OHz+uDh06pBYvXuy6mB8+fFibnO/bt08BUEOGDHFNzpVSqlevXpednGdkZCh/f381YMAAVVRU5Bo3e/ZsBUC98847SqkLJ1+9evXU8OHD3Y73o48+UgDU2rVrlVJK5ebmqvDwcDV27Fi3cWlpaSosLMwtPnLkSAVAPf7440oppfbs2aMAuE7ulJQUBUCdOnXK7b7i4+PVjBkzPHhWvYtdcnrz5s0KgFq5cqVS6kKO1K9fX/35z392G1cyialVq5Y6efKkK/7pp58qAOrzzz9XSumTmA8//FABUD179lRnz551bderVy9Vr149t0nMggULlI+Pj/r222/d9l3yx+y6desu+Vh69+6tAKiXX37ZFSsoKFDt2rVTUVFRronWzJkzFQD1/vvvu8YVFhaqbt26qeDgYJWTk6OUUmrp0qUKgHruuefc8nrEiBHK4XC45XVQUJAaOXIk85p5zbz2QnbJ6/JUenJeMgfx9fVVvr6+qlu3bmrSpEmqVatWbn+UlGzr7++v9u7d64pt27ZNAVCzZs1yxUyTcwBqxYoV2jGV5FtZeeM8xJZfa8nMzERRURGio6Pd4tHR0UhLS6uko7q8fv36oU6dOoiLi8Mdd9yB4OBgLFmyBPXq1dPGljyOwMBAt3jpxyz5+uuvUVhYiAkTJsDH578v4dixYxH6/9q7/+Aoq3MP4E8Skk1CfhLykxASAQmCgkYIuVRByUD9wVVUitjegt5bEBMs0o4jVfHWsYPirXJBbOltR6pFo3QKVFSUhoC3DoESQQfBGH4IUZJAgPwkJCk59w8me9mc75F9ySZ5993vZybT+nDO7vvuPu+7hyXPc2Ji5L333hORi+2PZs6cKe+//740NTW5x7399tsyaNAg+d73viciIlu3bpW6ujqZPXu21NbWun9CQkIkNzdXSkpKtGNYsGCBdHR0yKJFi2TixInu1l/V1dUSFhYmcXFx2nnZ+b3raXbJ6XXr1klycrLccsstInIxR2bNmiVFRUUev+LUadasWRIfH+/+75tuuklERI4cOaKNLSkpcf+z5BtvvOGxQ1tycrK0tbV5jF+/fr2MHDlSsrOzPfLu1ltvdT/e5fTr10/mz5/v/u+wsDCZP3++nDx5UsrKykRE5P3335eUlBSZPXu2e1xoaKg8+uij0tTUJDt27HCPCwkJkcLCQo+8/tnPfiZKKQkJCWFed8G8Zl47kV3yuid1nse7774r//qv/yqfffaZLF++XPbv3y+///3v5a9//avH+Pz8fBk6dKj7v6+77jqJiYmB10xXWVlZln5l1wqnrkNsuTj3V6tXr5atW7dKSUmJHDhwQI4cOdIjCXns2DERERkxYoRHPCwsTK666ir3n4tc/BBqaWlxX2hNTU3y/vvvy8yZM91FG51FTbfeeqskJiZ6/Hz00Ufa79j169dP0tPTpaCgQPbv3y9FRUU+P0fyvQsXLkhRUZHccsstcvToUTl06JAcOnRIcnNzpaamRoqLi7U5GRkZHv/duaA5e/asR/z8+fNyxx13yPDhw0XkYi5eTkVFhXzxxRdazl199dUiIl79bmdaWpr079/fI9Y5v/P3Ho8dOybDhw/3+IusiMjIkSPdf975v2lpafLEE0945HXnOMXNlG2Jec28pis3duxY+ctf/iJnz56V3bt3S3Z2tvzzn/+U++67Tw4cOOAe1/WaEbl43XS9ZpCsrCyfHvOlnLoO6dfXB4AMHDhQQkJCtMrampoaSUlJ6aOjurzx48fLjTfe6NXYzvO4tIBBRHxeTTxhwgTJzMyUd955Rx544AF59913paWlRWbNmuUe01nQ9MYbb8DXt18/zzRxuVzy6KOPyubNm+Xjjz+W9PR095+lpKRIW1ub1NXVefyt1e7vXU+zQ05v27ZNqqqqpKioCN7I1q1bJ1OnTvWIhYSEwMfq+oHucrnk9ttvl02bNonIxfNKTU11/3lNTY22sOno6JBrr71WXnrpJfgcgwcPvvxJ+djZs2dhXotcPF7mtSfmNfPaieyQ1z2t8zw6czosLEzGjRsnSUlJkp6eLn/7299k/fr18swzz4iI99cM0vU3BHylsLDQsesQW35zHhYWJjk5OR7feHR0dEhxcbHk5eX14ZF1z6UL8aysLElJSfH4m2lDQ4Ps2rXrso/T2UO0a+ugtrY2OXr0qNZj9Ac/+IFs2bJFGhoa5O2335bMzEyZMGGC+887/6kqKSlJ8vPztZ+uu9+1trbKhg0bZNu2bdrfiHNyciQ0NNTjvSsvL5fjx4/79XvXXXbI6XXr1klSUpKsX79e+5k9e7Zs2LDhiltcBQUFybp162TKlCkiIrJmzRr3n3XmdUxMjMecoUOHypkzZ2TKlCkw77r+yxBy4sQJ7S+4X331lYhc7BIgcvF6qaio0LpqfPnll+4/V0pJZWWlNDU1yV//+lePvO4cFxwc7H7/goKCpL6+nnnNvGZeO5Ad8rqnda5BLj3Hzpzu/MtsVVVVjx7Dd7Vc/C5KKSksLHT2OqSvftn9coqKipTL5VJr165VBw4cUPPmzVNxcXGqurq6rw9NY+rW0tjYqPbu3av27t2rRET169dPFRcXq2PHjimllHr00UeViKikpCT1+eefq7vuuktlZWV5XRD6/e9/X3V0dLjHvfrqqx4FoZ3KysqUiKiVK1cql8ulHn/8cY8/r6+vVzExMWrSpEnuYqNLnTx50v3/R4wYoUREbd++XVVVVbl/zp075x7z8MMPq4yMDLVt2za1Z88elZeXp/Ly8qy9qA7Ulzl97tw5FR0drR566CH455988okSEVVUVKSU+v/CuRdffFEbKyJq/vz57rweN26cioiIUMeOHVPnzp1TmZmZ7rmX5vVNN93kUTi3du1aJSJqzZo18Hibmpq+85y+q3AuMTFRK5x788033ePa29vVxIkT3YVzCxYsUJGRkUpE1JIlSzzyetasWSooKEjNnj3bndcJCQkqPj6eea2Y18xrZ/KnNYhJ1zXISy+9pPbu3eteg/zkJz9RsbGxatOmTR45/dxzz7nHK3Xx2uhaJKrUxWLPSws6TQWhd9xxBzy+5ORkrzoYdbVgwQIVGxvr6HWIbRfnSim1atUqlZGRocLCwtT48eNVaWlpXx8SZFqcl5SUKBHRfnJzc9XSpUtVUlKSSkpKUsHBwcrlcqkpU6ao8vJyS60Up06dql555RW1cOFCrZXipYYNG6aio6OViKiysjLtz9etW6eCg4PV6NGj1XPPPafWrFmjnnzySTV27FiPixKdj4io1157zT2mpaVFPfLIIyo+Pl5FRkaqGTNmqKqqqit/gR2kr3K6qKhIiYjauHEj/PMLFy6oxMRENX36dKXU5Rcx6KfzJl1XV6dSU1NVUFCQCg0Ndef1pEmTtJZzt99+uwoKClL333+/WrVqlVqxYoV6+OGH1YABA7TrqatLW84tXLhQrVq1yt1y7ne/+517XGfLubCwMPWzn/1MrVq1yr0A6mw5Zzqn66+/XomIWrRokUdeBwcHq5CQEPXMM8+ot956y7b3pt7CvJ7k8ZzMa2fwlzWIiWkN0pnTo0aNUnFxcSoyMlL169dPXX311erOO+9UISEhKjMz093tpKcW57fffrvq37+/+vWvf20p3wJhHWLrxbm/MC3Ou/rTn/6krrrqKhUWFqbGjh2rPvzwQzVnzhyPVopKKa8W50pdbJ2YnZ2tQkNDVXJyslqwYIHWOqjTk08+qUREDRs2zHh8JSUlatq0aSo2NlaFh4eroUOHqrlz56o9e/a4x8yZM8fdXoz8x/Tp01V4eLhqbm42jpk7d64KDQ1VtbW1l13EXJqfKCdqa2vVNddco1JSUlRFRYVSSmmLGKUutn574YUX1KhRo5TL5VLx8fEqJydH/fKXv1T19fXfeU6TJk1So0aNcn8rEh4eroYMGaJeeeUVbWxNTY168MEH1cCBA1VYWJi69tprPW7knRobG9Vjjz2m0tLSVGhoqBo+fLh68cUXPf6FSimlvvzyS3XzzTeriIgIjw876l3Ma+Y1XbkPPvhAPfTQQyo7O1tFRUWpsLAwNWzYMLVw4UJVU1PjHtdTi3Pmm1mQUizVJiIiIiKyA1sWhBIRERERBSIuzomIiIiIbIKLcyIiIiIim+DinIiIiIjIJrg4JyIiIiKyCS7OiYiIiIhsoscW56tXr5bMzEwJDw+X3Nxc2b17d089FVGvYV6TEzGvyYmY1+SveqTP+dtvvy0//vGP5be//a3k5ubKihUrZP369VJeXi5JSUnfObejo0NOnDgh0dHREhQU5OtDowCmlJLGxkZJS0uT4GDrfy9lXpMdMa/JiZjX5ERe53VP7Gw0fvx4j92kLly4oNLS0tSyZcsuO7eystK4NSt/+OOLn8rKSuY1fxz3w7zmjxN/mNf8ceLP5fK6n/hYW1ublJWVyZIlS9yx4OBgyc/Pl507d2rjW1tbpbW11f3fihuWUg+Ljo62PId5fRH6m35HR0e3HjMkJESLDRs2DI5NT0/XYmlpaXDs8OHDtVhKSooWi4yMhPPR2NOnT8Oxf//737XY73//ey3W0tIC5/sC85qciHntW+jeet9998Gx5eXlWiw3N1eLVVRUwPmVlZVa7IYbboBjt2/frsVKS0vhWCe4XF77fHFeW1srFy5ckOTkZI94cnKyfPnll9r4ZcuWyS9/+UtfHwaR0ZX8MyXz+qKe+Cde9JhowS4i0q+ffssKCwuDY8PDw7VYRESEFjMtzvv376/FTItrl8ulxXr7n8OZ1+REzGvfQvdWdK8UEQkNDfVqrOke7O18EXxvd7LL5XWfd2tZsmSJ1NfXu3/Q37SI/A3zmpyIeU1OxLwmu/H5X1UGDhwoISEhUlNT4xGvqamB/0zscrngt052ZOVv8N7+s5jpG8L169drsePHj2sx9DdTEfwNX35+Phz7gx/8QIt99dVXcCyCftXBdP7++s+FTs5rxFSocuHCBS2Gvgl54okn4PwxY8ZosbFjx2qxAQMGwPkxMTEw3h1VVVUwjq7NhIQEOBblwMSJE7XYjBkz4Hx0b+mNayXQ8poCg5Pz+sYbb9RiQ4YMgWMnTJigxdD92nSvQb8yWFtbq8VOnDgB56P7NfpVGRF8rNOmTdNiH3/8MZz/j3/8Q4vV1dXBsf7A59+ch4WFSU5OjhQXF7tjHR0dUlxcLHl5eb5+OqJewbwmJ2JekxMxr8nf9cgv+SxevFjmzJkjN954o4wfP15WrFghzc3N8uCDD/bE0xH1CuY1ORHzmpyIeU3+rEcW57NmzZJTp07J0qVLpbq6WsaOHStbtmzRijOI/AnzmpyIeU1OxLwmf9Zj5bGFhYVSWFjYUw9P1CeY1+REzGtyIuY1+as+79ZCREREREQXBSmbtc9oaGiQ2NjYPj0GU6eK7m62gjpQvPrqq3As6huKOjqYGtn/85//1GLnz5+HYxsbG7UY6vl66NAhON/f1NfX90jXj+9ih7y2wtS54NKNOjrdf//9WuyNN96A81EOobysr6+H89F1ER8fD8eiawC976ZrCF0X1dXVcKy3GxaZNuDwBeY1+RrqPY2uq57kD3nd3W5LP/zhD7UYuqeI4P0XTPelw4cPazHUrQV14TI9F7oHm3ICvS6mPueoQxx63BEjRsD56D5u+hw5c+aMFvvwww/h2J5yubzmN+dERERERDbBxTkRERERkU1wcU5EREREZBNcnBMRERER2USPtVL0Z1YKP9FWuiIiM2fO1GJoK1xUcCGCCzTQtrmoYEdE5OzZs16PRYWqL7zwghZDBRsiIuvWrdNi+/fvh2PJP7S3t3s9FhX4NDU1wbGoyBLl5WeffQbno2IklL8iIgkJCV49v6lwC90HxowZA8ei1wCdl6kAqKGhAcbJf6HGAqZc625fhttuu02LmYqPv//972uxQYMGwbHoM2fJkiVa7ODBg3C+aVt3p7Hy/v3bv/2bFps6daoW+/Of/wznf/3111osIiLC6+dHRZKmNU9dXZ0WQ+sYU0Eouq+ZGm6g1xCdV0VFBZyPziEqKgqOnTx5shZDBfx79uyB83sDvzknIiIiIrIJLs6JiIiIiGyCi3MiIiIiIpvg4pyIiIiIyCa4OCciIiIisgl2a7EAbWl/9dVXw7FtbW1aDHWKMHW1QF0pUAeWgQMHwvmoIttU0Y06w6BOE6Ztc3/1q19psQMHDsCxqNKf/Nv111+vxc6dOwfHour/9PR0LWbauhrlu6kLUWpqqhb79ttvtVhkZCScn5ycrMVMXWzQ9ZqUlKTFHnnkETj/+eefh3FyFitdPWbMmAHjK1eu1GLoGjJ1tQgJCdFipu3b0eMWFRVpMdPnEPocyM3NhWNNn4X+CnWLMtm3b58WM70nqDsV6jQigrtIoW4ppm4tcXFxWgzd203HiuKhoaFwLLo20L29paUFzkf3ZlMXmc2bN2sx9FqhzjQivdOFiN+cExERERHZBBfnREREREQ2wcU5EREREZFNcHFORERERGQTLAgFfvSjH8H46NGjtVhNTU23nsvKds6oSA4VuIngAjUrRRutra1a7Pz583A+KjDJycmBY5cvX67FHn/8cTiW+o6VwjVUOGkq2kGio6O9ipnipi2aUYGQaetoBBV0oaJuEVxUioqZli1bBuezINR5XC6XFjNdF/PmzdNiqAGBCG4MUF5ersXQZ4AILshDhdqmx0DNDk6ePAnno2vo1KlTcKyVLej9gamBAlJbW6vFTIXqaGxMTAwci95rK/dFVFBppaAUvaeo8NL0XIhpHYNy1fRc/fv312LHjh3TYpmZmXA+C0KJiIiIiAIIF+dERERERDbBxTkRERERkU1wcU5EREREZBMsCAVMO5iZisG8FRQUpMVMhXeowAIVaZqKaFDRh2knRfRcKGaaj87BtGPZDTfcAONkL1YKQlGBjqnwzduiL7S7pggukjLtOId2skNFQ6bCKzQf7ZgnIvKLX/xCi7388starLS0FM6fO3euFlu7di0cS/bj7U6GEyZMgPOfeuopLWYq0kTXECqUNu3Si3YIRcWrIvjaQgWhgwYNgvO//vprLYY+x0RECgsLtef53e9+B8f6g8GDB8M4KlI8c+aMFjMVeaLdWE3NGlDcdL/0FirctFLkaVpHeFsQapqPjsH0OYIKOtGaCeV6b+E350RERERENsHFORERERGRTXBxTkRERERkE1ycExERERHZBBfnREREREQ2wW4tgKlKGlU+myqH0Ra3qHrfVDntbbcMKxX9qFuMKY5iqJpZBJ+rqfK6u5Xi5HtWugghGRkZWszU/cEU7yoqKgrG0XGZOhWgHET5Z9p6GlXqm7rNVFRUwHhXt99+O4y/9957WozdWpzn7NmzMI46mJiuFfSZgzqzmDpNoK3LTXltpUMYgjrDJCYmevVcfdkpwxdSU1NhvL6+XoslJCRoMdP739zcrMVM3bHQe23KQQS9B2gdYPpct/IeorEo12NjY+F81DUMxURwdyP0/GicCM7hU6dOwbFXit+cExERERHZBBfnREREREQ2wcU5EREREZFNcHFORERERGQTLAgFTFvRokIIU8FAbW2tFrNSZIeKLFHhnqngAY01Fb6h40LPbyq8Q8URpnNFRXrx8fFazErRCnUPeq9MxcNoLCpm/OEPfwjno2IydK2YCq1R4RHKVRF8rFYKebzdTlpEZNq0aVps8+bNWgxt0y0i8vrrr3v9XGQ/pvtwV+Xl5TCOCietbImOct10TOjeaiXX0XGZCv9QUaPpuP7nf/7H62Owm+TkZC1muoeh1wq9ToMHD4bz0Trk9OnTcCyKo/fa9P6jc/C2qF/E2joCjUWxr7/+Gs7PysrSYqggVgS/LuhcGxsb4fxrrrlGi+3YsQOOvVL85pyIiIiIyCa4OCciIiIisgkuzomIiIiIbIKLcyIiIiIim+DinIiIiIjIJtitBTBtRYs6WKSkpMCxJ0+e9Gq+qYMKqt5HVe4xMTFwPnouK10t0PObKvJRt5WGhgY4FlVEDxkyRIuxW4v/eP75572KieBuFXFxcVrM1EEFbVNugnIY5V9JSQmcn5+fr8VMeTl37lwttnDhwssc4f/7zW9+4/VYsh8rnbgQ1LEoIyMDjkXbv5vu7Qj6zDF1tfBWU1MTjKPOHuiz0d/l5eVpsUmTJsGxb775pha76qqrtNgdd9wB5//Xf/2XFjN1MEHvq+lzHPE2r0zjUGeflpYWOBZ1skLXRVhYGJyPOs7ccsstcGxZWZkW27JlixbLycmB89FnFru1EBERERE5FBfnREREREQ2wcU5EREREZFNWF6cf/zxxzJ9+nRJS0uToKAg2bhxo8efK6Vk6dKlkpqaKhEREZKfny8VFRW+Ol6iHsG8JqdhTpMTMa8pEFguCG1ubpYxY8bIQw89JPfcc4/258uXL5eVK1fKH//4R8nKypKnn35apk2bJgcOHDBuR9yXoqKitBgqJBPBRT+mgkz0uKYiSQQdAyrkMW2zbqVACEHnaipeTUpK0mLNzc1wLDpeNL+3OS2vu6u7BW4mERERWiw2NtarmAjOH1Q0JILzdffu3VoMFTSL4KIfU0FoZmamFrvzzju12ObNm+F8bwvArWBO+49Dhw5pseuvvx6Oraqq0mKRkZFazLQlO4qbCq3RNTRw4EAthopURUQSEhK02IEDB+BYb9kxr7v+BUHEXKQ5Z84cLbZo0SIt9o9//APOR5+tiYmJcKy3W9KbCoJNn+NdmdYGaH10/vx5OBblsOlxEZTDQ4cOhWN//OMfa7Ff/OIXWmzXrl1w/nvvvef1cV0py4vz2267TW677Tb4Z0opWbFihTz11FNy1113iYjI66+/LsnJybJx40a5//77u3e0RD2EeU1Ow5wmJ2JeUyDw6e+cHz16VKqrqz1akMXGxkpubq7s3LkTzmltbZWGhgaPHyI7YV6T01xJToswr8nemNfkFD5dnFdXV4uISHJyskc8OTnZ/WddLVu2TGJjY90/gwcP9uUhEXUb85qc5kpyWoR5TfbGvCan6PNuLUuWLJH6+nr3T2VlZV8fElG3Ma/JiZjX5ETMa7Ibn+4Q2rlbZk1NjaSmprrjNTU1MnbsWDjH5XLBXcR6CypQs7KTpgkqxECFOKbdrtAuXmjnUtMxoWIyU4EZOl8UMxXKogKh48ePw7Ht7e1aDBWC2Ik/5nVPQQWZKC9MxWhod0BUPI2uH9PYuro6OBbtMopyddiwYXA+ugZM7ykqcnr99de12IABA+D87hZ/WnUlOS3i3Lzua59++qkWmzlzJhyLcsXbwj0RvGuj6Xo7ffq0FkOfWa2trXA+KlREuwT7ip3yet++fV7H0efl4cOH4fwHHnhAi/3xj3+EY709J9P9Gq05UOGlqTEG+mwwrXkQdA82FfWionxU5CkiMmvWLC323//9314fV2/w6TfnWVlZkpKSIsXFxe5YQ0OD7Nq1C25vS+QPmNfkNMxpciLmNTmF5W/Om5qaPNo+HT16VPbt2ycDBgyQjIwMWbRokTz33HMyfPhwdxujtLQ0ufvuu3153EQ+xbwmp2FOkxMxrykQWF6c79mzR2655Rb3fy9evFhELvbuXLt2rTz++OPS3Nws8+bNk7q6Ovne974nW7ZsYd9csjXmNTkNc5qciHlNgcDy4nzy5Mnf+bvXQUFB8uyzz8qzzz7brQMj6k3Ma3Ia5jQ5EfOaAkGfd2shIiIiIqKLfNqtxR+hbWtN3VpQ5bOpUwRqxdS196qIucoeVTmjbwtMVdbezjdBr4Fp213UgcO0HTCCOnCQPVnJQeTEiRNaLCsrS4udOXMGzkdbf3/xxRdwbGxsrBZLSkrSYmiLcRHcFcB0vaIOBuhxL90c5VJ/+9vfYJz8F7qHmrYjR11RTB18UF6jLkomKC9N93bUXQjd71taWrx+/oMHD3o91l9Yea+Rl19+2eux9913nxa7+uqr4Vi0PkHvlelYGxsbtRjq4GLKHzT/0i46l3tcNN/0eTNo0CAt9s4778Cxe/bsgfGurFxXVtZX3uA350RERERENsHFORERERGRTXBxTkRERERkE1ycExERERHZBAtCQeGiqeAAFYSaigDQFsXDhw/XYg0NDXA+KkZDBUKoiEIEF6iYCozQWLTFbk1NDZy/f/9+LTZixAg4FhX6mQpwyXlQDnobE8F5OXr0aDj2o48+0mKffPKJFnvyySfhfFQk1d7eDsei6xUVE6Fto0VYEOpEVgoC0eeQaZvztrY2LYZyrV8//PGOxpo+86w8rrdMnyP+zNfFgN/F1ITC27Eof1pbW+F8lBcRERFazFQQiubX1tbCsahQFT1XZGQknG963O5AjTVEzJ9PvsRVERERERGRTXBxTkRERERkE1ycExERERHZBBfnREREREQ2EfAFoWh3StMv+6Oir/r6ejj222+/1WKooNT0XN4WmJjmo2IkK0UMqBDIVKCECjFyc3PhWLRDpKnoguzH253wTO8pKt5ExUimYjq0OyLanVNEJDExUYuNHDlSi6FdEEXwToim80JFcqgYy1TMRIFt0qRJWsxU+Ieul7i4OC0WHR0N56OiZlNBqJVibW+dPXu2W/MDHXr9TJ/N6N6I7mEDBw6E8007NXdlul+jHDQVFKP1FSo0NTWQMBW1eguteUzn1RsFwPzmnIiIiIjIJrg4JyIiIiKyCS7OiYiIiIhsgotzIiIiIiKb4OKciIiIiMgmAr5bC6oQRtvbiogkJydrscOHD8OxqMoYdWsxVS6jamBUUW9li2YrFcZorGl+Y2OjFkPb7org1xZ14CB78nZL8hdeeAHGk5KStBjazruhoQHOR3mNuk+IiIwaNUqLjRkzRouh/DU9rqnbyjfffKPFUPeA7m59Tv6tf//+MD558mQthjp+iYjExMRoMfQ5Zurgga4tU6cL1NkDXYNWuhChLkj+rje6d3RCHVRMn6FozYHWJqZ7KHr/0T0M5aQIzjWUqyLmjkFdmdYWps553eHt511P4DfnREREREQ2wcU5EREREZFNcHFORERERGQTXJwTEREREdkEq5MAU3EMKnpBW9eLiISGhmoxK0UjqOjCVLSBoKIddEwiuOgDFZKYin5QgYppm/OWlhYtFhUVBceS/7rzzjthHBXtoO2YTbm2b98+LXb69Gk4Njs7W4uhLdFNxUyIqVgcXS8jRozQYsuXL/f6uaj3eFtAj8aZxiLPPvssjFu538fFxWkxtE37hQsX4HwrjQXQ4yKoyNDkX/7lX2D8008/9foxAhn6vDW9f96+L6bPa1MOefs8KNdMY1GhKLoGTNeaaX3jr/jNORERERGRTXBxTkRERERkE1ycExERERHZBBfnREREREQ2wcU5EREREZFNBHy3FrQVrKkiH1UDm7q1REdHazFU+W7aHhZVOaOYaT6q3jZVZCOoStq0HXR1dbUWS01NhWPROVjZ+pl6h5WuFHfffbcWGzRoEJyPtrlH1xW6fkREtmzZosW++uorOPYnP/mJFpswYYIWM3XFQF1kTN0LEhMTtdjRo0e12DvvvAPnB7LudkDpKej9t7Kd98KFC7XY4sWL4di9e/dqsYSEBDgWvS4oZuq0guL9+/eHY1G+W+lic+LECS02ZcoUOPaVV16Bcbo8lKsi+L1Cn+Omaw11VkFdVUzdYtD1YhqLeLsOciJ+c05EREREZBNcnBMRERER2QQX50RERERENsHFORERERGRTQR8QWhKSooWs1IgdujQITgWFbSh4gi07bfpGFAhh5UCJdN5ocdF25SbCjdRQZ7puNBriApMqG9ZKcbbsGGDFvv888/hWPT+p6ene/WYIrgg9IYbboBj0eOiomiU6yamwjdUOHX8+HGvH9dpTK+TlaJ0b4sRTVCumY6ru891xx13aLGf/vSnWuyuu+6C8//whz9osbNnz8KxqKAT5bCpIBQVL5veF/QaxsXFaTFTs4DGxkYtlp2dDceSdxoaGrRYcnIyHJuWlubV2Lq6OjgfFW+idYCpUB4ZPXo0jKPzQs0CTMXL3S0g7+sC9K74zTkRERERkU1wcU5EREREZBNcnBMRERER2QQX50RERERENsGCUFAQaioMGDVqlBb73//9Xzh25syZXj2/lUIcVJxh2i3Lyu52qOgCzR8wYACcj3ZJNR0XKhxCu7SS75l2kbNSVHzmzBkthnY3fOutt+D8559/Xovt3r1bi7W0tMD5Dz74oBabNGkSHIsKj9DjmnasQ6+Xlevtww8/hGO9nW/lfbEb0z20N3f364nXb/bs2TD+xBNPaLExY8ZosZ///OdwflRUlBY7fPgwHIs+s1BBKBongotiTc0CUGMDFGttbYXz0XsQHx8Px5InU/Hyj370Iy22detWOBbd29DjNjc3w/moYUVGRoYWO336NJzf1NSkxUyFzqjQFOWlqfgUNQb44IMP4Fh/uLfym3MiIiIiIpvg4pyIiIiIyCa4OCciIiIisglLi/Nly5bJuHHjJDo6WpKSkuTuu++W8vJyjzHnz5+XgoICSUhIkKioKLn33nulpqbGpwdN5EvMa3Ii5jU5EfOaAoGlxfmOHTukoKBASktLZevWrdLe3i5Tp071KCZ47LHH5N1335X169fLjh075MSJE3LPPff4/MCJfIV5TU7EvCYnYl5TILDUraXr1tlr166VpKQkKSsrk5tvvlnq6+vlD3/4g7z55pty6623iojIa6+9JiNHjpTS0lKZMGGC747cR1CnEFOnAbRFt6lKOSYmxqvnt7LNvZVxqAuMaaypUr8rU5V9fX29FkPbNouIhIeHazFTpX9v8Ze8NlXvo4p8lFdWtlhesWIFjKMcQF18SkpK4HyUgytXrtRiqMpfROShhx7SYiinRPBrgK7L48ePw/mos5DpPUBbpX/00UdwbG+xY14PHDhQi5nuP+i+0lPQuS5ZskSLZWVlwfkvv/yyFvvVr36lxX7605/C+adOndJiw4cPh2NRxxu0pbnpukDXlqljFuqggT4fTc+Frgt0XVlhx7y2At1D0Gtq6kKFHD16FMZvu+02LVZZWanFTp48CecPGjRIi6HjN12r6H5rev/ROgCtuVB3OBGR9PR0LYY6uIiI7NmzB8btpFu/c975hnR+OJeVlUl7e7vk5+e7x2RnZ0tGRobs3LmzO09F1GuY1+REzGtyIuY1OdEV9znv6OiQRYsWycSJE939hKurqyUsLEzi4uI8xiYnJ0t1dTV8nNbWVo+/MTU0NFzpIRF1G/OanIh5TU7EvCanuuJvzgsKCmT//v1SVFTUrQNYtmyZxMbGun8GDx7crccj6g7mNTkR85qciHlNTnVFi/PCwkLZvHmzlJSUePyeT0pKirS1tUldXZ3H+JqaGuNuZUuWLJH6+nr3D/p9KKLewLwmJ2JekxMxr8nJLP1ai1JKFi5cKBs2bJDt27drxTE5OTkSGhoqxcXFcu+994qISHl5uRw/flzy8vLgY7pcLrhFbG9BhTRoO3sRkW+//dbrx01KStJiqJDHVKRnpXgPQcVwpuJTVIyCirRQcZAILv785ptv4FhUTGJ6vXuLv+S1qVDZ24JeVIwngrcknz9/PhyLtkN++umntdi1114L56N/Lp43b54WM/3zc9cPXBFz8XViYqIWQ1tHo6I1EXxvaGlpgWPRNfD555/DsUhPbCfdl3n94IMPwvgDDzygxY4cOQLHoi3t0X3R9C0n2mZ+yJAhcCzKIVTQu3fvXjj/P//zP706LtN9EUHHb4qjQmtTUT4aa7qHVFVVaTHT9eItU/Got/zlfm1iuo93NWzYMBhH74np/UtISNBiaB1jahaRmZmpxdra2rQYulZFcEGnCbo2UaG06R6MilJNRdX+UBBqaXFeUFAgb775pmzatEmio6PdH6CxsbESEREhsbGx8u///u+yePFiGTBggMTExMjChQslLy+vzyukiUyY1+REzGtyIuY1BQJLi/Pf/OY3IiIyefJkj/hrr70mc+fOFZGLLaWCg4Pl3nvvldbWVpk2bZq8+uqrPjlYop7AvCYnYl6TEzGvKRBY/rWWywkPD5fVq1fL6tWrr/igiHoT85qciHlNTsS8pkDQrT7nRERERETkO1ycExERERHZxBVvQuQUqFOIqZr8q6++8vpx0ZbmqMo4JCQEzkddTVCVtKnLAzov07a56LnQ45rmo7FffvklHNt1YwgRbvjQXdOnT9disbGxWsxU/Y+2if7000/h2GuuuUaLXboTX6eamho4/+DBg1oMdR8wdSuaMWOGFjNdr83NzVoMbV3euXlJV2g7adSFSUTknXfegfFA9eijj8I46mCC7pUiuLPOmTNntJgpV9H7anqf4uPjtRgqHhw3bhycjzqYoK4Ypg4sqDMQylURkUOHDmkxtP266dc/0HOhXBfB3TZQBxBTBw00tic6EzlRWloajKP3GnWCE8EdusaMGaPFTDunpqamajHUBch0v0drBpR/IiK5ublaDK25TF2wUJtM9HklItKvn770Nb2GfYXfnBMRERER2QQX50RERERENsHFORERERGRTXBxTkRERERkEwFfEIqKyUxFmp999pnXj4u2L0fFTKZCGrSdrrfb/orgIk8Us6Lrpg+dIiMjtRgq/BPBRVaoeJV0X3zxBYyfPn1ai6ECN1ORJSr6iYiIgGNNBW1dDRo0CMZra2u12MiRI7WYaZt1dG2aCudQXnXd6lsEF62JiNx6661a7LrrroNj0fbtCCpEErFfMVJ3lZaWwnhMTIwWQ++/CC6yRO91dnY2nI9e66lTp8KxqHANHZdp63p0D0SfLaZ7MLpfosI7EZzD6LpE15qI98cqgot16+rqtFhlZSWcj4o/n3vuOTjWn6H31crnNXpPTPfgffv2aTHT+4ful0OHDtViqNmEiIjL5dJiVhpboPffdF5RUVFaDF3vpvUCuoZRrorgz5fDhw/DsUh3329v8JtzIiIiIiKb4OKciIiIiMgmuDgnIiIiIrIJLs6JiIiIiGwi4AtC0S/xm3YwM+1Eh3zwwQdaDBUstLe3w/mooA4VXZiO1UpxAtqhERWo/OlPf4LzUZHUkSNH4NibbrpJi3HHON3YsWO1WGZmJhyLXms0v76+Hs5HO66ZitFMhV9d3XDDDTA+YsQILYZ2ITTlL8oVU6Er2o1206ZNWgwVaouI/PnPf/YqZoXTCj9NFixYAOPovmIqpn344Ye1GCooRY8pgne9NBWfonszeq9MBb0oX1ExGir0F8ENBExQDqNrwHSu5eXlWsy0GyUqtkUFgbt27YLzUbH6mjVr4Fh/ht5/K8XfaHdLU7MItEsxuoeK4Nc/IyPD6+dCO/qimGlHcHRdmXYaT0xM1GLoGka7npqYmgWgJgioINRUwO3r4k+E35wTEREREdkEF+dERERERDbBxTkRERERkU1wcU5EREREZBNcnBMRERER2UTAd2tBXSlQ1bCIyNGjR71+3KVLl17xMTmBaStcVKluqjQPZE899ZQWM3W1QZ1Z0FhThTmqtK+oqIBjUceg9PR0LWbqQoSq79F2zqYOLOi8EhIS4Niamhot9h//8R9wLIK6CljpjmQ6B6cydTa4FOrs9Pjjj8OxKH7//fdrsZ///Odwfk5OzmWPpxN6/7w5H6tMnYF+/etfa7Hly5fDsSdPntRi8+fP12L33HMPnJ+cnKzFBgwYAMdu3LhRi6HOLqNHj4bzV61aBeOBwEpnpuuuu06LmTpmoQ4kbW1tcCzKN9Stp3///nD+Z599psXQtYI+F0RwxxrTc3399ddaDHWSc7lccD56vUyfeagbHtIbXVlM+M05EREREZFNcHFORERERGQTXJwTEREREdkEF+dERERERDYR8AWhqGgLbfstIvLNN994/bhom2Z/36beyla2poJCVHhk5XUNFJ988okWmzZtGhyLCmzQe4W2XRYRKSws9Pq40Hvd0NCgxUzbQaMiS5QT6DFFcJHV2bNn4dgpU6ZosdraWjgWMRVZEeZN8RTKSytFV0VFRV7FTCZPngzjqHjUVCSJVFZWarHt27drMdM25921Zs0aLVZaWgrHosI5VKgrIhIVFaXFUEFidXX15Q7R0bqb1yj/srOz4Vh0Xzp48CAcm5GRocWGDBmixZKSkuD8UaNGaTG0PjIVv6KxpmJM9Nlw8803e/1c4eHhWsx0DZs+M+yE35wTEREREdkEF+dERERERDbBxTkRERERkU1wcU5EREREZBNcnBMRERER2UTAd2tBW3wfOXIEjrXS6SHQnTp1CsZRVTl6DwLdyy+/rMX+8pe/wLETJkzQYsOGDfMqJiISHx+vxUxbLLe3t8N4V6ZOJ83NzVoM5cThw4fhfNTF5vjx414dk1Xd7cBAur5+/VAHle+K+zO09Tr1jO7m9dNPP63FMjMz4VjUQWf//v1wbHp6uhabM2eOFnvvvffg/M2bN2ux6667TosdO3YMzs/Ly9NiprUBOoeqqiotdubMGTgfdWZBnzci/rGW4zfnREREREQ2wcU5EREREZFNcHFORERERGQTtvud897+ncTW1lYtZtrd0Iq+/t3KnuCLc0Kv7fnz57v9uFb0xXvji+c07TCLfg/cSl67XC6vj8G0O1tXpt85RzsRomM1/W57b+6y62/XsL/mNdF3CdS8Nt3r0K7mVh4D3W+tnC96ftN89Hlh+mxAY9Hxm54LjbXzruyXe82DlB2y8BLffPONcZtxIl+orKyEhTI9iXlNPY15TU7EvCYnulxe225x3tHRISdOnJDo6GhpbGyUwYMHS2VlpcTExPT1oflMQ0MDz6sPKKWksbFR0tLSJDi4d3+ji3ntv+x+XszrnmX39/9K2f28mNc9y+7v/5Wy+3l5m9e2+7WW4OBg998mOluZxcTE2PJF7i6eV++LjY3tk+dlXvs/O58X87rn8bx6H/O65/G8ep83ec2CUCIiIiIim+DinIiIiIjIJmy9OHe5XPLMM89Y6ibhD3hegc2prxPPK7A59XXieQU2p75OPC97s11BKBERERFRoLL1N+dERERERIGEi3MiIiIiIpvg4pyIiIiIyCa4OCciIiIisglbL85Xr14tmZmZEh4eLrm5ubJ79+6+PiRLPv74Y5k+fbqkpaVJUFCQbNy40ePPlVKydOlSSU1NlYiICMnPz5eKioq+OVgvLVu2TMaNGyfR0dGSlJQkd999t5SXl3uMOX/+vBQUFEhCQoJERUXJvffeKzU1NX10xPbi7zktwrxmXuuY1/bEvO4ef89rJ+a0SGDktW0X52+//bYsXrxYnnnmGfn0009lzJgxMm3aNDl58mRfH5rXmpubZcyYMbJ69Wr458uXL5eVK1fKb3/7W9m1a5f0799fpk2bJufPn+/lI/Xejh07pKCgQEpLS2Xr1q3S3t4uU6dOlebmZveYxx57TN59911Zv3697NixQ06cOCH33HNPHx61PTghp0WY18xrT8xr5rUTOSGvnZjTIgGS18qmxo8frwoKCtz/feHCBZWWlqaWLVvWh0d15UREbdiwwf3fHR0dKiUlRb344ovuWF1dnXK5XOqtt97qgyO8MidPnlQionbs2KGUungOoaGhav369e4xBw8eVCKidu7c2VeHaQtOy2mlmNfMa+Y189qZnJbXTs1ppZyZ17b85rytrU3KysokPz/fHQsODpb8/HzZuXNnHx6Z7xw9elSqq6s9zjE2NlZyc3P96hzr6+tFRGTAgAEiIlJWVibt7e0e55WdnS0ZGRl+dV6+Fgg5LcK8DjTMa+a1EwVCXjslp0Wcmde2XJzX1tbKhQsXJDk52SOenJws1dXVfXRUvtV5Hv58jh0dHbJo0SKZOHGijB49WkQunldYWJjExcV5jPWn8+oJgZDTIszrQMO89p/zZF57LxDy2gk5LeLcvO7X1wdA/qugoED2798vf//73/v6UIh8hnlNTsS8Jidyal7b8pvzgQMHSkhIiFZZW1NTIykpKX10VL7VeR7+eo6FhYWyefNmKSkpkfT0dHc8JSVF2trapK6uzmO8v5xXTwmEnBZhXgca5rV/nCfz2ppAyGt/z2kRZ+e1LRfnYWFhkpOTI8XFxe5YR0eHFBcXS15eXh8eme9kZWVJSkqKxzk2NDTIrl27bH2OSikpLCyUDRs2yLZt2yQrK8vjz3NyciQ0NNTjvMrLy+X48eO2Pq+eFgg5LcK8DjTMa+a1EwVCXvtrTosESF73bT2qWVFRkXK5XGrt2rXqwIEDat68eSouLk5VV1f39aF5rbGxUe3du1ft3btXiYh66aWX1N69e9WxY8eUUko9//zzKi4uTm3atEl9/vnn6q677lJZWVmqpaWlj4/cbMGCBSo2NlZt375dVVVVuX/OnTvnHvPwww+rjIwMtW3bNrVnzx6Vl5en8vLy+vCo7cEJOa0U85p57Yl5zbx2IifktRNzWqnAyGvbLs6VUmrVqlUqIyNDhYWFqfHjx6vS0tK+PiRLSkpKlIhoP3PmzFFKXWxl9PTTT6vk5GTlcrnUlClTVHl5ed8e9GWg8xER9dprr7nHtLS0qEceeUTFx8eryMhINWPGDFVVVdV3B20j/p7TSjGvmdc65rU9Ma+7x9/z2ok5rVRg5HWQUkr5/vt4IiIiIiKyypa/c05EREREFIi4OCciIiIisgkuzomIiIiIbIKLcyIiIiIim+DinIiIiIjIJrg4JyIiIiKyCS7OiYiIiIhsgotzIiIiIiKb4OKciIiIiMgmuDgnIiIiIrIJLs6JiIiIiGyCi3MiIiIiIpv4P5xGYyc510vnAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data , test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O885pWOtI6un",
        "outputId": "7449392e-c9ec-4066-a327-0bec9f1846ae"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset FashionMNIST\n",
              "     Number of datapoints: 60000\n",
              "     Root location: data\n",
              "     Split: Train\n",
              "     StandardTransform\n",
              " Transform: ToTensor(),\n",
              " Dataset FashionMNIST\n",
              "     Number of datapoints: 10000\n",
              "     Root location: data\n",
              "     Split: Test\n",
              "     StandardTransform\n",
              " Transform: ToTensor())"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_data,batch_size=32,shuffle=True)\n",
        "test_dataloader = DataLoader(test_data,batch_size=32,shuffle=False)"
      ],
      "metadata": {
        "id": "wsvhwYHdJfSi"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataloader = DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "test_dataloader = DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle=False)\n",
        "train_dataloader , test_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwKfeCabKJF7",
        "outputId": "078ee9d9-af22-420d-c61c-5f6f33e7af85"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7f16973d2590>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7f16a14e6e90>)"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train dataloader : {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"test dataloader : {len(test_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"train data : {len(train_data)}\")\n",
        "print(f\"test data : {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3cE8aXTKruX",
        "outputId": "4a73d772-fa06-433c-e3b3-d885acf46ced"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataloader : 1875 batches of 32\n",
            "test dataloader : 313 batches of 32\n",
            "train data : 60000\n",
            "test data : 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_batch , train_labels_batch = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape : {train_features_batch.size()}\")\n",
        "print(f\"Labels batch shape : {train_labels_batch.size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEZvP3BsKuMb",
        "outputId": "b6d179da-cdfb-4a7a-f9b0-59d08f259acb"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape : torch.Size([32, 1, 28, 28])\n",
            "Labels batch shape : torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "random_idx = torch.randint(0,len(train_data),(1,)).item()\n",
        "image , label = train_data[random_idx]\n",
        "print(f\"image shape : {image.shape}\")\n",
        "print(f\"label : {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB1P1AC2LL6b",
        "outputId": "91a66c21-08ee-41a8-e205-4218e4485312"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image shape : torch.Size([1, 28, 28])\n",
            "label : 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.squeeze(),cmap='gray')\n",
        "plt.axis(False)\n",
        "print(f\"Image size : {image.shape}\")\n",
        "print(f\"Label : {label} ({class_name[label]})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "M73il9agLTC8",
        "outputId": "9c2a6c55-1398-45b8-d8a7-831af2ef7b06"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size : torch.Size([1, 28, 28])\n",
            "Label : 9 (Ankle boot)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flatten_model=nn.Flatten()\n",
        "x = train_features_batch[0]\n",
        "print(f\"Shape before flatten : {x.shape}\")\n",
        "x = flatten_model(x)\n",
        "print(f\"Shape after flatten : {x.shape}\")\n",
        "output = flatten_model(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jC0pJenDLi2Y",
        "outputId": "9dbb8d97-6c8b-44d8-9d64-7a6d3217d845"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before flatten : torch.Size([1, 28, 28])\n",
            "Shape after flatten : torch.Size([1, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class FaishonMNISTModelV0 (nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "      nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "75lWuQg0MJsW"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_0 = FaishonMNISTModelV0(\n",
        "    input_shape=784,\n",
        "    hidden_units=10,\n",
        "    output_shape=10\n",
        ").to(\"cpu\")\n",
        "model_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8jD4k_RMzoh",
        "outputId": "ed3d8f85-e53b-464b-e1eb-9b7053fc6487"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FaishonMNISTModelV0(\n",
              "  (layer_stack): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
              "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.rand(32,784)\n",
        "print(f\"Input shape : {dummy_input.shape}\")\n",
        "output = model_0(dummy_input)\n",
        "print(f\"Output shape : {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXHxgTd-NCFw",
        "outputId": "a73b2862-1693-4c9f-dd9b-d34ef325a8af"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape : torch.Size([32, 784])\n",
            "Output shape : torch.Size([32, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGNY_HupNPaY",
        "outputId": "1b7999aa-39ea-4168-af69-3eadd3cea596"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('layer_stack.1.weight',\n",
              "              tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],\n",
              "                      [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115],\n",
              "                      [-0.0008,  0.0017,  0.0045,  ..., -0.0127, -0.0188,  0.0059],\n",
              "                      ...,\n",
              "                      [-0.0116,  0.0273, -0.0344,  ...,  0.0176,  0.0283, -0.0011],\n",
              "                      [-0.0230,  0.0257,  0.0291,  ..., -0.0187, -0.0087,  0.0001],\n",
              "                      [ 0.0176, -0.0147,  0.0053,  ..., -0.0336, -0.0221,  0.0205]])),\n",
              "             ('layer_stack.1.bias',\n",
              "              tensor([-0.0093,  0.0283, -0.0033,  0.0255,  0.0017,  0.0037, -0.0302, -0.0123,\n",
              "                       0.0018,  0.0163])),\n",
              "             ('layer_stack.2.weight',\n",
              "              tensor([[ 0.0614, -0.0687,  0.0021,  0.2718,  0.2109,  0.1079, -0.2279, -0.1063,\n",
              "                        0.2019,  0.2847],\n",
              "                      [-0.1495,  0.1344, -0.0740,  0.2006, -0.0475, -0.2514, -0.3130, -0.0118,\n",
              "                        0.0932, -0.1864],\n",
              "                      [ 0.2488,  0.1500,  0.1907,  0.1457, -0.3050, -0.0580,  0.1643,  0.1565,\n",
              "                       -0.2877, -0.1792],\n",
              "                      [ 0.2305, -0.2618,  0.2397, -0.0610,  0.0232,  0.1542,  0.0851, -0.2027,\n",
              "                        0.1030, -0.2715],\n",
              "                      [-0.1596, -0.0555, -0.0633,  0.2302, -0.1726,  0.2654,  0.1473,  0.1029,\n",
              "                        0.2252, -0.2160],\n",
              "                      [-0.2725,  0.0118,  0.1559,  0.1596,  0.0132,  0.3024,  0.1124,  0.1366,\n",
              "                       -0.1533,  0.0965],\n",
              "                      [-0.1184, -0.2555, -0.2057, -0.1909, -0.0477, -0.1324,  0.2905,  0.1307,\n",
              "                       -0.2629,  0.0133],\n",
              "                      [ 0.2727, -0.0127,  0.0513,  0.0863, -0.1043, -0.2047, -0.1185, -0.0825,\n",
              "                        0.2488, -0.2571],\n",
              "                      [ 0.0425, -0.1209, -0.0336, -0.0281, -0.1227,  0.0730,  0.0747, -0.1816,\n",
              "                        0.1943,  0.2853],\n",
              "                      [-0.1310,  0.0645, -0.1171,  0.2168, -0.0245, -0.2820,  0.0736,  0.2621,\n",
              "                        0.0012, -0.0810]])),\n",
              "             ('layer_stack.2.bias',\n",
              "              tensor([-0.0087,  0.1791,  0.2712, -0.0791,  0.1685,  0.1762,  0.2825,  0.2266,\n",
              "                      -0.2612, -0.2613]))])"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\",\"wb\") as f:\n",
        "    f.write(request.content)\n"
      ],
      "metadata": {
        "id": "ieK_EsqLNJPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb1ceb0-ad72-4cde-bdea-06cce9d33189"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "helper_functions.py already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn"
      ],
      "metadata": {
        "id": "_PvMHF1-OVol"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),lr=0.1)"
      ],
      "metadata": {
        "id": "uNFJ6gpAOaJ2"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import  default_timer as timer\n",
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "  total_time = end - start\n",
        "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "  return total_time"
      ],
      "metadata": {
        "id": "yHQtdw_4OlOP"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = timer()\n",
        "end_time = timer()\n",
        "print_train_time(start=start_time , end=end_time ,device='cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgzlgRNkPX_I",
        "outputId": "8dd712ce-db45-4aaa-ea14-163e7359257f"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train time on cpu: 0.000 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.888100011157803e-05"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n",
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch:\", {epoch})\n",
        "  train_loss = 0\n",
        "  for batch, (X,y) in enumerate(train_dataloader):\n",
        "    model_0.train()\n",
        "    y_pred = model_0(X)\n",
        "    loss_train = loss(y_pred,y)\n",
        "    optimizer.zero_grad()\n",
        "    loss_train = loss(y_pred,y)\n",
        "    train_loss += loss_train\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 400 == 0:\n",
        "      print(f\"Looked at {batch * len(X)}/{len(train_data)} samples.\")\n",
        "\n",
        "      train_loss/=(batch+1)\n",
        "      print(f\"Train loss : {train_loss}\")\n",
        "      test_loss,test_acc = 0,0\n",
        "      model_0.eval()\n",
        "      with torch.inference_mode():\n",
        "        for X_test,y_test in test_dataloader:\n",
        "          test_pred = model_0(X_test)\n",
        "          loss_test = loss(test_pred,y_test)\n",
        "          test_loss += loss_test.clone()\n",
        "          test_acc += accuracy_fn(y_true=y_test,y_pred=test_pred.argmax(dim=1))\n",
        "      test_loss = test_loss.clone() / len(test_dataloader)\n",
        "      test_acc /= len(test_dataloader)\n",
        "      print(f\"Test loss : {test_loss}\")\n",
        "      print(f\"Test acc : {test_acc}\")\n",
        "train_time_end_on_cpu = timer()\n",
        "train_time_model_0 = print_train_time(start=train_time_start_on_cpu,end=train_time_end_on_cpu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b680e161abf443de8538f72967097d96",
            "edec48abf3be40cc89f7f43026dcf1b5",
            "558785316867421bbf03c31ef36789a3",
            "608eb08f0d3340d08ef0a6457c388424",
            "1f149fc3f992410493f44e90d71a40da",
            "42c9e059f3b64a528fcd6e15e463e46d",
            "9a572bbc1ed34d1892199e3f609afb84",
            "f2e3562531ea4b6c995e5c9d597bef12",
            "20b10f32281845869252426b96c50c31",
            "b3ecaea3bb304184b79dfec67518fe18",
            "a08777c1f21e40cdbcf6bcedf1e274e0"
          ]
        },
        "id": "Ms7wevSHPnAn",
        "outputId": "b7c96ffe-e7be-4032-ca38-7c27501378cf"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b680e161abf443de8538f72967097d96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: {0}\n",
            "Looked at 0/60000 samples.\n",
            "Train loss : 2.366476535797119\n",
            "Test loss : 2.3208961486816406\n",
            "Test acc : 15.115814696485623\n",
            "Looked at 12800/60000 samples.\n",
            "Train loss : 0.7930365800857544\n",
            "Test loss : 0.7165123820304871\n",
            "Test acc : 71.30591054313099\n",
            "Looked at 25600/60000 samples.\n",
            "Train loss : 0.28993624448776245\n",
            "Test loss : 0.5962220430374146\n",
            "Test acc : 77.78554313099042\n",
            "Looked at 38400/60000 samples.\n",
            "Train loss : 0.18200421333312988\n",
            "Test loss : 0.5310074687004089\n",
            "Test acc : 80.11182108626198\n",
            "Looked at 51200/60000 samples.\n",
            "Train loss : 0.1274300217628479\n",
            "Test loss : 0.5078408122062683\n",
            "Test acc : 81.85902555910543\n",
            "Epoch: {1}\n",
            "Looked at 0/60000 samples.\n",
            "Train loss : 0.5545473694801331\n",
            "Test loss : 0.5166405439376831\n",
            "Test acc : 82.04872204472844\n",
            "Looked at 12800/60000 samples.\n",
            "Train loss : 0.4786631166934967\n",
            "Test loss : 0.4899939000606537\n",
            "Test acc : 82.76757188498402\n",
            "Looked at 25600/60000 samples.\n",
            "Train loss : 0.24704641103744507\n",
            "Test loss : 0.4787352383136749\n",
            "Test acc : 83.13698083067092\n",
            "Looked at 38400/60000 samples.\n",
            "Train loss : 0.16029296815395355\n",
            "Test loss : 0.48660552501678467\n",
            "Test acc : 82.77755591054313\n",
            "Looked at 51200/60000 samples.\n",
            "Train loss : 0.1162223368883133\n",
            "Test loss : 0.49154964089393616\n",
            "Test acc : 82.96725239616613\n",
            "Epoch: {2}\n",
            "Looked at 0/60000 samples.\n",
            "Train loss : 0.4129005968570709\n",
            "Test loss : 0.5145924687385559\n",
            "Test acc : 81.66932907348243\n",
            "Looked at 12800/60000 samples.\n",
            "Train loss : 0.46043673157691956\n",
            "Test loss : 0.48510733246803284\n",
            "Test acc : 83.31669329073482\n",
            "Looked at 25600/60000 samples.\n",
            "Train loss : 0.23106999695301056\n",
            "Test loss : 0.518303632736206\n",
            "Test acc : 81.31988817891374\n",
            "Looked at 38400/60000 samples.\n",
            "Train loss : 0.15242676436901093\n",
            "Test loss : 0.48654496669769287\n",
            "Test acc : 83.16693290734824\n",
            "Looked at 51200/60000 samples.\n",
            "Train loss : 0.11590705811977386\n",
            "Test loss : 0.47054818272590637\n",
            "Test acc : 83.59624600638978\n",
            "Train time on None: 60.171 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: nn.Module, data_loader: DataLoader, loss_fn: nn.Module,accuracy_fn):\n",
        "  loss, acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X, y in data_loader:\n",
        "      y_pred = model(X)\n",
        "      loss += loss_fn(y_pred, y)\n",
        "      acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "  loss = loss.clone() / len(data_loader)\n",
        "  acc /= len(data_loader)\n",
        "  return {'model_name':model.__class__.__name__,'model_loss':loss.item(),'model_acc':acc}\n",
        "  model_0_results = eval_model(model=model_0,data_loader=test_dataloader,loss_fn=loss,accuracy_fn=accuracy_fn)\n",
        "  model_0_results"
      ],
      "metadata": {
        "id": "95IFtvG_TD2b"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()\n",
        "import torch\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RHqUUhkkUoy7",
        "outputId": "a49d9d4a-d3f3-4569-d3f3-5147a768f0d7"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FaishonMNISTModelV1(nn.Module):\n",
        "  def __init__(self,input_shape:int,hidden_units:int,output_shape:int):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape,out_features=hidden_units),\n",
        "        nn.Linear(in_features=hidden_units,out_features=output_shape)\n",
        "    )\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features=hidden_units,out_features=output_shape)\n",
        "  def forward(self,x):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "xymvRl37EX5F"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),lr=0.1)\n"
      ],
      "metadata": {
        "id": "reBXYX1eFU0A"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(\n",
        "    model: nn.Module,\n",
        "  data_loader: DataLoader,\n",
        "  loss_fn: nn.Module,\n",
        "  optimizer: torch.optim.Optimizer,\n",
        "  accuracy_fn,\n",
        "  device: torch.device\n",
        "):\n",
        " train_loss = 0\n",
        " for batch, (X,y) in enumerate(train_dataloader):\n",
        "  model_0.train()\n",
        "  y_pred = model_0(X)\n",
        "  loss=loss_fn(y_pred,y)\n",
        "  train_loss += loss\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if batch % 400 == 0:\n",
        "    print(f\"Looked at {batch * len(X)}/{len(train_data)} samples.\")\n",
        "  train_loss /= (batch+1)\n",
        "  print(f\"Train loss : {train_loss}\")"
      ],
      "metadata": {
        "id": "LvtvVk_TFmuy"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device):\n",
        "  test_loss,test_acc = 0,0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X_test,y_test in test_dataloader:\n",
        "      test_pred = model(X_test)\n",
        "      loss_test = loss(test_pred,y_test)\n",
        "      test_loss += loss_test.clone()\n",
        "      test_acc += accuracy_fn(y_true=y_test,y_pred=test_pred.argmax(dim=1))\n",
        "      test_accuracy = test_acc / len(test_dataloader)\n",
        "      test_loss = test_loss.clone() / len(test_dataloader)\n",
        "      print(f\"Test loss : {test_loss}\")\n",
        "      print(f\"Test acc : {test_acc}\")"
      ],
      "metadata": {
        "id": "27fSGBt-Gp6s"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_on_gpu = timer()\n",
        "train_time_end_on_gpu = timer()\n",
        "print_train_time(start=train_time_start_on_gpu,end=train_time_end_on_gpu)\n",
        "epochs = 3\n",
        "from helper_functions import print_train_time\n",
        "\n",
        "model_1 = FaishonMNISTModelV1(input_shape=784, hidden_units=10, output_shape=10).to(device)\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  train_step(model=model_0,data_loader=train_dataloader,loss_fn=loss,optimizer=optimizer,accuracy_fn=accuracy_fn,device=device)\n",
        "  print(f\"Epoch : {epoch}\")\n",
        "  test_step(model=model_1,\n",
        "            data_loader=test_dataloader,\n",
        "            loss_fn=loss,\n",
        "            accuracy_fn=accuracy_fn,\n",
        "            device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "14f2acba9ef546b9bd37f0479186a89f",
            "61aa11b39e9841c0b40f1df018a5e305",
            "31648da1b3e4432db77ccc78e1076d41",
            "b057a745a72b4774ac1e1fd564bf430d",
            "8c12a766ed0b4611a1248106fcb5ab38",
            "8e35fc05828f4e738fc552bd7905aec0",
            "7afff0fb02c14fca9d981d6847e3ae64",
            "b71b9a60d9094699855dd44174d91c1f",
            "0906df6b57374138ac4956669c369b5d",
            "daaa57208d274f9ca417b591c70a85bc",
            "ec4f3251f2794056b1ece84fe2be03ce"
          ]
        },
        "id": "dmglx21OH-O8",
        "outputId": "92559f03-4bb7-4529-fbeb-6f3413ff11bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train time on None: 0.000 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14f2acba9ef546b9bd37f0479186a89f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train loss : 0.00031720835249871016\n",
            "Train loss : 0.00033626460935920477\n",
            "Train loss : 0.0002920394763350487\n",
            "Train loss : 0.00030403464916162193\n",
            "Train loss : 0.0003565377846825868\n",
            "Train loss : 0.00036187798832543194\n",
            "Train loss : 0.00039259070763364434\n",
            "Train loss : 0.000266415678197518\n",
            "Train loss : 0.000411987624829635\n",
            "Train loss : 0.0003154953592456877\n",
            "Train loss : 0.0003129300894215703\n",
            "Train loss : 0.00027129388763569295\n",
            "Train loss : 0.0004560005327221006\n",
            "Train loss : 0.0003581219061743468\n",
            "Train loss : 0.0004843442002311349\n",
            "Train loss : 0.0002554125676397234\n",
            "Train loss : 0.00040139778866432607\n",
            "Train loss : 0.0003643762320280075\n",
            "Train loss : 0.00037869910011067986\n",
            "Train loss : 0.0002879697713069618\n",
            "Train loss : 0.00032669882057234645\n",
            "Train loss : 0.0003359744732733816\n",
            "Train loss : 0.00046686295536346734\n",
            "Train loss : 0.0005116058164276183\n",
            "Train loss : 0.00018877550610341132\n",
            "Train loss : 0.0003706048591993749\n",
            "Train loss : 0.0003386572643648833\n",
            "Train loss : 0.0003295878123026341\n",
            "Train loss : 0.0005782634834758937\n",
            "Train loss : 0.0003704299742821604\n",
            "Train loss : 5.518340185517445e-05\n",
            "Train loss : 0.00015148684906307608\n",
            "Train loss : 0.00014484890562016517\n",
            "Train loss : 0.00019914295990020037\n",
            "Train loss : 0.00021110706438776106\n",
            "Train loss : 0.000411862856708467\n",
            "Train loss : 0.00048202730249613523\n",
            "Train loss : 0.00022586942941416055\n",
            "Train loss : 0.0003372773062437773\n",
            "Train loss : 0.000436976901255548\n",
            "Train loss : 0.00021120233577676117\n",
            "Train loss : 0.0005288464599289\n",
            "Train loss : 0.0003820441197603941\n",
            "Train loss : 0.0002896135556511581\n",
            "Train loss : 0.00020256270363461226\n",
            "Train loss : 0.00032634433591738343\n",
            "Train loss : 0.0003858783165924251\n",
            "Train loss : 0.0003568885149434209\n",
            "Train loss : 0.0004469157720450312\n",
            "Train loss : 0.0004271022044122219\n",
            "Train loss : 0.0006040312582626939\n",
            "Train loss : 0.000517283333465457\n",
            "Train loss : 0.00035265294718556106\n",
            "Train loss : 0.00029992617783136666\n",
            "Train loss : 0.00035412205033935606\n",
            "Train loss : 0.0003744302084669471\n",
            "Train loss : 0.0003884246398229152\n",
            "Train loss : 0.00039333017775788903\n",
            "Train loss : 0.00033202330814674497\n",
            "Train loss : 0.00021553231636062264\n",
            "Train loss : 0.0003294563211966306\n",
            "Train loss : 0.0002956598764285445\n",
            "Train loss : 0.00019492233695928007\n",
            "Train loss : 0.0003020663862116635\n",
            "Train loss : 0.0006233991589397192\n",
            "Train loss : 0.0002787772682495415\n",
            "Train loss : 0.00036608712980523705\n",
            "Train loss : 0.000662955513689667\n",
            "Train loss : 0.0003164986555930227\n",
            "Train loss : 0.00026070454623550177\n",
            "Train loss : 0.00021676975302398205\n",
            "Train loss : 0.000322833308018744\n",
            "Train loss : 0.0003006387851200998\n",
            "Train loss : 0.0002127872285200283\n",
            "Train loss : 0.00016906547534745187\n",
            "Train loss : 0.0005344039527699351\n",
            "Train loss : 0.0005814135074615479\n",
            "Train loss : 0.0003788597823586315\n",
            "Train loss : 0.00031595598557032645\n",
            "Train loss : 0.0003345325530972332\n",
            "Looked at 38400/60000 samples.\n",
            "Train loss : 0.00044419849291443825\n",
            "Train loss : 0.0005794566823169589\n",
            "Train loss : 0.00021444396406877786\n",
            "Train loss : 0.0003159649495501071\n",
            "Train loss : 0.0005335025489330292\n",
            "Train loss : 0.0002333844022359699\n",
            "Train loss : 0.0004601847904268652\n",
            "Train loss : 0.0002628405636642128\n",
            "Train loss : 0.00031695549841970205\n",
            "Train loss : 0.000349662994267419\n",
            "Train loss : 0.000525882001966238\n",
            "Train loss : 0.00042958278208971024\n",
            "Train loss : 0.0003115519939456135\n",
            "Train loss : 0.0005085411248728633\n",
            "Train loss : 0.00026626489125192165\n",
            "Train loss : 0.00035849306732416153\n",
            "Train loss : 0.0002958174445666373\n",
            "Train loss : 0.00020423873502295464\n",
            "Train loss : 0.00033900252310559154\n",
            "Train loss : 0.0002783175732474774\n",
            "Train loss : 0.00042646864312700927\n",
            "Train loss : 0.00017596369434613734\n",
            "Train loss : 0.0005658163572661579\n",
            "Train loss : 0.00039581424789503217\n",
            "Train loss : 0.00030055930255912244\n",
            "Train loss : 0.0002492720086593181\n",
            "Train loss : 0.0003184369998052716\n",
            "Train loss : 0.0003015750553458929\n",
            "Train loss : 0.0002949551271740347\n",
            "Train loss : 0.000556475599296391\n",
            "Train loss : 0.00029404490487650037\n",
            "Train loss : 0.00024346444115508348\n",
            "Train loss : 0.00032909485162235796\n",
            "Train loss : 0.00038021805812604725\n",
            "Train loss : 0.0004169101594015956\n",
            "Train loss : 0.00017871937598101795\n",
            "Train loss : 0.0003936579742003232\n",
            "Train loss : 0.00043380045099183917\n",
            "Train loss : 0.00022688171884510666\n",
            "Train loss : 0.0005104778683744371\n",
            "Train loss : 0.00028944184305146337\n",
            "Train loss : 0.00021463308075908571\n",
            "Train loss : 0.0002993887464981526\n",
            "Train loss : 0.00023307064839173108\n",
            "Train loss : 0.0005757188191637397\n",
            "Train loss : 0.0002827576536219567\n",
            "Train loss : 0.00017151868087239563\n",
            "Train loss : 0.0003340803668834269\n",
            "Train loss : 0.0004093552415724844\n",
            "Train loss : 0.000279442872852087\n",
            "Train loss : 0.00022155506303533912\n",
            "Train loss : 0.00023456734197679907\n",
            "Train loss : 0.0003663462121039629\n",
            "Train loss : 0.000358964636689052\n",
            "Train loss : 0.00034775942913256586\n",
            "Train loss : 0.00021744232799392194\n",
            "Train loss : 0.0004003896319773048\n",
            "Train loss : 0.0002482535783201456\n",
            "Train loss : 0.00042154351831413805\n",
            "Train loss : 0.0005022167460992932\n",
            "Train loss : 0.0004644495202228427\n",
            "Train loss : 0.0004525940748862922\n",
            "Train loss : 0.0004115509509574622\n",
            "Train loss : 0.0002695433213375509\n",
            "Train loss : 0.0003595149319153279\n",
            "Train loss : 0.0004343601467553526\n",
            "Train loss : 0.0003351082850713283\n",
            "Train loss : 0.0002079153200611472\n",
            "Train loss : 0.0003099753230344504\n",
            "Train loss : 0.00036531133810058236\n",
            "Train loss : 0.00039269670378416777\n",
            "Train loss : 0.00031519500771537423\n",
            "Train loss : 0.00017337966710329056\n",
            "Train loss : 0.0003857519186567515\n",
            "Train loss : 0.00011595297110034153\n",
            "Train loss : 0.0006632909644395113\n",
            "Train loss : 0.00020600999414455146\n",
            "Train loss : 0.00026051129680126905\n",
            "Train loss : 0.00035524831037037075\n",
            "Train loss : 0.00017052996554411948\n",
            "Train loss : 0.0006384867592714727\n",
            "Train loss : 0.0003987083618994802\n",
            "Train loss : 0.0002357991033932194\n",
            "Train loss : 0.0002068095054710284\n",
            "Train loss : 0.00022045367222744972\n",
            "Train loss : 0.0003238152130506933\n",
            "Train loss : 0.00025789299979805946\n",
            "Train loss : 0.0001668834884185344\n",
            "Train loss : 0.00024675848544575274\n",
            "Train loss : 0.000175866560311988\n",
            "Train loss : 0.0002526883617974818\n",
            "Train loss : 0.0006475999252870679\n",
            "Train loss : 0.0002155197726096958\n",
            "Train loss : 0.00023796292953193188\n",
            "Train loss : 0.00029005459509789944\n",
            "Train loss : 0.0002341902581974864\n",
            "Train loss : 0.0002620463783387095\n",
            "Train loss : 0.00018731159798335284\n",
            "Train loss : 0.0004187587765045464\n",
            "Train loss : 0.0004876225139014423\n",
            "Train loss : 0.0002021114487433806\n",
            "Train loss : 0.0005088773905299604\n",
            "Train loss : 0.0005128962802700698\n",
            "Train loss : 0.000403872923925519\n",
            "Train loss : 0.00030459905974566936\n",
            "Train loss : 0.00031693893834017217\n",
            "Train loss : 0.0002715890295803547\n",
            "Train loss : 0.000264879286987707\n",
            "Train loss : 0.0005062127602286637\n",
            "Train loss : 0.00039111135993152857\n",
            "Train loss : 0.00032474257750436664\n",
            "Train loss : 0.0003045850608032197\n",
            "Train loss : 0.0002468762977514416\n",
            "Train loss : 0.0006302815745584667\n",
            "Train loss : 0.00025222645490430295\n",
            "Train loss : 0.0002421110839350149\n",
            "Train loss : 0.00039916668902151287\n",
            "Train loss : 0.00045846772263757885\n",
            "Train loss : 0.0002906200534198433\n",
            "Train loss : 0.0001805053761927411\n",
            "Train loss : 0.0002002101537073031\n",
            "Train loss : 0.00015065119077917188\n",
            "Train loss : 0.0003371864149812609\n",
            "Train loss : 0.0003013472305610776\n",
            "Train loss : 0.0004625382134690881\n",
            "Train loss : 0.00013743987074121833\n",
            "Train loss : 0.0002550044737290591\n",
            "Train loss : 0.0005432626930996776\n",
            "Train loss : 0.00015180397895164788\n",
            "Train loss : 0.0005663328338414431\n",
            "Train loss : 0.0003684336261358112\n",
            "Train loss : 0.0005575717077590525\n",
            "Train loss : 0.0002619026054162532\n",
            "Train loss : 0.0003520373720675707\n",
            "Train loss : 0.00020580970158334821\n",
            "Train loss : 0.0003605687525123358\n",
            "Train loss : 0.00041284941835328937\n",
            "Train loss : 0.0005527728353627026\n",
            "Train loss : 0.00028375390684232116\n",
            "Train loss : 0.000250184879405424\n",
            "Train loss : 0.0005728380638174713\n",
            "Train loss : 0.0004289432836230844\n",
            "Train loss : 0.0002738738839980215\n",
            "Train loss : 0.00030397408409044147\n",
            "Train loss : 0.00044522745884023607\n",
            "Train loss : 0.0003618353803176433\n",
            "Train loss : 0.000897437974344939\n",
            "Train loss : 0.00014951849880162627\n",
            "Train loss : 0.00014581141294911504\n",
            "Train loss : 0.0001902339863590896\n",
            "Train loss : 0.0002362401137361303\n",
            "Train loss : 0.00029149127658456564\n",
            "Train loss : 0.0003776676603592932\n",
            "Train loss : 8.308590622618794e-05\n",
            "Train loss : 0.0005630237865261734\n",
            "Train loss : 0.00010266703611705452\n",
            "Train loss : 0.0002877225633710623\n",
            "Train loss : 0.0002918340323958546\n",
            "Train loss : 0.00021624712098855525\n",
            "Train loss : 0.0005244964268058538\n",
            "Train loss : 0.0003735420759767294\n",
            "Train loss : 0.0002260238106828183\n",
            "Train loss : 0.00028286559972912073\n",
            "Train loss : 0.00047643465222790837\n",
            "Train loss : 0.0002101960708387196\n",
            "Train loss : 0.0003143185458611697\n",
            "Train loss : 0.0003271577297709882\n",
            "Train loss : 0.00033267377875745296\n",
            "Train loss : 0.00015431898646056652\n",
            "Train loss : 0.00039098222623579204\n",
            "Train loss : 0.0005113153601996601\n",
            "Train loss : 0.000376259587937966\n",
            "Train loss : 0.00034284923458471894\n",
            "Train loss : 0.0003208169946447015\n",
            "Train loss : 0.0002161834272556007\n",
            "Train loss : 0.00046607994590885937\n",
            "Train loss : 0.00020658716675825417\n",
            "Train loss : 0.0003388579934835434\n",
            "Train loss : 0.00031760925776325166\n",
            "Train loss : 0.0006006687181070447\n",
            "Train loss : 0.0002914497454185039\n",
            "Train loss : 0.0002637582365423441\n",
            "Train loss : 0.00017304734501522034\n",
            "Train loss : 0.0003142152854707092\n",
            "Train loss : 0.00046517065493389964\n",
            "Train loss : 0.0002685442450456321\n",
            "Train loss : 0.00033510744106024504\n",
            "Train loss : 0.00047666809405200183\n",
            "Train loss : 0.0003661610826384276\n",
            "Train loss : 0.000604775792453438\n",
            "Train loss : 0.0005707150558009744\n",
            "Train loss : 0.0003768576425500214\n",
            "Train loss : 0.00034537658211775124\n",
            "Train loss : 0.00019003315537702292\n",
            "Train loss : 0.0002853962068911642\n",
            "Train loss : 0.0004899874911643565\n",
            "Train loss : 0.00038289380609057844\n",
            "Train loss : 0.0003448343195486814\n",
            "Train loss : 0.0002085811283905059\n",
            "Train loss : 0.00034346975735388696\n",
            "Train loss : 0.0005355825996957719\n",
            "Train loss : 0.0004865737573709339\n",
            "Train loss : 0.0004022763459943235\n",
            "Train loss : 0.0002863613481167704\n",
            "Train loss : 0.00045727554243057966\n",
            "Train loss : 0.0003584780206438154\n",
            "Train loss : 0.0002452945336699486\n",
            "Train loss : 0.00026838446501642466\n",
            "Train loss : 0.0002579927968326956\n",
            "Train loss : 0.0004719096759799868\n",
            "Train loss : 0.0005763072986155748\n",
            "Train loss : 0.00033864626311697066\n",
            "Train loss : 0.00031400268198922276\n",
            "Train loss : 0.0002779260103125125\n",
            "Train loss : 0.0004397276497911662\n",
            "Train loss : 0.00027892953949049115\n",
            "Train loss : 0.00034369679633527994\n",
            "Train loss : 0.00018661111244000494\n",
            "Train loss : 0.00023734578280709684\n",
            "Train loss : 0.0004485522804316133\n",
            "Train loss : 0.0006670303991995752\n",
            "Train loss : 0.0004978791694156826\n",
            "Train loss : 0.000305583409499377\n",
            "Train loss : 0.00020289655367378145\n",
            "Train loss : 0.0005094754742458463\n",
            "Train loss : 0.0003890732186846435\n",
            "Train loss : 0.00039976093103177845\n",
            "Train loss : 0.0004908577539026737\n",
            "Train loss : 0.00024978574947454035\n",
            "Train loss : 0.000320856663165614\n",
            "Train loss : 0.0002622562169563025\n",
            "Train loss : 0.00017634996038395911\n",
            "Train loss : 0.0005102859577164054\n",
            "Train loss : 0.00033946646726690233\n",
            "Train loss : 0.00029837171314284205\n",
            "Train loss : 0.0003119650937151164\n",
            "Train loss : 0.00048176522250287235\n",
            "Train loss : 0.0004309058131184429\n",
            "Train loss : 0.00032265327172353864\n",
            "Train loss : 0.00019207617151550949\n",
            "Train loss : 0.00032578554237261415\n",
            "Train loss : 0.0004391986003611237\n",
            "Train loss : 0.00044094776967540383\n",
            "Train loss : 0.00033225768129341304\n",
            "Train loss : 0.0002876359212677926\n",
            "Train loss : 0.00013743322051595896\n",
            "Train loss : 0.0003118184977211058\n",
            "Train loss : 0.00017965491861104965\n",
            "Train loss : 0.00028992831357754767\n",
            "Train loss : 0.00019131095905322582\n",
            "Train loss : 0.0005366685800254345\n",
            "Train loss : 0.00031895368010737\n",
            "Train loss : 0.0003532097034621984\n",
            "Train loss : 0.0005966611788608134\n",
            "Train loss : 0.0004016454622615129\n",
            "Train loss : 0.0004468279657885432\n",
            "Train loss : 0.0005133869708515704\n",
            "Train loss : 0.00036868080496788025\n",
            "Train loss : 0.00032167130848392844\n",
            "Train loss : 0.0002564512542448938\n",
            "Train loss : 0.00019228299788665026\n",
            "Train loss : 0.000165687088156119\n",
            "Train loss : 0.000535346451215446\n",
            "Train loss : 0.00043207628186792135\n",
            "Train loss : 0.0004086309636477381\n",
            "Train loss : 0.00033732212614268064\n",
            "Train loss : 0.00026755116414278746\n",
            "Train loss : 0.00031604908872395754\n",
            "Train loss : 0.00039243162609636784\n",
            "Train loss : 0.0003123540955130011\n",
            "Train loss : 0.00035430811112746596\n",
            "Train loss : 0.00026541241095401347\n",
            "Train loss : 0.0001996274513658136\n",
            "Train loss : 0.00044371391413733363\n",
            "Train loss : 0.00044440696365199983\n",
            "Train loss : 0.0002336084289709106\n",
            "Train loss : 0.0005387776182033122\n",
            "Train loss : 0.0005002066609449685\n",
            "Train loss : 0.0003603709046728909\n",
            "Train loss : 0.0002726083330344409\n",
            "Train loss : 0.0002871113538276404\n",
            "Train loss : 0.00012034101382596418\n",
            "Train loss : 0.00047487294068560004\n",
            "Train loss : 0.00033529376378282905\n",
            "Train loss : 0.00012884200259577483\n",
            "Train loss : 0.00022342312149703503\n",
            "Train loss : 0.00038587706512771547\n",
            "Train loss : 0.0002708094543777406\n",
            "Train loss : 0.00021317496430128813\n",
            "Train loss : 0.00022282027930486947\n",
            "Train loss : 0.00019341969164088368\n",
            "Train loss : 0.0003434861428104341\n",
            "Train loss : 0.00039512652438133955\n",
            "Train loss : 0.000316890625981614\n",
            "Train loss : 0.00025138785713352263\n",
            "Train loss : 0.0004549695586320013\n",
            "Train loss : 0.0003324546851217747\n",
            "Train loss : 0.00011110216291854158\n",
            "Train loss : 0.0002861922257579863\n",
            "Train loss : 0.00023651325318496674\n",
            "Train loss : 0.0002573766396380961\n",
            "Train loss : 0.00022986039402894676\n",
            "Train loss : 0.00019255970255471766\n",
            "Train loss : 0.0002566376351751387\n",
            "Train loss : 0.00024041133292485029\n",
            "Train loss : 0.0003977439773734659\n",
            "Train loss : 0.0003823390288744122\n",
            "Train loss : 0.00013281081919558346\n",
            "Train loss : 0.000338915444444865\n",
            "Train loss : 0.0002439521049382165\n",
            "Train loss : 0.0002798776258714497\n",
            "Train loss : 0.00023533405328635126\n",
            "Train loss : 0.00015686867118347436\n",
            "Train loss : 0.00015407984028570354\n",
            "Train loss : 0.0002537037944421172\n",
            "Train loss : 0.00027750455774366856\n",
            "Train loss : 0.0002399268269073218\n",
            "Train loss : 0.00019633624469861388\n",
            "Train loss : 0.0003627771802712232\n",
            "Train loss : 0.00017725140787661076\n",
            "Train loss : 0.00021291837038006634\n",
            "Train loss : 0.0004198696988169104\n",
            "Train loss : 0.0002739628835115582\n",
            "Train loss : 0.000583911722060293\n",
            "Train loss : 0.0001424864458385855\n",
            "Train loss : 0.00036197432200424373\n",
            "Train loss : 0.0004842719354201108\n",
            "Train loss : 0.00031385847250930965\n",
            "Train loss : 0.00024291199224535376\n",
            "Train loss : 0.0004989286535419524\n",
            "Train loss : 0.00019098023767583072\n",
            "Train loss : 0.00024005939485505223\n",
            "Train loss : 0.0003800281847361475\n",
            "Train loss : 0.0001408921816619113\n",
            "Train loss : 0.00023578120453748852\n",
            "Train loss : 0.00016984414833132178\n",
            "Train loss : 0.00016118343046400696\n",
            "Train loss : 0.0002933134092018008\n",
            "Train loss : 0.00037590303691104054\n",
            "Train loss : 0.00019445781072136015\n",
            "Train loss : 0.0001593722408870235\n",
            "Train loss : 0.00016353910905309021\n",
            "Train loss : 0.0002197060384787619\n",
            "Train loss : 0.00023548214812763035\n",
            "Train loss : 0.00029966136207804084\n",
            "Train loss : 0.00040770380292087793\n",
            "Train loss : 0.00018789578462019563\n",
            "Train loss : 0.0002657177101355046\n",
            "Train loss : 0.00030542947934009135\n",
            "Train loss : 0.0004116254858672619\n",
            "Train loss : 0.00025390172959305346\n",
            "Train loss : 0.00021967993234284222\n",
            "Train loss : 0.0001380519533995539\n",
            "Train loss : 0.00010724488674895838\n",
            "Train loss : 0.0004289932257961482\n",
            "Train loss : 0.0003785490698646754\n",
            "Train loss : 0.00019704957958310843\n",
            "Train loss : 0.000365758576663211\n",
            "Train loss : 0.00018569137318991125\n",
            "Train loss : 0.000365706771844998\n",
            "Train loss : 0.00025788365746848285\n",
            "Train loss : 0.00026126857846975327\n",
            "Train loss : 0.00016462491475977004\n",
            "Train loss : 0.00020524066349025816\n",
            "Train loss : 0.00040848355274647474\n",
            "Train loss : 0.00021818940876983106\n",
            "Train loss : 0.0003384880837984383\n",
            "Train loss : 0.00025524268858134747\n",
            "Train loss : 0.00028897964511998\n",
            "Train loss : 0.00013490502897184342\n",
            "Train loss : 0.00026149905170314014\n",
            "Train loss : 0.00013446944649331272\n",
            "Train loss : 0.0002185739140259102\n",
            "Train loss : 0.00013556527846958488\n",
            "Train loss : 0.00019410964159760624\n",
            "Train loss : 0.0004269184428267181\n",
            "Train loss : 0.00022217130754143\n",
            "Train loss : 0.0002766885154414922\n",
            "Train loss : 0.0004342143947724253\n",
            "Train loss : 0.00042706821113824844\n",
            "Train loss : 0.0002655383723322302\n",
            "Train loss : 0.000344015919836238\n",
            "Train loss : 0.00023794487060513347\n",
            "Train loss : 0.00038372213020920753\n",
            "Train loss : 0.0004011494165752083\n",
            "Train loss : 8.197937131626531e-05\n",
            "Train loss : 0.00012616733147297055\n",
            "Train loss : 0.00033567106584087014\n",
            "Train loss : 0.0001832100679166615\n",
            "Train loss : 0.0005669257370755076\n",
            "Train loss : 0.00018679503409657627\n",
            "Train loss : 0.00023364908702205867\n",
            "Train loss : 0.0002755671157501638\n",
            "Train loss : 0.0002047470334218815\n",
            "Train loss : 0.0001356929133180529\n",
            "Train loss : 0.00033634385908953846\n",
            "Train loss : 0.0002164721954613924\n",
            "Train loss : 0.00015021856233943254\n",
            "Train loss : 0.00012740852253045887\n",
            "Train loss : 0.00020903062249999493\n",
            "Looked at 51200/60000 samples.\n",
            "Train loss : 0.00025745955645106733\n",
            "Train loss : 0.0001943737588590011\n",
            "Train loss : 0.00018488890782464296\n",
            "Train loss : 0.00016150549345184118\n",
            "Train loss : 0.0001805423089535907\n",
            "Train loss : 0.0002992979425471276\n",
            "Train loss : 0.00019113352755084634\n",
            "Train loss : 0.0002557053812779486\n",
            "Train loss : 0.00021438444673549384\n",
            "Train loss : 0.00024516929988749325\n",
            "Train loss : 0.00027251109713688493\n",
            "Train loss : 0.00011442899995017797\n",
            "Train loss : 0.0003788476751651615\n",
            "Train loss : 0.00020757174934260547\n",
            "Train loss : 0.0005948892212472856\n",
            "Train loss : 0.0003105165669694543\n",
            "Train loss : 0.00014356450992636383\n",
            "Train loss : 0.0001472408912377432\n",
            "Train loss : 0.00018185799126513302\n",
            "Train loss : 0.0004076446348335594\n",
            "Train loss : 0.00024433434009552\n",
            "Train loss : 0.0005356198525987566\n",
            "Train loss : 0.00012418729602359235\n",
            "Train loss : 0.00031611465965397656\n",
            "Train loss : 0.0003066607459913939\n",
            "Train loss : 0.00023888892610557377\n",
            "Train loss : 0.0003145086520817131\n",
            "Train loss : 0.00022923536016605794\n",
            "Train loss : 0.00019697037350852042\n",
            "Train loss : 0.00020405890245456249\n",
            "Train loss : 0.0002427500585326925\n",
            "Train loss : 0.00014801645011175424\n",
            "Train loss : 0.00030610436806455255\n",
            "Train loss : 0.0002629030786920339\n",
            "Train loss : 0.00034671343746595085\n",
            "Train loss : 0.00023441015218850225\n",
            "Train loss : 0.00015521884779445827\n",
            "Train loss : 0.0003201618092134595\n",
            "Train loss : 0.0003073206462431699\n",
            "Train loss : 0.0003650540893431753\n",
            "Train loss : 0.0004057598998770118\n",
            "Train loss : 0.00033818144584074616\n",
            "Train loss : 0.0003399582637939602\n",
            "Train loss : 0.0003155812155455351\n",
            "Train loss : 0.00040304014692083\n",
            "Train loss : 0.000195094762602821\n",
            "Train loss : 0.0003234054602216929\n",
            "Train loss : 0.0006263303221203387\n",
            "Train loss : 0.00019888102542608976\n",
            "Train loss : 0.00031999649945646524\n",
            "Train loss : 0.00028855918208137155\n",
            "Train loss : 0.00031651719473302364\n",
            "Train loss : 0.00020797207253053784\n",
            "Train loss : 0.00031262426637113094\n",
            "Train loss : 0.0002033448254223913\n",
            "Train loss : 0.00027619238244369626\n",
            "Train loss : 0.000309017050312832\n",
            "Train loss : 0.0002756694739218801\n",
            "Train loss : 0.00023741282348055393\n",
            "Train loss : 0.00019732453802134842\n",
            "Train loss : 0.00047493260353803635\n",
            "Train loss : 0.00019507494289427996\n",
            "Train loss : 0.0002406921557849273\n",
            "Train loss : 9.007874905364588e-05\n",
            "Train loss : 0.00016026983212213963\n",
            "Train loss : 0.00027988097281195223\n",
            "Train loss : 0.00018674714374355972\n",
            "Train loss : 0.00043366343015804887\n",
            "Train loss : 0.0001567392173456028\n",
            "Train loss : 0.0002747773251030594\n",
            "Train loss : 0.0002546612813603133\n",
            "Train loss : 0.00013689525076188147\n",
            "Train loss : 0.0001764611224643886\n",
            "Train loss : 0.00029126027948223054\n",
            "Train loss : 0.0004002546484116465\n",
            "Train loss : 0.00016668946773279458\n",
            "Train loss : 0.0001296018745051697\n",
            "Train loss : 0.00038506934652104974\n",
            "Train loss : 0.00023382258950732648\n",
            "Train loss : 9.445405885344371e-05\n",
            "Train loss : 0.0002368550340179354\n",
            "Train loss : 0.00023159562260843813\n",
            "Train loss : 0.00025248428573831916\n",
            "Train loss : 0.0002840925590135157\n",
            "Train loss : 0.00023945853172335774\n",
            "Train loss : 0.0001341163006145507\n",
            "Train loss : 0.00025450222892686725\n",
            "Train loss : 0.00013116037007421255\n",
            "Train loss : 0.000250146520556882\n",
            "Train loss : 0.0003381504211574793\n",
            "Train loss : 8.017079380806535e-05\n",
            "Train loss : 0.00048326581600122154\n",
            "Train loss : 0.00026828006957657635\n",
            "Train loss : 0.00024060400028247386\n",
            "Train loss : 0.0003259896475356072\n",
            "Train loss : 0.00024124507035594434\n",
            "Train loss : 9.535397839499637e-05\n",
            "Train loss : 0.00016950037388596684\n",
            "Train loss : 0.00026824555243365467\n",
            "Train loss : 0.000432694039773196\n",
            "Train loss : 0.00023804632655810565\n",
            "Train loss : 0.0002781868679448962\n",
            "Train loss : 0.0001491791190346703\n",
            "Train loss : 0.00022590764274355024\n",
            "Train loss : 0.0003655143082141876\n",
            "Train loss : 0.0003731965261977166\n",
            "Train loss : 0.00016201006656046957\n",
            "Train loss : 0.0003986443334724754\n",
            "Train loss : 0.0003885377082042396\n",
            "Train loss : 0.0005241146427579224\n",
            "Train loss : 0.0003871748049277812\n",
            "Train loss : 0.00024818506790325046\n",
            "Train loss : 0.00017999247938860208\n",
            "Train loss : 0.00036805792478844523\n",
            "Train loss : 0.0003066426143050194\n",
            "Train loss : 0.00034582294756546617\n",
            "Train loss : 0.0006771351327188313\n",
            "Train loss : 0.00022754192468710244\n",
            "Train loss : 0.00021818478126078844\n",
            "Train loss : 0.0004353375406935811\n",
            "Train loss : 0.00019974134920630604\n",
            "Train loss : 0.0005039219977334142\n",
            "Train loss : 0.00030590040842071176\n",
            "Train loss : 0.00014287634985521436\n",
            "Train loss : 0.00030356788192875683\n",
            "Train loss : 0.00019075504678767174\n",
            "Train loss : 0.00011865881242556497\n",
            "Train loss : 0.00039574431139044464\n",
            "Train loss : 0.00013894498988520354\n",
            "Train loss : 0.0001369765232084319\n",
            "Train loss : 0.0002690131077542901\n",
            "Train loss : 0.00027302405214868486\n",
            "Train loss : 0.000330963172018528\n",
            "Train loss : 0.00012161063932580873\n",
            "Train loss : 0.0002579473948571831\n",
            "Train loss : 0.0002073973446385935\n",
            "Train loss : 0.0002735407033469528\n",
            "Train loss : 0.00022260317928157747\n",
            "Train loss : 0.00018329813610762358\n",
            "Train loss : 0.0003691221645567566\n",
            "Train loss : 0.0002999691932927817\n",
            "Train loss : 0.00024559092707931995\n",
            "Train loss : 0.00023478569346480072\n",
            "Train loss : 0.0003229674184694886\n",
            "Train loss : 0.00018888452905230224\n",
            "Train loss : 0.0004017163591925055\n",
            "Train loss : 0.00023715106362942606\n",
            "Train loss : 0.0002428567095194012\n",
            "Train loss : 0.0006126489024609327\n",
            "Train loss : 0.00021051800285931677\n",
            "Train loss : 0.0003054612025152892\n",
            "Train loss : 0.0003313987981528044\n",
            "Train loss : 0.00011374895257176831\n",
            "Train loss : 0.00021146649669390172\n",
            "Train loss : 0.00016236254305113107\n",
            "Train loss : 0.00023981994308996946\n",
            "Train loss : 0.0001883053919300437\n",
            "Train loss : 0.0003896031412295997\n",
            "Train loss : 0.000394083239370957\n",
            "Train loss : 0.00021977016876917332\n",
            "Train loss : 0.00017666869098320603\n",
            "Train loss : 0.00015203103248495609\n",
            "Train loss : 0.00036332960007712245\n",
            "Train loss : 0.00019128202984575182\n",
            "Train loss : 0.00011463341070339084\n",
            "Train loss : 0.00022890104446560144\n",
            "Train loss : 0.0002063409483525902\n",
            "Train loss : 0.00022840691963210702\n",
            "Train loss : 0.00015329291636589915\n",
            "Train loss : 0.00034484712523408234\n",
            "Train loss : 0.00025376028497703373\n",
            "Train loss : 0.00020898888760711998\n",
            "Train loss : 0.0002468590100761503\n",
            "Train loss : 0.00042977239354513586\n",
            "Train loss : 0.00017973885405808687\n",
            "Train loss : 0.0002374857576796785\n",
            "Train loss : 0.0002537805412430316\n",
            "Train loss : 0.0002793735184241086\n",
            "Train loss : 0.0002490441838745028\n",
            "Train loss : 0.00021300245134625584\n",
            "Train loss : 0.0003045080811716616\n",
            "Train loss : 0.0001996812061406672\n",
            "Train loss : 0.0003039544099010527\n",
            "Train loss : 0.000222694463445805\n",
            "Train loss : 0.0003475986304692924\n",
            "Train loss : 7.289205677807331e-05\n",
            "Train loss : 0.00022988260025158525\n",
            "Train loss : 0.0002090344059979543\n",
            "Train loss : 0.00038038435741327703\n",
            "Train loss : 0.00017280822794418782\n",
            "Train loss : 0.00026177216204814613\n",
            "Train loss : 0.0002389112050877884\n",
            "Train loss : 0.00014633126556873322\n",
            "Train loss : 0.00014768932305742055\n",
            "Train loss : 0.00011900842218892649\n",
            "Train loss : 0.00016310738283209503\n",
            "Train loss : 0.000262557587120682\n",
            "Train loss : 0.00042191133252345026\n",
            "Train loss : 0.0002451568143442273\n",
            "Train loss : 0.00026078333030454814\n",
            "Train loss : 0.00030855508521199226\n",
            "Train loss : 0.00014603364979848266\n",
            "Train loss : 0.0001263497833861038\n",
            "Train loss : 0.0001895081950351596\n",
            "Train loss : 0.00023424754908774048\n",
            "Train loss : 0.00034563153167255223\n",
            "Train loss : 0.0003547663800418377\n",
            "Train loss : 0.00016721485008019954\n",
            "Train loss : 0.0002782113733701408\n",
            "Train loss : 0.00016129747382365167\n",
            "Train loss : 0.00029890122823417187\n",
            "Train loss : 0.0003063129261136055\n",
            "Train loss : 0.0002489089092705399\n",
            "Train loss : 0.0002649320231284946\n",
            "Train loss : 0.00017770149861462414\n",
            "Train loss : 0.00015636325406376272\n",
            "Train loss : 0.00018061482114717364\n",
            "Train loss : 0.00015446172619704157\n",
            "Train loss : 0.00028485406073741615\n",
            "Train loss : 0.0001445404050173238\n",
            "Train loss : 0.00014905602438375354\n",
            "Train loss : 0.0002535554813221097\n",
            "Train loss : 0.00014900724636390805\n",
            "Train loss : 0.00019312601943966\n",
            "Train loss : 8.480391261400655e-05\n",
            "Train loss : 0.00017904698324855417\n",
            "Train loss : 0.00016625695570837706\n",
            "Train loss : 0.00016455940203741193\n",
            "Train loss : 0.00015694253670517355\n",
            "Train loss : 0.0001843526551965624\n",
            "Train loss : 0.0002808553399518132\n",
            "Train loss : 0.0001918880152516067\n",
            "Train loss : 0.00027201083139516413\n",
            "Train loss : 0.0002909935137722641\n",
            "Train loss : 0.00033205130603164434\n",
            "Train loss : 0.00015404231089632958\n",
            "Train loss : 0.00020906593999825418\n",
            "Train loss : 0.0002202521573053673\n",
            "Train loss : 0.00014422224194277078\n",
            "Train loss : 0.000273052544798702\n",
            "Train loss : 0.00021387689048424363\n",
            "Train loss : 0.00012818504183087498\n",
            "Train loss : 0.0005323560326360166\n",
            "Train loss : 0.0002044840803137049\n",
            "Train loss : 0.0001327866775682196\n",
            "Train loss : 0.00038153439527377486\n",
            "Train loss : 0.00023450839216820896\n",
            "Train loss : 0.00021273919264785945\n",
            "Train loss : 0.0003071325190830976\n",
            "Train loss : 0.00021016350365243852\n",
            "Train loss : 0.00011749368422897533\n",
            "Train loss : 0.00021798374655190855\n",
            "Train loss : 0.00017074156494345516\n",
            "Train loss : 0.00030949123902246356\n",
            "Train loss : 0.0001330288650933653\n",
            "Train loss : 0.00024884447338990867\n",
            "Train loss : 0.00017422651581000537\n",
            "Train loss : 0.00014687619113828987\n",
            "Train loss : 0.00012780315591953695\n",
            "Train loss : 0.0001969341974472627\n",
            "Train loss : 0.00020600933930836618\n",
            "Train loss : 0.00022458803141489625\n",
            "Train loss : 0.0002597725542727858\n",
            "Train loss : 0.00021086503693368286\n",
            "Train loss : 5.088943362352438e-05\n",
            "Train loss : 0.00013455537555273622\n",
            "Train loss : 0.0002261151239508763\n",
            "Train loss : 0.00018715913756750524\n",
            "Train loss : 0.00022678039385937154\n",
            "Train loss : 0.00025820560404099524\n",
            "Train loss : 0.00017114277579821646\n",
            "Train loss : 0.00017837996711023152\n",
            "Train loss : 0.00017918292724061757\n",
            "Train loss : 0.0002452021581120789\n",
            "Train loss : 0.00015150895342230797\n",
            "Epoch : 0\n",
            "Test loss : 0.007319142576307058\n",
            "Test acc : 9.375\n",
            "Test loss : 0.007444972172379494\n",
            "Test acc : 18.75\n",
            "Test loss : 0.007525293156504631\n",
            "Test acc : 21.875\n",
            "Test loss : 0.00752651272341609\n",
            "Test acc : 31.25\n",
            "Test loss : 0.007272820919752121\n",
            "Test acc : 40.625\n",
            "Test loss : 0.007341706193983555\n",
            "Test acc : 56.25\n",
            "Test loss : 0.007378073874861002\n",
            "Test acc : 68.75\n",
            "Test loss : 0.00725072855129838\n",
            "Test acc : 81.25\n",
            "Test loss : 0.007652316242456436\n",
            "Test acc : 84.375\n",
            "Test loss : 0.007418801076710224\n",
            "Test acc : 96.875\n",
            "Test loss : 0.007325940299779177\n",
            "Test acc : 106.25\n",
            "Test loss : 0.007492990233004093\n",
            "Test acc : 118.75\n",
            "Test loss : 0.007134376093745232\n",
            "Test acc : 134.375\n",
            "Test loss : 0.007414682302623987\n",
            "Test acc : 153.125\n",
            "Test loss : 0.007573993876576424\n",
            "Test acc : 165.625\n",
            "Test loss : 0.007530008442699909\n",
            "Test acc : 171.875\n",
            "Test loss : 0.007586809806525707\n",
            "Test acc : 184.375\n",
            "Test loss : 0.007347517646849155\n",
            "Test acc : 193.75\n",
            "Test loss : 0.007401051931083202\n",
            "Test acc : 200.0\n",
            "Test loss : 0.007405619136989117\n",
            "Test acc : 206.25\n",
            "Test loss : 0.007250261027365923\n",
            "Test acc : 228.125\n",
            "Test loss : 0.007679341360926628\n",
            "Test acc : 243.75\n",
            "Test loss : 0.007349241059273481\n",
            "Test acc : 259.375\n",
            "Test loss : 0.007549960631877184\n",
            "Test acc : 265.625\n",
            "Test loss : 0.007378558162599802\n",
            "Test acc : 281.25\n",
            "Test loss : 0.007303732912987471\n",
            "Test acc : 306.25\n",
            "Test loss : 0.007433417718857527\n",
            "Test acc : 309.375\n",
            "Test loss : 0.007393495179712772\n",
            "Test acc : 318.75\n",
            "Test loss : 0.007453824859112501\n",
            "Test acc : 325.0\n",
            "Test loss : 0.007344197016209364\n",
            "Test acc : 343.75\n",
            "Test loss : 0.007451508194208145\n",
            "Test acc : 356.25\n",
            "Test loss : 0.007584382314234972\n",
            "Test acc : 362.5\n",
            "Test loss : 0.007258040364831686\n",
            "Test acc : 381.25\n",
            "Test loss : 0.00740512739866972\n",
            "Test acc : 393.75\n",
            "Test loss : 0.0073617915622889996\n",
            "Test acc : 403.125\n",
            "Test loss : 0.007387731224298477\n",
            "Test acc : 421.875\n",
            "Test loss : 0.00722085777670145\n",
            "Test acc : 431.25\n",
            "Test loss : 0.007301735691726208\n",
            "Test acc : 440.625\n",
            "Test loss : 0.0072183976881206036\n",
            "Test acc : 459.375\n",
            "Test loss : 0.007570626214146614\n",
            "Test acc : 471.875\n",
            "Test loss : 0.007465643808245659\n",
            "Test acc : 484.375\n",
            "Test loss : 0.00768063310533762\n",
            "Test acc : 500.0\n",
            "Test loss : 0.00729401595890522\n",
            "Test acc : 518.75\n",
            "Test loss : 0.007187616545706987\n",
            "Test acc : 540.625\n",
            "Test loss : 0.00732866395264864\n",
            "Test acc : 546.875\n",
            "Test loss : 0.007506913505494595\n",
            "Test acc : 556.25\n",
            "Test loss : 0.007279102690517902\n",
            "Test acc : 562.5\n",
            "Test loss : 0.0075258417055010796\n",
            "Test acc : 565.625\n",
            "Test loss : 0.0076474398374557495\n",
            "Test acc : 575.0\n",
            "Test loss : 0.0073445201851427555\n",
            "Test acc : 587.5\n",
            "Test loss : 0.007570585235953331\n",
            "Test acc : 593.75\n",
            "Test loss : 0.007311319932341576\n",
            "Test acc : 603.125\n",
            "Test loss : 0.0073540667071938515\n",
            "Test acc : 625.0\n",
            "Test loss : 0.007399949710816145\n",
            "Test acc : 634.375\n",
            "Test loss : 0.0072926986031234264\n",
            "Test acc : 643.75\n",
            "Test loss : 0.007382893934845924\n",
            "Test acc : 656.25\n",
            "Test loss : 0.007408333942294121\n",
            "Test acc : 668.75\n",
            "Test loss : 0.007603383157402277\n",
            "Test acc : 678.125\n",
            "Test loss : 0.007430148310959339\n",
            "Test acc : 687.5\n",
            "Test loss : 0.007443159818649292\n",
            "Test acc : 700.0\n",
            "Test loss : 0.0073358602821826935\n",
            "Test acc : 709.375\n",
            "Test loss : 0.007745567709207535\n",
            "Test acc : 718.75\n",
            "Test loss : 0.007518823258578777\n",
            "Test acc : 721.875\n",
            "Test loss : 0.0074107227846980095\n",
            "Test acc : 731.25\n",
            "Test loss : 0.007517639547586441\n",
            "Test acc : 740.625\n",
            "Test loss : 0.007293124683201313\n",
            "Test acc : 753.125\n",
            "Test loss : 0.007202120032161474\n",
            "Test acc : 775.0\n",
            "Test loss : 0.007556438446044922\n",
            "Test acc : 781.25\n",
            "Test loss : 0.007497278042137623\n",
            "Test acc : 787.5\n",
            "Test loss : 0.007474628277122974\n",
            "Test acc : 793.75\n",
            "Test loss : 0.007482025306671858\n",
            "Test acc : 806.25\n",
            "Test loss : 0.007403487805277109\n",
            "Test acc : 821.875\n",
            "Test loss : 0.007552063092589378\n",
            "Test acc : 837.5\n",
            "Test loss : 0.0076764971017837524\n",
            "Test acc : 843.75\n",
            "Test loss : 0.007469272240996361\n",
            "Test acc : 859.375\n",
            "Test loss : 0.007338934578001499\n",
            "Test acc : 881.25\n",
            "Test loss : 0.007575809024274349\n",
            "Test acc : 893.75\n",
            "Test loss : 0.007468816824257374\n",
            "Test acc : 903.125\n",
            "Test loss : 0.007593436632305384\n",
            "Test acc : 909.375\n",
            "Test loss : 0.0072798659093678\n",
            "Test acc : 925.0\n",
            "Test loss : 0.007422140333801508\n",
            "Test acc : 934.375\n",
            "Test loss : 0.007111228071153164\n",
            "Test acc : 956.25\n",
            "Test loss : 0.0074285236187279224\n",
            "Test acc : 968.75\n",
            "Test loss : 0.0073059131391346455\n",
            "Test acc : 987.5\n",
            "Test loss : 0.007396260742098093\n",
            "Test acc : 993.75\n",
            "Test loss : 0.007443184498697519\n",
            "Test acc : 1003.125\n",
            "Test loss : 0.007328942883759737\n",
            "Test acc : 1025.0\n",
            "Test loss : 0.007678337395191193\n",
            "Test acc : 1031.25\n",
            "Test loss : 0.00760155962780118\n",
            "Test acc : 1034.375\n",
            "Test loss : 0.007360846735537052\n",
            "Test acc : 1046.875\n",
            "Test loss : 0.007294501177966595\n",
            "Test acc : 1056.25\n",
            "Test loss : 0.00727534294128418\n",
            "Test acc : 1065.625\n",
            "Test loss : 0.007431583479046822\n",
            "Test acc : 1075.0\n",
            "Test loss : 0.007295174524188042\n",
            "Test acc : 1081.25\n",
            "Test loss : 0.007659889291971922\n",
            "Test acc : 1084.375\n",
            "Test loss : 0.007607427425682545\n",
            "Test acc : 1087.5\n",
            "Test loss : 0.007395966909825802\n",
            "Test acc : 1103.125\n",
            "Test loss : 0.007439987268298864\n",
            "Test acc : 1115.625\n",
            "Test loss : 0.007478254847228527\n",
            "Test acc : 1115.625\n",
            "Test loss : 0.0073531558737158775\n",
            "Test acc : 1125.0\n",
            "Test loss : 0.007409143727272749\n",
            "Test acc : 1131.25\n",
            "Test loss : 0.007379772607237101\n",
            "Test acc : 1143.75\n",
            "Test loss : 0.007288216147571802\n",
            "Test acc : 1156.25\n",
            "Test loss : 0.00767391175031662\n",
            "Test acc : 1168.75\n",
            "Test loss : 0.0076570212841033936\n",
            "Test acc : 1171.875\n",
            "Test loss : 0.007460782304406166\n",
            "Test acc : 1181.25\n",
            "Test loss : 0.007468746043741703\n",
            "Test acc : 1187.5\n",
            "Test loss : 0.007226433604955673\n",
            "Test acc : 1203.125\n",
            "Test loss : 0.007471099961549044\n",
            "Test acc : 1218.75\n",
            "Test loss : 0.0074538118205964565\n",
            "Test acc : 1225.0\n",
            "Test loss : 0.007421909365803003\n",
            "Test acc : 1231.25\n",
            "Test loss : 0.007408453617244959\n",
            "Test acc : 1231.25\n",
            "Test loss : 0.007501377258449793\n",
            "Test acc : 1240.625\n",
            "Test loss : 0.007332938257604837\n",
            "Test acc : 1259.375\n",
            "Test loss : 0.007483900524675846\n",
            "Test acc : 1265.625\n",
            "Test loss : 0.007448122370988131\n",
            "Test acc : 1275.0\n",
            "Test loss : 0.007369488477706909\n",
            "Test acc : 1284.375\n",
            "Test loss : 0.007587250787764788\n",
            "Test acc : 1290.625\n",
            "Test loss : 0.007568845991045237\n",
            "Test acc : 1296.875\n",
            "Test loss : 0.007534482050687075\n",
            "Test acc : 1306.25\n",
            "Test loss : 0.007430430967360735\n",
            "Test acc : 1312.5\n",
            "Test loss : 0.007313127163797617\n",
            "Test acc : 1315.625\n",
            "Test loss : 0.007682661060243845\n",
            "Test acc : 1315.625\n",
            "Test loss : 0.007431915961205959\n",
            "Test acc : 1328.125\n",
            "Test loss : 0.007360937539488077\n",
            "Test acc : 1337.5\n",
            "Test loss : 0.007504965178668499\n",
            "Test acc : 1353.125\n",
            "Test loss : 0.0073564243502914906\n",
            "Test acc : 1362.5\n",
            "Test loss : 0.007411884609609842\n",
            "Test acc : 1375.0\n",
            "Test loss : 0.007409369107335806\n",
            "Test acc : 1384.375\n",
            "Test loss : 0.007433450315147638\n",
            "Test acc : 1393.75\n",
            "Test loss : 0.007371791172772646\n",
            "Test acc : 1406.25\n",
            "Test loss : 0.007577334064990282\n",
            "Test acc : 1412.5\n",
            "Test loss : 0.007381855044513941\n",
            "Test acc : 1418.75\n",
            "Test loss : 0.007323172874748707\n",
            "Test acc : 1440.625\n",
            "Test loss : 0.007677766028791666\n",
            "Test acc : 1453.125\n",
            "Test loss : 0.007379299495369196\n",
            "Test acc : 1462.5\n",
            "Test loss : 0.007427283562719822\n",
            "Test acc : 1471.875\n",
            "Test loss : 0.007467766758054495\n",
            "Test acc : 1484.375\n",
            "Test loss : 0.00746166380122304\n",
            "Test acc : 1509.375\n",
            "Test loss : 0.007604554761201143\n",
            "Test acc : 1515.625\n",
            "Test loss : 0.007212955504655838\n",
            "Test acc : 1540.625\n",
            "Test loss : 0.007377536967396736\n",
            "Test acc : 1553.125\n",
            "Test loss : 0.007429931778460741\n",
            "Test acc : 1571.875\n",
            "Test loss : 0.007458633277565241\n",
            "Test acc : 1581.25\n",
            "Test loss : 0.007570772431790829\n",
            "Test acc : 1587.5\n",
            "Test loss : 0.007487169001251459\n",
            "Test acc : 1606.25\n",
            "Test loss : 0.007439847104251385\n",
            "Test acc : 1609.375\n",
            "Test loss : 0.007409464567899704\n",
            "Test acc : 1621.875\n",
            "Test loss : 0.0075912210159003735\n",
            "Test acc : 1637.5\n",
            "Test loss : 0.007187660317867994\n",
            "Test acc : 1656.25\n",
            "Test loss : 0.007584169041365385\n",
            "Test acc : 1659.375\n",
            "Test loss : 0.007361377589404583\n",
            "Test acc : 1671.875\n",
            "Test loss : 0.007442090660333633\n",
            "Test acc : 1678.125\n",
            "Test loss : 0.0074773491360247135\n",
            "Test acc : 1693.75\n",
            "Test loss : 0.007623401470482349\n",
            "Test acc : 1706.25\n",
            "Test loss : 0.007657334674149752\n",
            "Test acc : 1706.25\n",
            "Test loss : 0.007378706708550453\n",
            "Test acc : 1721.875\n",
            "Test loss : 0.007308389991521835\n",
            "Test acc : 1740.625\n",
            "Test loss : 0.007441795896738768\n",
            "Test acc : 1750.0\n",
            "Test loss : 0.007548256777226925\n",
            "Test acc : 1756.25\n",
            "Test loss : 0.00736412825062871\n",
            "Test acc : 1771.875\n",
            "Test loss : 0.007345712278038263\n",
            "Test acc : 1790.625\n",
            "Test loss : 0.007378934882581234\n",
            "Test acc : 1800.0\n",
            "Test loss : 0.00737244077026844\n",
            "Test acc : 1812.5\n",
            "Test loss : 0.007300748489797115\n",
            "Test acc : 1831.25\n",
            "Test loss : 0.007453511469066143\n",
            "Test acc : 1837.5\n",
            "Test loss : 0.0075087640434503555\n",
            "Test acc : 1846.875\n",
            "Test loss : 0.00740423146635294\n",
            "Test acc : 1862.5\n",
            "Test loss : 0.007605156395584345\n",
            "Test acc : 1878.125\n",
            "Test loss : 0.007486076559871435\n",
            "Test acc : 1884.375\n",
            "Test loss : 0.007305190432816744\n",
            "Test acc : 1896.875\n",
            "Test loss : 0.0070731136947870255\n",
            "Test acc : 1921.875\n",
            "Test loss : 0.007334108930081129\n",
            "Test acc : 1931.25\n",
            "Test loss : 0.007596463896334171\n",
            "Test acc : 1937.5\n",
            "Test loss : 0.007290873676538467\n",
            "Test acc : 1946.875\n",
            "Test loss : 0.007405395153909922\n",
            "Test acc : 1953.125\n",
            "Test loss : 0.007555884774774313\n",
            "Test acc : 1956.25\n",
            "Test loss : 0.00759274372830987\n",
            "Test acc : 1962.5\n",
            "Test loss : 0.007404418662190437\n",
            "Test acc : 1971.875\n",
            "Test loss : 0.007398455869406462\n",
            "Test acc : 1981.25\n",
            "Test loss : 0.007564317900687456\n",
            "Test acc : 1993.75\n",
            "Test loss : 0.007192626129835844\n",
            "Test acc : 2003.125\n",
            "Test loss : 0.007259955629706383\n",
            "Test acc : 2018.75\n",
            "Test loss : 0.007356029935181141\n",
            "Test acc : 2025.0\n",
            "Test loss : 0.007463433779776096\n",
            "Test acc : 2037.5\n",
            "Test loss : 0.007696542423218489\n",
            "Test acc : 2040.625\n",
            "Test loss : 0.007472024764865637\n",
            "Test acc : 2050.0\n",
            "Test loss : 0.007293884176760912\n",
            "Test acc : 2062.5\n",
            "Test loss : 0.0073058162815868855\n",
            "Test acc : 2068.75\n",
            "Test loss : 0.007716672495007515\n",
            "Test acc : 2068.75\n",
            "Test loss : 0.007481725886464119\n",
            "Test acc : 2071.875\n",
            "Test loss : 0.007268757093697786\n",
            "Test acc : 2081.25\n",
            "Test loss : 0.007322973106056452\n",
            "Test acc : 2100.0\n",
            "Test loss : 0.00767617579549551\n",
            "Test acc : 2103.125\n",
            "Test loss : 0.007425539195537567\n",
            "Test acc : 2121.875\n",
            "Test loss : 0.007242697756737471\n",
            "Test acc : 2143.75\n",
            "Test loss : 0.0073485723696649075\n",
            "Test acc : 2153.125\n",
            "Test loss : 0.00760929798707366\n",
            "Test acc : 2162.5\n",
            "Test loss : 0.007460783235728741\n",
            "Test acc : 2165.625\n",
            "Test loss : 0.007396992761641741\n",
            "Test acc : 2175.0\n",
            "Test loss : 0.007303369697183371\n",
            "Test acc : 2193.75\n",
            "Test loss : 0.007398608606308699\n",
            "Test acc : 2209.375\n",
            "Test loss : 0.007425854913890362\n",
            "Test acc : 2221.875\n",
            "Test loss : 0.0072680250741541386\n",
            "Test acc : 2228.125\n",
            "Test loss : 0.007413102313876152\n",
            "Test acc : 2243.75\n",
            "Test loss : 0.007387197110801935\n",
            "Test acc : 2250.0\n",
            "Test loss : 0.007423937786370516\n",
            "Test acc : 2265.625\n",
            "Test loss : 0.0077552879229187965\n",
            "Test acc : 2265.625\n",
            "Test loss : 0.007347553968429565\n",
            "Test acc : 2275.0\n",
            "Test loss : 0.007570980582386255\n",
            "Test acc : 2281.25\n",
            "Test loss : 0.007584987208247185\n",
            "Test acc : 2293.75\n",
            "Test loss : 0.007475751452147961\n",
            "Test acc : 2293.75\n",
            "Test loss : 0.007414642721414566\n",
            "Test acc : 2309.375\n",
            "Test loss : 0.007492566481232643\n",
            "Test acc : 2321.875\n",
            "Test loss : 0.007329570595175028\n",
            "Test acc : 2331.25\n",
            "Test loss : 0.007706950418651104\n",
            "Test acc : 2334.375\n",
            "Test loss : 0.007547914981842041\n",
            "Test acc : 2337.5\n",
            "Test loss : 0.007266555912792683\n",
            "Test acc : 2353.125\n",
            "Test loss : 0.007273718249052763\n",
            "Test acc : 2368.75\n",
            "Test loss : 0.007376234978437424\n",
            "Test acc : 2371.875\n",
            "Test loss : 0.007455111481249332\n",
            "Test acc : 2390.625\n",
            "Test loss : 0.007505044341087341\n",
            "Test acc : 2403.125\n",
            "Test loss : 0.0073562972247600555\n",
            "Test acc : 2409.375\n",
            "Test loss : 0.00753746647387743\n",
            "Test acc : 2415.625\n",
            "Test loss : 0.007373741362243891\n",
            "Test acc : 2421.875\n",
            "Test loss : 0.007330749183893204\n",
            "Test acc : 2428.125\n",
            "Test loss : 0.007403216790407896\n",
            "Test acc : 2434.375\n",
            "Test loss : 0.00769714405760169\n",
            "Test acc : 2440.625\n",
            "Test loss : 0.007133524399250746\n",
            "Test acc : 2468.75\n",
            "Test loss : 0.0076151625253260136\n",
            "Test acc : 2471.875\n",
            "Test loss : 0.007498652674257755\n",
            "Test acc : 2493.75\n",
            "Test loss : 0.007614111993461847\n",
            "Test acc : 2503.125\n",
            "Test loss : 0.007328998763114214\n",
            "Test acc : 2525.0\n",
            "Test loss : 0.007596056442707777\n",
            "Test acc : 2534.375\n",
            "Test loss : 0.007379553746432066\n",
            "Test acc : 2546.875\n",
            "Test loss : 0.007552156690508127\n",
            "Test acc : 2562.5\n",
            "Test loss : 0.007605183403939009\n",
            "Test acc : 2568.75\n",
            "Test loss : 0.007395651191473007\n",
            "Test acc : 2584.375\n",
            "Test loss : 0.007653020787984133\n",
            "Test acc : 2587.5\n",
            "Test loss : 0.007485178764909506\n",
            "Test acc : 2600.0\n",
            "Test loss : 0.007254099939018488\n",
            "Test acc : 2612.5\n",
            "Test loss : 0.0074019357562065125\n",
            "Test acc : 2628.125\n",
            "Test loss : 0.007353448309004307\n",
            "Test acc : 2631.25\n",
            "Test loss : 0.0073029217310249805\n",
            "Test acc : 2637.5\n",
            "Test loss : 0.007629196625202894\n",
            "Test acc : 2643.75\n",
            "Test loss : 0.007404195610433817\n",
            "Test acc : 2653.125\n",
            "Test loss : 0.007417604792863131\n",
            "Test acc : 2656.25\n",
            "Test loss : 0.007333540823310614\n",
            "Test acc : 2671.875\n",
            "Test loss : 0.007200109306722879\n",
            "Test acc : 2690.625\n",
            "Test loss : 0.007445324677973986\n",
            "Test acc : 2703.125\n",
            "Test loss : 0.00740595068782568\n",
            "Test acc : 2703.125\n",
            "Test loss : 0.00750848138704896\n",
            "Test acc : 2715.625\n",
            "Test loss : 0.007655723951756954\n",
            "Test acc : 2731.25\n",
            "Test loss : 0.007355198729783297\n",
            "Test acc : 2750.0\n",
            "Test loss : 0.007401267532259226\n",
            "Test acc : 2756.25\n",
            "Test loss : 0.007376599125564098\n",
            "Test acc : 2768.75\n",
            "Test loss : 0.007371092680841684\n",
            "Test acc : 2771.875\n",
            "Test loss : 0.007235590368509293\n",
            "Test acc : 2787.5\n",
            "Test loss : 0.007346493192017078\n",
            "Test acc : 2793.75\n",
            "Test loss : 0.007303704973310232\n",
            "Test acc : 2812.5\n",
            "Test loss : 0.007533635012805462\n",
            "Test acc : 2821.875\n",
            "Test loss : 0.007346208207309246\n",
            "Test acc : 2828.125\n",
            "Test loss : 0.00750221312046051\n",
            "Test acc : 2843.75\n",
            "Test loss : 0.007482846267521381\n",
            "Test acc : 2853.125\n",
            "Test loss : 0.00754398200660944\n",
            "Test acc : 2865.625\n",
            "Test loss : 0.007312828209251165\n",
            "Test acc : 2881.25\n",
            "Test loss : 0.007089817430824041\n",
            "Test acc : 2896.875\n",
            "Test loss : 0.0075436849147081375\n",
            "Test acc : 2903.125\n",
            "Test loss : 0.007553300354629755\n",
            "Test acc : 2915.625\n",
            "Test loss : 0.007335198111832142\n",
            "Test acc : 2925.0\n",
            "Test loss : 0.0073556494899094105\n",
            "Test acc : 2931.25\n",
            "Test loss : 0.007415637373924255\n",
            "Test acc : 2946.875\n",
            "Test loss : 0.007426118478178978\n",
            "Test acc : 2956.25\n",
            "Test loss : 0.007392565719783306\n",
            "Test acc : 2962.5\n",
            "Test loss : 0.007405739743262529\n",
            "Test acc : 2965.625\n",
            "Test loss : 0.007268758025020361\n",
            "Test acc : 2984.375\n",
            "Test loss : 0.007440072484314442\n",
            "Test acc : 2993.75\n",
            "Test loss : 0.00738753005862236\n",
            "Test acc : 3006.25\n",
            "Test loss : 0.007516409270465374\n",
            "Test acc : 3018.75\n",
            "Test loss : 0.007345364894717932\n",
            "Test acc : 3037.5\n",
            "Test loss : 0.007532777264714241\n",
            "Test acc : 3046.875\n",
            "Test loss : 0.0075017306953668594\n",
            "Test acc : 3056.25\n",
            "Test loss : 0.007463560439646244\n",
            "Test acc : 3065.625\n",
            "Test loss : 0.007408566307276487\n",
            "Test acc : 3081.25\n",
            "Test loss : 0.007445263676345348\n",
            "Test acc : 3087.5\n",
            "Test loss : 0.007303743623197079\n",
            "Test acc : 3103.125\n",
            "Test loss : 0.007554533425718546\n",
            "Test acc : 3112.5\n",
            "Test loss : 0.007467860821634531\n",
            "Test acc : 3128.125\n",
            "Test loss : 0.007638915441930294\n",
            "Test acc : 3140.625\n",
            "Test loss : 0.0074743772856891155\n",
            "Test acc : 3146.875\n",
            "Test loss : 0.007421574089676142\n",
            "Test acc : 3165.625\n",
            "Test loss : 0.00755599420517683\n",
            "Test acc : 3175.0\n",
            "Test loss : 0.007253597490489483\n",
            "Test acc : 3200.0\n",
            "Test loss : 0.007556100841611624\n",
            "Test acc : 3206.25\n",
            "Test loss : 0.0074975742027163506\n",
            "Test acc : 3212.5\n",
            "Test loss : 0.007597215007990599\n",
            "Test acc : 3212.5\n",
            "Test loss : 0.00733166141435504\n",
            "Test acc : 3228.125\n",
            "Test loss : 0.007459175772964954\n",
            "Test acc : 3234.375\n",
            "Test loss : 0.007412469480186701\n",
            "Test acc : 3243.75\n",
            "Test loss : 0.007471026852726936\n",
            "Test acc : 3253.125\n",
            "Test loss : 0.007338140159845352\n",
            "Test acc : 3262.5\n",
            "Test loss : 0.007272417191416025\n",
            "Test acc : 3271.875\n",
            "Test loss : 0.007556391414254904\n",
            "Test acc : 3281.25\n",
            "Test loss : 0.0072556608356535435\n",
            "Test acc : 3300.0\n",
            "Test loss : 0.007453979458659887\n",
            "Test acc : 3318.75\n",
            "Test loss : 0.007320080883800983\n",
            "Test acc : 3331.25\n",
            "Test loss : 0.007676729932427406\n",
            "Test acc : 3337.5\n",
            "Test loss : 0.007338725496083498\n",
            "Test acc : 3343.75\n",
            "Test loss : 0.007488212548196316\n",
            "Test acc : 3353.125\n",
            "Test loss : 0.0073072942905128\n",
            "Test acc : 3371.875\n",
            "Test loss : 0.007679843343794346\n",
            "Test acc : 3378.125\n",
            "Test loss : 0.00743069825693965\n",
            "Test acc : 3390.625\n",
            "Test loss : 0.007558837067335844\n",
            "Test acc : 3396.875\n",
            "Looked at 0/60000 samples.\n",
            "Train loss : 0.4053603410720825\n",
            "Train loss : 0.35842645168304443\n",
            "Train loss : 0.2929316461086273\n",
            "Train loss : 0.14177736639976501\n",
            "Train loss : 0.18109211325645447\n",
            "Train loss : 0.07463281601667404\n",
            "Train loss : 0.0757254809141159\n",
            "Train loss : 0.04416301101446152\n",
            "Train loss : 0.06352163851261139\n",
            "Train loss : 0.032575882971286774\n",
            "Train loss : 0.033700890839099884\n",
            "Train loss : 0.03448216989636421\n",
            "Train loss : 0.0721767321228981\n",
            "Train loss : 0.04068129509687424\n",
            "Train loss : 0.016480453312397003\n",
            "Train loss : 0.03193247318267822\n",
            "Train loss : 0.04641416296362877\n",
            "Train loss : 0.014132875949144363\n",
            "Train loss : 0.04438683018088341\n",
            "Train loss : 0.01348812598735094\n",
            "Train loss : 0.017749516293406487\n",
            "Train loss : 0.014972994104027748\n",
            "Train loss : 0.017284255474805832\n",
            "Train loss : 0.022537842392921448\n",
            "Train loss : 0.0281031746417284\n",
            "Train loss : 0.0241457000374794\n",
            "Train loss : 0.02443946897983551\n",
            "Train loss : 0.02746536396443844\n",
            "Train loss : 0.021498197689652443\n",
            "Train loss : 0.008514904417097569\n",
            "Train loss : 0.01675291918218136\n",
            "Train loss : 0.011775524355471134\n",
            "Train loss : 0.008095519617199898\n",
            "Train loss : 0.025020280852913857\n",
            "Train loss : 0.014191350899636745\n",
            "Train loss : 0.0074318647384643555\n",
            "Train loss : 0.011862891726195812\n",
            "Train loss : 0.01699936017394066\n",
            "Train loss : 0.0063387611880898476\n",
            "Train loss : 0.005824368912726641\n",
            "Train loss : 0.016988931223750114\n",
            "Train loss : 0.013215390034019947\n",
            "Train loss : 0.009109037928283215\n",
            "Train loss : 0.009686657227575779\n",
            "Train loss : 0.008829549886286259\n",
            "Train loss : 0.011432587169110775\n",
            "Train loss : 0.007584707345813513\n",
            "Train loss : 0.008480159565806389\n",
            "Train loss : 0.009725039824843407\n",
            "Train loss : 0.004806335084140301\n",
            "Train loss : 0.007045156322419643\n",
            "Train loss : 0.004525312688201666\n",
            "Train loss : 0.006141194142401218\n",
            "Train loss : 0.00901346281170845\n",
            "Train loss : 0.010197270661592484\n",
            "Train loss : 0.010901831090450287\n",
            "Train loss : 0.004706431180238724\n",
            "Train loss : 0.005644067190587521\n",
            "Train loss : 0.010426216758787632\n",
            "Train loss : 0.005606420803815126\n",
            "Train loss : 0.00996785145252943\n",
            "Train loss : 0.0065172952599823475\n",
            "Train loss : 0.005985478404909372\n",
            "Train loss : 0.006343239918351173\n",
            "Train loss : 0.006623561028391123\n",
            "Train loss : 0.006888659205287695\n",
            "Train loss : 0.006016746163368225\n",
            "Train loss : 0.0064604622311890125\n",
            "Train loss : 0.0077339159324765205\n",
            "Train loss : 0.006631010212004185\n",
            "Train loss : 0.0034813638776540756\n",
            "Train loss : 0.003680186113342643\n",
            "Train loss : 0.008655808866024017\n",
            "Train loss : 0.008233331143856049\n",
            "Train loss : 0.004219368100166321\n",
            "Train loss : 0.006918150465935469\n",
            "Train loss : 0.0016556382179260254\n",
            "Train loss : 0.004774695262312889\n",
            "Train loss : 0.004634711891412735\n",
            "Train loss : 0.0024848708417266607\n",
            "Train loss : 0.00539895985275507\n",
            "Train loss : 0.0036448119208216667\n",
            "Train loss : 0.005060547962784767\n",
            "Train loss : 0.005042542237788439\n",
            "Train loss : 0.0033464613370597363\n",
            "Train loss : 0.0038697102572768927\n",
            "Train loss : 0.007031670771539211\n",
            "Train loss : 0.003795160911977291\n",
            "Train loss : 0.0027979237493127584\n",
            "Train loss : 0.003927009645849466\n",
            "Train loss : 0.005226879846304655\n",
            "Train loss : 0.006432775408029556\n",
            "Train loss : 0.003943063784390688\n",
            "Train loss : 0.0034634245093911886\n",
            "Train loss : 0.0032612367067486048\n",
            "Train loss : 0.005382103845477104\n",
            "Train loss : 0.003470974275842309\n",
            "Train loss : 0.004630464129149914\n",
            "Train loss : 0.00540562579408288\n",
            "Train loss : 0.002309144241735339\n",
            "Train loss : 0.004804749973118305\n",
            "Train loss : 0.004162781406193972\n",
            "Train loss : 0.004887641407549381\n",
            "Train loss : 0.0034968098625540733\n",
            "Train loss : 0.004695055074989796\n",
            "Train loss : 0.00693090632557869\n",
            "Train loss : 0.003143962239846587\n",
            "Train loss : 0.0038578377570956945\n",
            "Train loss : 0.005539680831134319\n",
            "Train loss : 0.0032911861781030893\n",
            "Train loss : 0.0030145884957164526\n",
            "Train loss : 0.004339125007390976\n",
            "Train loss : 0.0046127489767968655\n",
            "Train loss : 0.002677361713722348\n",
            "Train loss : 0.004544789437204599\n",
            "Train loss : 0.004922205116599798\n",
            "Train loss : 0.005031083710491657\n",
            "Train loss : 0.005291265901178122\n",
            "Train loss : 0.004599375184625387\n",
            "Train loss : 0.0024973105173557997\n",
            "Train loss : 0.0037984647788107395\n",
            "Train loss : 0.004422904923558235\n",
            "Train loss : 0.0013227196177467704\n",
            "Train loss : 0.004742123652249575\n",
            "Train loss : 0.002992760855704546\n",
            "Train loss : 0.006440560799092054\n",
            "Train loss : 0.005220422055572271\n",
            "Train loss : 0.0016332977684214711\n",
            "Train loss : 0.002255775732919574\n",
            "Train loss : 0.001493840478360653\n",
            "Train loss : 0.002744344063103199\n",
            "Train loss : 0.004708848427981138\n",
            "Train loss : 0.0014847521670162678\n",
            "Train loss : 0.002770262537524104\n",
            "Train loss : 0.003120277775451541\n",
            "Train loss : 0.0045473575592041016\n",
            "Train loss : 0.0047913179732859135\n",
            "Train loss : 0.004308420233428478\n",
            "Train loss : 0.0025947189424186945\n",
            "Train loss : 0.00184912933036685\n",
            "Train loss : 0.004529960919171572\n",
            "Train loss : 0.003178571118041873\n",
            "Train loss : 0.003654025960713625\n",
            "Train loss : 0.0017457175999879837\n",
            "Train loss : 0.0020222263410687447\n",
            "Train loss : 0.002107699401676655\n",
            "Train loss : 0.0029250613879412413\n",
            "Train loss : 0.004054746124893427\n",
            "Train loss : 0.0026565452571958303\n",
            "Train loss : 0.0032444829121232033\n",
            "Train loss : 0.0020919928792864084\n",
            "Train loss : 0.003925876226276159\n",
            "Train loss : 0.0014781634090468287\n",
            "Train loss : 0.0031364194583147764\n",
            "Train loss : 0.0019220039248466492\n",
            "Train loss : 0.0031455601565539837\n",
            "Train loss : 0.0028231267351657152\n",
            "Train loss : 0.003643770469352603\n",
            "Train loss : 0.0021695508621633053\n",
            "Train loss : 0.002608313923701644\n",
            "Train loss : 0.002132491674274206\n",
            "Train loss : 0.002205935074016452\n",
            "Train loss : 0.002708686050027609\n",
            "Train loss : 0.004748706705868244\n",
            "Train loss : 0.0031859292648732662\n",
            "Train loss : 0.001905402634292841\n",
            "Train loss : 0.0016846280777826905\n",
            "Train loss : 0.0035434418823570013\n",
            "Train loss : 0.00235586310736835\n",
            "Train loss : 0.002647477202117443\n",
            "Train loss : 0.003436190774664283\n",
            "Train loss : 0.0019735300447791815\n",
            "Train loss : 0.0023926866706460714\n",
            "Train loss : 0.0030564144253730774\n",
            "Train loss : 0.003815536852926016\n",
            "Train loss : 0.001464979024603963\n",
            "Train loss : 0.005908973515033722\n",
            "Train loss : 0.0015698440838605165\n",
            "Train loss : 0.0024513474199920893\n",
            "Train loss : 0.001905233832076192\n",
            "Train loss : 0.0017404831014573574\n",
            "Train loss : 0.0011959215626120567\n",
            "Train loss : 0.002669691573828459\n",
            "Train loss : 0.0019206677097827196\n",
            "Train loss : 0.0022633839398622513\n",
            "Train loss : 0.0016997678903862834\n",
            "Train loss : 0.002454940928146243\n",
            "Train loss : 0.0022427334915846586\n",
            "Train loss : 0.0015740112867206335\n",
            "Train loss : 0.002024877816438675\n",
            "Train loss : 0.0018672089790925384\n",
            "Train loss : 0.0025481940247118473\n",
            "Train loss : 0.0022061094641685486\n",
            "Train loss : 0.0018735715420916677\n",
            "Train loss : 0.0009466808405704796\n",
            "Train loss : 0.0014535398222506046\n",
            "Train loss : 0.001870659296400845\n",
            "Train loss : 0.0035853798035532236\n",
            "Train loss : 0.001634078100323677\n",
            "Train loss : 0.0011387630365788937\n",
            "Train loss : 0.0021694430615752935\n",
            "Train loss : 0.0032995666842907667\n",
            "Train loss : 0.0019463257631286979\n",
            "Train loss : 0.0022324908059090376\n",
            "Train loss : 0.001417421968653798\n",
            "Train loss : 0.003649989841505885\n",
            "Train loss : 0.0017535864608362317\n",
            "Train loss : 0.003952891565859318\n",
            "Train loss : 0.0023051416501402855\n",
            "Train loss : 0.0022023213095963\n",
            "Train loss : 0.002161898650228977\n",
            "Train loss : 0.0014397382037714124\n",
            "Train loss : 0.0021998691372573376\n",
            "Train loss : 0.0020520782563835382\n",
            "Train loss : 0.001808570115827024\n",
            "Train loss : 0.0018207577522844076\n",
            "Train loss : 0.0034289988689124584\n",
            "Train loss : 0.0026761009357869625\n",
            "Train loss : 0.0017597449477761984\n",
            "Train loss : 0.0011206063209101558\n",
            "Train loss : 0.002213133731856942\n",
            "Train loss : 0.0018479484133422375\n",
            "Train loss : 0.0011484634596854448\n",
            "Train loss : 0.0023212044034153223\n",
            "Train loss : 0.0021615736186504364\n",
            "Train loss : 0.002070872113108635\n",
            "Train loss : 0.0034810665529221296\n",
            "Train loss : 0.0025802019517868757\n",
            "Train loss : 0.001108041382394731\n",
            "Train loss : 0.0019202795810997486\n",
            "Train loss : 0.0024951810482889414\n",
            "Train loss : 0.0019217722583562136\n",
            "Train loss : 0.0024156924337148666\n",
            "Train loss : 0.0006965872598811984\n",
            "Train loss : 0.001512667047791183\n",
            "Train loss : 0.003283006139099598\n",
            "Train loss : 0.0015987188089638948\n",
            "Train loss : 0.0018791586626321077\n",
            "Train loss : 0.0016676342347636819\n",
            "Train loss : 0.0014625494368374348\n",
            "Train loss : 0.0012639076448976994\n",
            "Train loss : 0.0020643966272473335\n",
            "Train loss : 0.0013760346919298172\n",
            "Train loss : 0.00043193824240006506\n",
            "Train loss : 0.0023587991017848253\n",
            "Train loss : 0.0019093003356829286\n",
            "Train loss : 0.0014798096381127834\n",
            "Train loss : 0.001153001794591546\n",
            "Train loss : 0.0016110553406178951\n",
            "Train loss : 0.0015776384389027953\n",
            "Train loss : 0.0014576488174498081\n",
            "Train loss : 0.0014244472840800881\n",
            "Train loss : 0.0011753193102777004\n",
            "Train loss : 0.0019942433573305607\n",
            "Train loss : 0.002947494387626648\n",
            "Train loss : 0.002725167665630579\n",
            "Train loss : 0.002658751094713807\n",
            "Train loss : 0.003146225120872259\n",
            "Train loss : 0.0026161051355302334\n",
            "Train loss : 0.002925517503172159\n",
            "Train loss : 0.0013024910585954785\n",
            "Train loss : 0.0030198267195373774\n",
            "Train loss : 0.0016215154901146889\n",
            "Train loss : 0.0018834469374269247\n",
            "Train loss : 0.001136507955379784\n",
            "Train loss : 0.001133812707848847\n",
            "Train loss : 0.001795732183381915\n",
            "Train loss : 0.002160428324714303\n",
            "Train loss : 0.0025873950216919184\n",
            "Train loss : 0.001535554532893002\n",
            "Train loss : 0.0007719519780948758\n",
            "Train loss : 0.0008110424969345331\n",
            "Train loss : 0.0011165692703798413\n",
            "Train loss : 0.001448196591809392\n",
            "Train loss : 0.0020856696646660566\n",
            "Train loss : 0.0025524303782731295\n",
            "Train loss : 0.0020940762478858232\n",
            "Train loss : 0.0016416595317423344\n",
            "Train loss : 0.0021892772056162357\n",
            "Train loss : 0.0019404011545702815\n",
            "Train loss : 0.0017937499796971679\n",
            "Train loss : 0.002479260554537177\n",
            "Train loss : 0.0007316538831219077\n",
            "Train loss : 0.0008452748297713697\n",
            "Train loss : 0.001327443984337151\n",
            "Train loss : 0.0019424476195126772\n",
            "Train loss : 0.002089398680254817\n",
            "Train loss : 0.0009486451745033264\n",
            "Train loss : 0.0029806485399603844\n",
            "Train loss : 0.0007495995378121734\n",
            "Train loss : 0.0012643958907574415\n",
            "Train loss : 0.0016643879935145378\n",
            "Train loss : 0.0017279264284297824\n",
            "Train loss : 0.001355605898424983\n",
            "Train loss : 0.0024269213899970055\n",
            "Train loss : 0.0015338524244725704\n",
            "Train loss : 0.0015035681426525116\n",
            "Train loss : 0.0023886614944785833\n",
            "Train loss : 0.001205915934406221\n",
            "Train loss : 0.001292101456783712\n",
            "Train loss : 0.002116978168487549\n",
            "Train loss : 0.000990184722468257\n",
            "Train loss : 0.001489508431404829\n",
            "Train loss : 0.0013771295780315995\n",
            "Train loss : 0.0012603438226506114\n",
            "Train loss : 0.0010559160728007555\n",
            "Train loss : 0.0007219461840577424\n",
            "Train loss : 0.0017625864129513502\n",
            "Train loss : 0.0017119282856583595\n",
            "Train loss : 0.003006749553605914\n",
            "Train loss : 0.0010271037463098764\n",
            "Train loss : 0.0008414058829657733\n",
            "Train loss : 0.0013154675252735615\n",
            "Train loss : 0.0010472886497154832\n",
            "Train loss : 0.0009839232079684734\n",
            "Train loss : 0.0015841496642678976\n",
            "Train loss : 0.002542153000831604\n",
            "Train loss : 0.001073095016181469\n",
            "Train loss : 0.0026095183566212654\n",
            "Train loss : 0.0015259243082255125\n",
            "Train loss : 0.0007034082082100213\n",
            "Train loss : 0.0011701249750331044\n",
            "Train loss : 0.0017328395042568445\n",
            "Train loss : 0.001834940630942583\n",
            "Train loss : 0.0015026606852188706\n",
            "Train loss : 0.0010951803997159004\n",
            "Train loss : 0.0013882765779271722\n",
            "Train loss : 0.002815576968714595\n",
            "Train loss : 0.0012666417751461267\n",
            "Train loss : 0.0012040395522490144\n",
            "Train loss : 0.0014253697590902448\n",
            "Train loss : 0.0015359842218458652\n",
            "Train loss : 0.0020379689522087574\n",
            "Train loss : 0.0008383594686165452\n",
            "Train loss : 0.0009617347386665642\n",
            "Train loss : 0.0018968137446790934\n",
            "Train loss : 0.0017571465577930212\n",
            "Train loss : 0.0018279589712619781\n",
            "Train loss : 0.0012756314827129245\n",
            "Train loss : 0.0009890609653666615\n",
            "Train loss : 0.0014533251523971558\n",
            "Train loss : 0.001053843181580305\n",
            "Train loss : 0.0013446353841573\n",
            "Train loss : 0.0005922586424276233\n",
            "Train loss : 0.0006485424819402397\n",
            "Train loss : 0.0009567025117576122\n",
            "Train loss : 0.0017309424001723528\n",
            "Train loss : 0.0018401156412437558\n",
            "Train loss : 0.0018304151017218828\n",
            "Train loss : 0.0013325598556548357\n",
            "Train loss : 0.0008059304673224688\n",
            "Train loss : 0.0012253595050424337\n",
            "Train loss : 0.0014448834117501974\n",
            "Train loss : 0.0011305012740194798\n",
            "Train loss : 0.0015533455880358815\n",
            "Train loss : 0.0008760715136304498\n",
            "Train loss : 0.0010016744490712881\n",
            "Train loss : 0.0017204416217282414\n",
            "Train loss : 0.0016867166850715876\n",
            "Train loss : 0.0007448988617397845\n",
            "Train loss : 0.0015867282636463642\n",
            "Train loss : 0.001223469735123217\n",
            "Train loss : 0.0013379664160311222\n",
            "Train loss : 0.0014921068213880062\n",
            "Train loss : 0.0011670858366414905\n",
            "Train loss : 0.0009976582368835807\n",
            "Train loss : 0.0011559856357052922\n",
            "Train loss : 0.0014374747406691313\n",
            "Train loss : 0.00110835419036448\n",
            "Train loss : 0.0010824550408869982\n",
            "Train loss : 0.0020375284366309643\n",
            "Train loss : 0.0011805574176833034\n",
            "Train loss : 0.0015932925743982196\n",
            "Train loss : 0.0009060962474904954\n",
            "Train loss : 0.0005368978600017726\n",
            "Train loss : 0.0007154139457270503\n",
            "Train loss : 0.0011172598460689187\n",
            "Train loss : 0.0019946482498198748\n",
            "Train loss : 0.0018507143249735236\n",
            "Train loss : 0.0012933682883158326\n",
            "Train loss : 0.0011574645759537816\n",
            "Train loss : 0.0015315505443140864\n",
            "Train loss : 0.0024228787515312433\n",
            "Train loss : 0.0014522605342790484\n",
            "Train loss : 0.0012858007103204727\n",
            "Train loss : 0.0009808347094804049\n",
            "Train loss : 0.0011456041829660535\n",
            "Train loss : 0.0006432788213714957\n",
            "Train loss : 0.0017165904864668846\n",
            "Train loss : 0.0018224160885438323\n",
            "Train loss : 0.00041601492557674646\n",
            "Train loss : 0.0013824424240738153\n",
            "Train loss : 0.0013473068829625845\n",
            "Train loss : 0.0006819318514317274\n",
            "Train loss : 0.0011542305583134294\n",
            "Train loss : 0.0012255910551175475\n",
            "Train loss : 0.0013727449113503098\n",
            "Train loss : 0.0013347883941605687\n",
            "Train loss : 0.0008360350038856268\n",
            "Train loss : 0.0006301086978055537\n",
            "Looked at 12800/60000 samples.\n",
            "Train loss : 0.0009490509401075542\n",
            "Train loss : 0.0011483813868835568\n",
            "Train loss : 0.0008595484541729093\n",
            "Train loss : 0.0007981039816513658\n",
            "Train loss : 0.001194269279949367\n",
            "Train loss : 0.0006209605489857495\n",
            "Train loss : 0.0010248181642964482\n",
            "Train loss : 0.0006104062194935977\n",
            "Train loss : 0.0010986109264194965\n",
            "Train loss : 0.000709027168340981\n",
            "Train loss : 0.0010446966625750065\n",
            "Train loss : 0.0015928209759294987\n",
            "Train loss : 0.001209234818816185\n",
            "Train loss : 0.0007523652166128159\n",
            "Train loss : 0.0009611690766178071\n",
            "Train loss : 0.0016578701324760914\n",
            "Train loss : 0.0014537512324750423\n",
            "Train loss : 0.0007008204120211303\n",
            "Train loss : 0.0008715020376257598\n",
            "Train loss : 0.0010796957649290562\n",
            "Train loss : 0.000891445146407932\n",
            "Train loss : 0.000975878385361284\n",
            "Train loss : 0.000517406384460628\n",
            "Train loss : 0.0009509433293715119\n",
            "Train loss : 0.0007198421517387033\n",
            "Train loss : 0.0007957653142511845\n",
            "Train loss : 0.0008550265920348465\n",
            "Train loss : 0.00245838169939816\n",
            "Train loss : 0.0010655679507181048\n",
            "Train loss : 0.0017597818514332175\n",
            "Train loss : 0.0009344473364762962\n",
            "Train loss : 0.0008460113313049078\n",
            "Train loss : 0.0006209920975379646\n",
            "Train loss : 0.001547973952256143\n",
            "Train loss : 0.0019559485372155905\n",
            "Train loss : 0.0013064806116744876\n",
            "Train loss : 0.0011949187610298395\n",
            "Train loss : 0.0008784141391515732\n",
            "Train loss : 0.0017898064106702805\n",
            "Train loss : 0.000701393757481128\n",
            "Train loss : 0.0007708278717473149\n",
            "Train loss : 0.0016284489538520575\n",
            "Train loss : 0.0005118466215208173\n",
            "Train loss : 0.0008008580189198256\n",
            "Train loss : 0.0011459843954071403\n",
            "Train loss : 0.0009592139977030456\n",
            "Train loss : 0.00027239188784733415\n",
            "Train loss : 0.0012559012975543737\n",
            "Train loss : 0.0019411178072914481\n",
            "Train loss : 0.00043781561544165015\n",
            "Train loss : 0.0007153441547416151\n",
            "Train loss : 0.0006413598894141614\n",
            "Train loss : 0.0005829870351590216\n",
            "Train loss : 0.0005725047667510808\n",
            "Train loss : 0.0004220874106977135\n",
            "Train loss : 0.0010495102033019066\n",
            "Train loss : 0.0008538677939213812\n",
            "Train loss : 0.0006540236063301563\n",
            "Train loss : 0.0009667194099165499\n",
            "Train loss : 0.0011061890982091427\n",
            "Train loss : 0.0006752002518624067\n",
            "Train loss : 0.0009042088058777153\n",
            "Train loss : 0.0007304356549866498\n",
            "Train loss : 0.0008910905453376472\n",
            "Train loss : 0.0005184480687603354\n",
            "Train loss : 0.0013612249167636037\n",
            "Train loss : 0.0006639992934651673\n",
            "Train loss : 0.0005780196515843272\n",
            "Train loss : 0.0007403999334201217\n",
            "Train loss : 0.0006070354138500988\n",
            "Train loss : 0.0005470768664963543\n",
            "Train loss : 0.0006443742313422263\n",
            "Train loss : 0.0005138243432156742\n",
            "Train loss : 0.001126364921219647\n",
            "Train loss : 0.0004903964581899345\n",
            "Train loss : 0.0007105504046194255\n",
            "Train loss : 0.0011916069779545069\n",
            "Train loss : 0.0008894386701285839\n",
            "Train loss : 0.0011265400098636746\n",
            "Train loss : 0.00039880856638774276\n",
            "Train loss : 0.0007048406987451017\n",
            "Train loss : 0.0011164852185174823\n",
            "Train loss : 0.00037579945637844503\n",
            "Train loss : 0.0015991119435057044\n",
            "Train loss : 0.001330710481852293\n",
            "Train loss : 0.0005225989734753966\n",
            "Train loss : 0.0017162376316264272\n",
            "Train loss : 0.0011085130972787738\n",
            "Train loss : 0.0006642314838245511\n",
            "Train loss : 0.0006925114430487156\n",
            "Train loss : 0.001008789986371994\n",
            "Train loss : 0.0007768137729726732\n",
            "Train loss : 0.0010848796227946877\n",
            "Train loss : 0.0006499310838989913\n",
            "Train loss : 0.0008051638724282384\n",
            "Train loss : 0.0005254955613054335\n",
            "Train loss : 0.0004925435641780496\n",
            "Train loss : 0.0005169836222194135\n",
            "Train loss : 0.000711754139047116\n",
            "Train loss : 0.0007094032480381429\n",
            "Train loss : 0.0007596687646582723\n",
            "Train loss : 0.0011083793360739946\n",
            "Train loss : 0.0006201089126989245\n",
            "Train loss : 0.0010785071644932032\n",
            "Train loss : 0.0014791234862059355\n",
            "Train loss : 0.0009715852793306112\n",
            "Train loss : 0.00042963417945429683\n",
            "Train loss : 0.0008021621033549309\n",
            "Train loss : 0.0006594453006982803\n",
            "Train loss : 0.000770015234593302\n",
            "Train loss : 0.0006655189790762961\n",
            "Train loss : 0.0010117462370544672\n",
            "Train loss : 0.00117283477447927\n",
            "Train loss : 0.0004839295579586178\n",
            "Train loss : 0.0009544626227580011\n",
            "Train loss : 0.0007744921022094786\n",
            "Train loss : 0.0006214997847564518\n",
            "Train loss : 0.0005021195393055677\n",
            "Train loss : 0.0007947596022859216\n",
            "Train loss : 0.0008557624532841146\n",
            "Train loss : 0.0005486401496455073\n",
            "Train loss : 0.0008123514708131552\n",
            "Train loss : 0.0007945075631141663\n",
            "Train loss : 0.0006519306334666908\n",
            "Train loss : 0.0008221447933465242\n",
            "Train loss : 0.0008572337683290243\n",
            "Train loss : 0.0007248034235090017\n",
            "Train loss : 0.0011693905107676983\n",
            "Train loss : 0.0009140399051830173\n",
            "Train loss : 0.00126771186478436\n",
            "Train loss : 0.0006620234926231205\n",
            "Train loss : 0.0007258871337398887\n",
            "Train loss : 0.0007308314088732004\n",
            "Train loss : 0.0006227608537301421\n",
            "Train loss : 0.0010629933094605803\n",
            "Train loss : 0.0010006515076383948\n",
            "Train loss : 0.0010980920633301139\n",
            "Train loss : 0.0013242993736639619\n",
            "Train loss : 0.0008313540020026267\n",
            "Train loss : 0.0004994618357159197\n",
            "Train loss : 0.0008379406062886119\n",
            "Train loss : 0.0010064959060400724\n",
            "Train loss : 0.0006865947507321835\n",
            "Train loss : 0.001378736225888133\n",
            "Train loss : 0.0009036011178977787\n",
            "Train loss : 0.0010407017543911934\n",
            "Train loss : 0.00039123676833696663\n",
            "Train loss : 0.0005262959166429937\n",
            "Train loss : 0.0005645600613206625\n",
            "Train loss : 0.0009706910350359976\n",
            "Train loss : 0.0008218414732255042\n",
            "Train loss : 0.0008410389418713748\n",
            "Train loss : 0.0005180154112167656\n",
            "Train loss : 0.000460540089989081\n",
            "Train loss : 0.0007505157846026123\n",
            "Train loss : 0.0006083507323637605\n",
            "Train loss : 0.0012922404566779733\n",
            "Train loss : 0.0015727378195151687\n",
            "Train loss : 0.0005574581446126103\n",
            "Train loss : 0.0006169046391732991\n",
            "Train loss : 0.00041348274680785835\n",
            "Train loss : 0.0007650320767425001\n",
            "Train loss : 0.0004375361022539437\n",
            "Train loss : 0.0006428954657167196\n",
            "Train loss : 0.0009971034014597535\n",
            "Train loss : 0.0007204196299426258\n",
            "Train loss : 0.0006632365984842181\n",
            "Train loss : 0.0007006922387517989\n",
            "Train loss : 0.0012105609057471156\n",
            "Train loss : 0.0007014847942627966\n",
            "Train loss : 0.0007957437192089856\n",
            "Train loss : 0.00048970733769238\n",
            "Train loss : 0.000549175834748894\n",
            "Train loss : 0.0008170296787284315\n",
            "Train loss : 0.0010842917254194617\n",
            "Train loss : 0.0007304792525246739\n",
            "Train loss : 0.0009846192551776767\n",
            "Train loss : 0.0006557955057360232\n",
            "Train loss : 0.0006459623109549284\n",
            "Train loss : 0.0007777118007652462\n",
            "Train loss : 0.0010378187289461493\n",
            "Train loss : 0.0006478065042756498\n",
            "Train loss : 0.0008436111966148019\n",
            "Train loss : 0.0010078701889142394\n",
            "Train loss : 0.0005352644366212189\n",
            "Train loss : 0.0006210542051121593\n",
            "Train loss : 0.000660605903249234\n",
            "Train loss : 0.001400842098519206\n",
            "Train loss : 0.0009330147877335548\n",
            "Train loss : 0.0005338329938240349\n",
            "Train loss : 0.0006696170312352479\n",
            "Train loss : 0.0007142113172449172\n",
            "Train loss : 0.000599766499362886\n",
            "Train loss : 0.0006131198024377227\n",
            "Train loss : 0.0004406094958540052\n",
            "Train loss : 0.0004635865625459701\n",
            "Train loss : 0.0005146458861418068\n",
            "Train loss : 0.000599660153966397\n",
            "Train loss : 0.0007020562188699841\n",
            "Train loss : 0.000683253922034055\n",
            "Train loss : 0.0008060111431404948\n",
            "Train loss : 0.00025289563927799463\n",
            "Train loss : 0.0008321485365740955\n",
            "Train loss : 0.000876390899065882\n",
            "Train loss : 0.0008490023319609463\n",
            "Train loss : 0.000561846187338233\n",
            "Train loss : 0.001279690070077777\n",
            "Train loss : 0.001247034640982747\n",
            "Train loss : 0.0009604947990737855\n",
            "Train loss : 0.000655811105389148\n",
            "Train loss : 0.0007594683556817472\n",
            "Train loss : 0.0007174784550443292\n",
            "Train loss : 0.000959052937105298\n",
            "Train loss : 0.0010426795342937112\n",
            "Train loss : 0.0007988144061528146\n",
            "Train loss : 0.000541832996532321\n",
            "Train loss : 0.0005150026991032064\n",
            "Train loss : 0.0010319724678993225\n",
            "Train loss : 0.0004389056994114071\n",
            "Train loss : 0.0006692600436508656\n",
            "Train loss : 0.0010566626442596316\n",
            "Train loss : 0.0006530409445986152\n",
            "Train loss : 0.0006951357354409993\n",
            "Train loss : 0.0010636382503435016\n",
            "Train loss : 0.0010565284173935652\n",
            "Train loss : 0.000773594481870532\n",
            "Train loss : 0.0006354673532769084\n",
            "Train loss : 0.0003070739039685577\n",
            "Train loss : 0.0008921834523789585\n",
            "Train loss : 0.0005187049973756075\n",
            "Train loss : 0.00042788765858858824\n",
            "Train loss : 0.0005253818235360086\n",
            "Train loss : 0.0007959019276313484\n",
            "Train loss : 0.0011265489738434553\n",
            "Train loss : 0.0005603283061645925\n",
            "Train loss : 0.0003625651297625154\n",
            "Train loss : 0.0006039949366822839\n",
            "Train loss : 0.000276469363598153\n",
            "Train loss : 0.0008883979171514511\n",
            "Train loss : 0.0007360299932770431\n",
            "Train loss : 0.0012662694789469242\n",
            "Train loss : 0.0005723746726289392\n",
            "Train loss : 0.0005461559630930424\n",
            "Train loss : 0.0004982192767783999\n",
            "Train loss : 0.0006420749123208225\n",
            "Train loss : 0.0005032024928368628\n",
            "Train loss : 0.0010778175201267004\n",
            "Train loss : 0.0006750322063453496\n",
            "Train loss : 0.0010808139340952039\n",
            "Train loss : 0.0006162821664474905\n",
            "Train loss : 0.000586693175137043\n",
            "Train loss : 0.0005735787563025951\n",
            "Train loss : 0.0004691949870903045\n",
            "Train loss : 0.0003512315743137151\n",
            "Train loss : 0.0006440758588723838\n",
            "Train loss : 0.0005432043690234423\n",
            "Train loss : 0.0009029874345287681\n",
            "Train loss : 0.001000348711386323\n",
            "Train loss : 0.0004670758207794279\n",
            "Train loss : 0.0006391886272467673\n",
            "Train loss : 0.0008315660525113344\n",
            "Train loss : 0.00047421810450032353\n",
            "Train loss : 0.00039612207910977304\n",
            "Train loss : 0.0008702980121597648\n",
            "Train loss : 0.001115861814469099\n",
            "Train loss : 0.0008548696059733629\n",
            "Train loss : 0.0005168627249076962\n",
            "Train loss : 0.001057986170053482\n",
            "Train loss : 0.0008334078011102974\n",
            "Train loss : 0.0006356617086566985\n",
            "Train loss : 0.000769460282754153\n",
            "Train loss : 0.00029522873228415847\n",
            "Train loss : 0.0008068020106293261\n",
            "Train loss : 0.0007807271322235465\n",
            "Train loss : 0.00038066046545282006\n",
            "Train loss : 0.0007211811025626957\n",
            "Train loss : 0.0011899614473804832\n",
            "Train loss : 0.0003303854900877923\n",
            "Train loss : 0.0005115735693834722\n",
            "Train loss : 0.0007995396154001355\n",
            "Train loss : 0.000353028008248657\n",
            "Train loss : 0.0012063096510246396\n",
            "Train loss : 0.0006295058410614729\n",
            "Train loss : 0.0006661162478849292\n",
            "Train loss : 0.000571571639738977\n",
            "Train loss : 0.0005693097482435405\n",
            "Train loss : 0.0008001686655916274\n",
            "Train loss : 0.00041495199548080564\n",
            "Train loss : 0.0006596765015274286\n",
            "Train loss : 0.0004268903285264969\n",
            "Train loss : 0.0005404616822488606\n",
            "Train loss : 0.00029181435820646584\n",
            "Train loss : 0.0005337405600585043\n",
            "Train loss : 0.000537694722879678\n",
            "Train loss : 0.0005050466861575842\n",
            "Train loss : 0.0005580172291956842\n",
            "Train loss : 0.0004184918652754277\n",
            "Train loss : 0.0008175854454748333\n",
            "Train loss : 0.000655852141790092\n",
            "Train loss : 0.0002922829007729888\n",
            "Train loss : 0.0010225701844319701\n",
            "Train loss : 0.0007488182745873928\n",
            "Train loss : 0.0008632785174995661\n",
            "Train loss : 0.0003708612348418683\n",
            "Train loss : 0.001240518526174128\n",
            "Train loss : 0.0009869177592918277\n",
            "Train loss : 0.0005067967576906085\n",
            "Train loss : 0.0005888399900868535\n",
            "Train loss : 0.0007517952471971512\n",
            "Train loss : 0.0005239254678599536\n",
            "Train loss : 0.0005925932200625539\n",
            "Train loss : 0.0003559562610462308\n",
            "Train loss : 0.0004769718798343092\n",
            "Train loss : 0.0007718601846136153\n",
            "Train loss : 0.00035022146767005324\n",
            "Train loss : 0.000555148406419903\n",
            "Train loss : 0.0005616102716885507\n",
            "Train loss : 0.0006592708523385227\n",
            "Train loss : 0.000978182884864509\n",
            "Train loss : 0.0005830525769852102\n",
            "Train loss : 0.000498567707836628\n",
            "Train loss : 0.0003978614113293588\n",
            "Train loss : 0.0004480764036998153\n",
            "Train loss : 0.0005472196498885751\n",
            "Train loss : 0.0004325348709244281\n",
            "Train loss : 0.00047905140672810376\n",
            "Train loss : 0.00038674965617246926\n",
            "Train loss : 0.0005351764848455787\n",
            "Train loss : 0.0005771435680799186\n",
            "Train loss : 0.0004933004966005683\n",
            "Train loss : 0.0002751861757133156\n",
            "Train loss : 0.0007984169060364366\n",
            "Train loss : 0.00044479803182184696\n",
            "Train loss : 0.0005970624042674899\n",
            "Train loss : 0.0004917935584671795\n",
            "Train loss : 0.00043524158536456525\n",
            "Train loss : 0.0005678121815435588\n",
            "Train loss : 0.0006908793584443629\n",
            "Train loss : 0.0004020110354758799\n",
            "Train loss : 0.0003602994838729501\n",
            "Train loss : 0.0005960803246125579\n",
            "Train loss : 0.00029539287788793445\n",
            "Train loss : 0.00023336299636866897\n",
            "Train loss : 0.0006575454026460648\n",
            "Train loss : 0.0008800033829174936\n",
            "Train loss : 0.0005514590884558856\n",
            "Train loss : 0.0005686548538506031\n",
            "Train loss : 0.0008317268802784383\n",
            "Train loss : 0.0005509061738848686\n",
            "Train loss : 0.0004617213853634894\n",
            "Train loss : 0.0006862196023575962\n",
            "Train loss : 0.000662942067719996\n",
            "Train loss : 0.00044941052328795195\n",
            "Train loss : 0.0004839337489102036\n",
            "Train loss : 0.0006024673930369318\n",
            "Train loss : 0.0005049248575232923\n",
            "Train loss : 0.0002307170070707798\n",
            "Train loss : 0.0007884361548349261\n",
            "Train loss : 0.0006250838050618768\n",
            "Train loss : 0.0005449954769574106\n",
            "Train loss : 0.0007036185706965625\n",
            "Train loss : 0.0004477959591895342\n",
            "Train loss : 0.0004138709045946598\n",
            "Train loss : 0.0009866127511486411\n",
            "Train loss : 0.0006461747107096016\n",
            "Train loss : 0.000307805894408375\n",
            "Train loss : 0.0004384633502922952\n",
            "Train loss : 0.0006296960054896772\n",
            "Train loss : 0.0005567687912844121\n",
            "Train loss : 0.0002975630050059408\n",
            "Train loss : 0.000311806594254449\n",
            "Train loss : 0.00027140422025695443\n",
            "Train loss : 0.0004642452986445278\n",
            "Train loss : 0.00021502198069356382\n",
            "Train loss : 0.00056932755978778\n",
            "Train loss : 0.0008662937325425446\n",
            "Train loss : 0.0004535429470706731\n",
            "Train loss : 0.0009441351285204291\n",
            "Train loss : 0.0004081742954440415\n",
            "Train loss : 0.0007269902271218598\n",
            "Train loss : 0.0008926753071136773\n",
            "Train loss : 0.0003641692746896297\n",
            "Train loss : 0.0007294320384971797\n",
            "Train loss : 0.0005079565453343093\n",
            "Train loss : 0.000564665300771594\n",
            "Train loss : 0.0005304359365254641\n",
            "Train loss : 0.000721811200492084\n",
            "Train loss : 0.00048638260341249406\n",
            "Train loss : 0.0005462808767333627\n",
            "Train loss : 0.00040599648491479456\n",
            "Train loss : 0.0006916364072822034\n",
            "Train loss : 0.0004395435389596969\n",
            "Train loss : 0.0009933100081980228\n",
            "Train loss : 0.00012309948215261102\n",
            "Train loss : 0.0006940349121578038\n",
            "Train loss : 0.0005902119446545839\n",
            "Train loss : 0.00033900237758643925\n",
            "Train loss : 0.0008049225434660912\n",
            "Train loss : 0.0004635097284335643\n",
            "Train loss : 0.000347059074556455\n",
            "Looked at 25600/60000 samples.\n",
            "Train loss : 0.0008472735644318163\n",
            "Train loss : 0.0001959489018190652\n",
            "Train loss : 0.00028174155158922076\n",
            "Train loss : 0.0004713795497082174\n",
            "Train loss : 0.0003406832693144679\n",
            "Train loss : 0.0009875636314973235\n",
            "Train loss : 0.0008138840785250068\n",
            "Train loss : 0.0004393878043629229\n",
            "Train loss : 0.0007127317949198186\n",
            "Train loss : 0.0005817216006107628\n",
            "Train loss : 0.0008826894336380064\n",
            "Train loss : 0.00047843396896496415\n",
            "Train loss : 0.0008748178370296955\n",
            "Train loss : 0.0004192370397504419\n",
            "Train loss : 0.00037360319402068853\n",
            "Train loss : 0.0005819749203510582\n",
            "Train loss : 0.0003702632384374738\n",
            "Train loss : 0.00033297008485533297\n",
            "Train loss : 0.0004144037375226617\n",
            "Train loss : 0.00043793939403258264\n",
            "Train loss : 0.00026298483135178685\n",
            "Train loss : 0.00047183025162667036\n",
            "Train loss : 0.0002549430064391345\n",
            "Train loss : 0.0006593963480554521\n",
            "Train loss : 0.00040969206020236015\n",
            "Train loss : 0.0005457702209241688\n",
            "Train loss : 0.00048823689576238394\n",
            "Train loss : 0.00043681584065780044\n",
            "Train loss : 0.0004782378673553467\n",
            "Train loss : 0.0004251757054589689\n",
            "Train loss : 0.0004999744705855846\n",
            "Train loss : 0.000653106952086091\n",
            "Train loss : 0.0007216461235657334\n",
            "Train loss : 0.0005733161233365536\n",
            "Train loss : 0.0007960348157212138\n",
            "Train loss : 0.0007239168044179678\n",
            "Train loss : 0.0006787563906982541\n",
            "Train loss : 0.0007269888301379979\n",
            "Train loss : 0.0008087278110906482\n",
            "Train loss : 0.00029112992342561483\n",
            "Train loss : 0.0007754893158562481\n",
            "Train loss : 0.0005516741075553\n",
            "Train loss : 0.00035485843545757234\n",
            "Train loss : 0.0011418636422604322\n",
            "Train loss : 0.0005122395814396441\n",
            "Train loss : 0.00041653806692920625\n",
            "Train loss : 0.0006254025502130389\n",
            "Train loss : 0.0006435135146602988\n",
            "Train loss : 0.0004849350079894066\n",
            "Train loss : 0.0004716221010312438\n",
            "Train loss : 0.0006561496993526816\n",
            "Train loss : 0.0004036014142911881\n",
            "Train loss : 0.00038053389289416373\n",
            "Train loss : 0.00020204205065965652\n",
            "Train loss : 0.0006126799853518605\n",
            "Train loss : 0.0007761153974570334\n",
            "Train loss : 0.0004946515546180308\n",
            "Train loss : 0.000469610036816448\n",
            "Train loss : 0.0003894507244694978\n",
            "Train loss : 0.00045139226131141186\n",
            "Train loss : 0.0008971362258307636\n",
            "Train loss : 0.0003372560895513743\n",
            "Train loss : 0.0006351391784846783\n",
            "Train loss : 0.0005718517350032926\n",
            "Train loss : 0.0005983595619909465\n",
            "Train loss : 0.0007615699432790279\n",
            "Train loss : 0.00017735219444148242\n",
            "Train loss : 0.0005478360690176487\n",
            "Train loss : 0.0003719409287441522\n",
            "Train loss : 0.0010129543952643871\n",
            "Train loss : 0.0005145909963175654\n",
            "Train loss : 0.0006258912035264075\n",
            "Train loss : 0.0006408712361007929\n",
            "Train loss : 0.00081229442730546\n",
            "Train loss : 0.0006810930208303034\n",
            "Train loss : 0.000437102688010782\n",
            "Train loss : 0.0005418212967924774\n",
            "Train loss : 0.000306378526147455\n",
            "Train loss : 0.0005694900755770504\n",
            "Train loss : 0.00045333063462749124\n",
            "Train loss : 0.00046832975931465626\n",
            "Train loss : 0.00037528201937675476\n",
            "Train loss : 0.0004222881980240345\n",
            "Train loss : 0.0005996422260068357\n",
            "Train loss : 0.000585255038458854\n",
            "Train loss : 0.00018858339171856642\n",
            "Train loss : 0.00048484656144864857\n",
            "Train loss : 0.0006194965681061149\n",
            "Train loss : 0.00047996113426052034\n",
            "Train loss : 0.0005238300655037165\n",
            "Train loss : 0.0003486979112494737\n",
            "Train loss : 0.0003507900983095169\n",
            "Train loss : 0.0007669192273169756\n",
            "Train loss : 0.0005575274699367583\n",
            "Train loss : 0.0005168229690752923\n",
            "Train loss : 0.00042290191049687564\n",
            "Train loss : 0.00014599594578612596\n",
            "Train loss : 0.0006432614754885435\n",
            "Train loss : 0.000588886730838567\n",
            "Train loss : 0.00040618213824927807\n",
            "Train loss : 0.00028423246112652123\n",
            "Train loss : 0.0003808165492955595\n",
            "Train loss : 0.000490863632876426\n",
            "Train loss : 0.000424306868808344\n",
            "Train loss : 0.00041915656765922904\n",
            "Train loss : 0.0004309122741688043\n",
            "Train loss : 0.0005605150363408029\n",
            "Train loss : 0.0004181454423815012\n",
            "Train loss : 0.0005864215781912208\n",
            "Train loss : 0.00029857936897315085\n",
            "Train loss : 0.00021465022291522473\n",
            "Train loss : 0.00042432063492015004\n",
            "Train loss : 0.0003080031310673803\n",
            "Train loss : 0.000621157290879637\n",
            "Train loss : 0.0003379702684469521\n",
            "Train loss : 0.00019819519366137683\n",
            "Train loss : 0.0004315776750445366\n",
            "Train loss : 0.00042690974078141153\n",
            "Train loss : 0.00048122182488441467\n",
            "Train loss : 0.0005428203148767352\n",
            "Train loss : 0.0006666662520729005\n",
            "Train loss : 0.00047484011156484485\n",
            "Train loss : 0.00038988172309473157\n",
            "Train loss : 0.0004664001753553748\n",
            "Train loss : 0.00048319774214178324\n",
            "Train loss : 0.0007651671185158193\n",
            "Train loss : 0.00018444943998474628\n",
            "Train loss : 0.0002648758818395436\n",
            "Train loss : 0.00039623305201530457\n",
            "Train loss : 0.000366957247024402\n",
            "Train loss : 0.0004607666051015258\n",
            "Train loss : 0.0008580063586123288\n",
            "Train loss : 0.0004881350905634463\n",
            "Train loss : 0.00047641858691349626\n",
            "Train loss : 0.000328235502820462\n",
            "Train loss : 0.0004413228889461607\n",
            "Train loss : 0.0005023634876124561\n",
            "Train loss : 0.00034914989373646677\n",
            "Train loss : 0.00028600869700312614\n",
            "Train loss : 0.001074007130227983\n",
            "Train loss : 0.0005855528288520873\n",
            "Train loss : 0.00033919583074748516\n",
            "Train loss : 0.00034055934520438313\n",
            "Train loss : 0.0004527885466814041\n",
            "Train loss : 0.0003397081163711846\n",
            "Train loss : 0.00031899960595183074\n",
            "Train loss : 0.00028091686544939876\n",
            "Train loss : 0.0005810112343169749\n",
            "Train loss : 0.00044831965351477265\n",
            "Train loss : 0.00031241882243193686\n",
            "Train loss : 0.0005317491013556719\n",
            "Train loss : 0.0002743243530858308\n",
            "Train loss : 0.00029964884743094444\n",
            "Train loss : 0.0005988351185806096\n",
            "Train loss : 0.0003072253311984241\n",
            "Train loss : 0.0004648837784770876\n",
            "Train loss : 0.00041610526386648417\n",
            "Train loss : 0.0003434397222008556\n",
            "Train loss : 0.0002692931448109448\n",
            "Train loss : 0.0005605242331512272\n",
            "Train loss : 0.0004266518517397344\n",
            "Train loss : 0.0008457056246697903\n",
            "Train loss : 0.0001401145418640226\n",
            "Train loss : 0.0006035049445927143\n",
            "Train loss : 0.00036569408257491887\n",
            "Train loss : 0.00040448602521792054\n",
            "Train loss : 0.0007079037604853511\n",
            "Train loss : 0.0005199772422201931\n",
            "Train loss : 0.00039265159284695983\n",
            "Train loss : 0.0010457572061568499\n",
            "Train loss : 0.0004240135895088315\n",
            "Train loss : 0.00047570280730724335\n",
            "Train loss : 0.0004328575450927019\n",
            "Train loss : 0.00031315581873059273\n",
            "Train loss : 0.0004367394431028515\n",
            "Train loss : 0.00043388543417677283\n",
            "Train loss : 0.0004196873342152685\n",
            "Train loss : 0.0003916776040568948\n",
            "Train loss : 0.0005517946556210518\n",
            "Train loss : 0.0003010709770023823\n",
            "Train loss : 0.0004896789905615151\n",
            "Train loss : 0.0003553615533746779\n",
            "Train loss : 0.0007079083588905632\n",
            "Train loss : 0.000768748635891825\n",
            "Train loss : 0.0005652491236105561\n",
            "Train loss : 0.0003553360002115369\n",
            "Train loss : 0.00048805802362039685\n",
            "Train loss : 0.0003215030883438885\n",
            "Train loss : 0.0005546465399675071\n",
            "Train loss : 0.0002395845076534897\n",
            "Train loss : 0.0006602514768019319\n",
            "Train loss : 0.00017725587531458586\n",
            "Train loss : 0.0007009520195424557\n",
            "Train loss : 0.00031655686325393617\n",
            "Train loss : 0.00044072355376556516\n",
            "Train loss : 0.00018878866103477776\n",
            "Train loss : 0.0005173732060939074\n",
            "Train loss : 0.0005270225228741765\n",
            "Train loss : 0.0003195450408384204\n",
            "Train loss : 0.00046300829853862524\n",
            "Train loss : 0.00021280064538586885\n",
            "Train loss : 0.00041451992001384497\n",
            "Train loss : 0.00024964052136056125\n",
            "Train loss : 0.0006035774131305516\n",
            "Train loss : 0.000755005341488868\n",
            "Train loss : 9.335124923381954e-05\n",
            "Train loss : 0.0005680845933966339\n",
            "Train loss : 0.0007915823953226209\n",
            "Train loss : 0.0006205799872986972\n",
            "Train loss : 0.0003950509417336434\n",
            "Train loss : 0.0005773660377599299\n",
            "Train loss : 0.0006104911444708705\n",
            "Train loss : 0.0005981094436720014\n",
            "Train loss : 0.0005891375476494431\n",
            "Train loss : 0.0006189044215716422\n",
            "Train loss : 0.00044746161438524723\n",
            "Train loss : 0.0003347962920088321\n",
            "Train loss : 0.0006596359889954329\n",
            "Train loss : 0.0003089422534685582\n",
            "Train loss : 0.00032480485970154405\n",
            "Train loss : 0.0005583962192758918\n",
            "Train loss : 0.0005261440528556705\n",
            "Train loss : 0.0003170153358951211\n",
            "Train loss : 0.0006269103032536805\n",
            "Train loss : 0.0005505230510607362\n",
            "Train loss : 0.0004134102782700211\n",
            "Train loss : 0.0003169369592797011\n",
            "Train loss : 0.0005152410012669861\n",
            "Train loss : 0.0005035088397562504\n",
            "Train loss : 0.0002499175607226789\n",
            "Train loss : 0.0005477680242620409\n",
            "Train loss : 0.0002114339149557054\n",
            "Train loss : 0.00023781548952683806\n",
            "Train loss : 0.00027095311088487506\n",
            "Train loss : 0.000394715229049325\n",
            "Train loss : 0.000206548094865866\n",
            "Train loss : 0.00041442245128564537\n",
            "Train loss : 0.0004256083338987082\n",
            "Train loss : 0.00047211089986376464\n",
            "Train loss : 0.0002985669416375458\n",
            "Train loss : 0.0005700070760212839\n",
            "Train loss : 0.0004331058298703283\n",
            "Train loss : 0.00025021599140018225\n",
            "Train loss : 0.00037747601163573563\n",
            "Train loss : 0.0005524909938685596\n",
            "Train loss : 0.0008151186630129814\n",
            "Train loss : 0.00025331886718049645\n",
            "Train loss : 0.000370413763448596\n",
            "Train loss : 0.0005622936878353357\n",
            "Train loss : 0.0003045014454983175\n",
            "Train loss : 0.00043297847150824964\n",
            "Train loss : 0.0005025429418310523\n",
            "Train loss : 0.0003839200653601438\n",
            "Train loss : 0.00046304220450110734\n",
            "Train loss : 0.0004484331584535539\n",
            "Train loss : 0.0003237750206608325\n",
            "Train loss : 0.00026622964651323855\n",
            "Train loss : 0.00044082230306230485\n",
            "Train loss : 0.00026698861620388925\n",
            "Train loss : 0.0005024898564442992\n",
            "Train loss : 0.00015920850273687392\n",
            "Train loss : 0.00045625263010151684\n",
            "Train loss : 0.00029962329426780343\n",
            "Train loss : 0.0005672412808053195\n",
            "Train loss : 0.00041853360016830266\n",
            "Train loss : 0.00025559726054780185\n",
            "Train loss : 0.0003639811766333878\n",
            "Train loss : 0.0006136156735010445\n",
            "Train loss : 0.0003040443698409945\n",
            "Train loss : 0.0004195463552605361\n",
            "Train loss : 0.0003569636319298297\n",
            "Train loss : 0.000437212351243943\n",
            "Train loss : 0.0008803170640021563\n",
            "Train loss : 0.0003586584352888167\n",
            "Train loss : 0.00033544813049957156\n",
            "Train loss : 0.0003951385442633182\n",
            "Train loss : 0.0003510852111503482\n",
            "Train loss : 0.00029543868731707335\n",
            "Train loss : 0.000406536360969767\n",
            "Train loss : 0.0005310864071361721\n",
            "Train loss : 0.0002921774284914136\n",
            "Train loss : 0.00015900254948064685\n",
            "Train loss : 0.0007225197041407228\n",
            "Train loss : 0.0003276671632193029\n",
            "Train loss : 0.0004538926878012717\n",
            "Train loss : 0.00023312278790399432\n",
            "Train loss : 0.0005152848898433149\n",
            "Train loss : 0.00038814826984889805\n",
            "Train loss : 0.0006333934143185616\n",
            "Train loss : 0.0003052224055863917\n",
            "Train loss : 0.0003969598619733006\n",
            "Train loss : 0.0005778586491942406\n",
            "Train loss : 0.00030421355040743947\n",
            "Train loss : 0.00034378826967440546\n",
            "Train loss : 0.00032394088339060545\n",
            "Train loss : 0.0004336999845691025\n",
            "Train loss : 0.0004289158678147942\n",
            "Train loss : 0.0004262041475158185\n",
            "Train loss : 0.0003920481540262699\n",
            "Train loss : 0.000383571139536798\n",
            "Train loss : 0.00028385306359268725\n",
            "Train loss : 0.00031985685927793384\n",
            "Train loss : 0.00032544834539294243\n",
            "Train loss : 0.0006090964889153838\n",
            "Train loss : 0.00028812504024244845\n",
            "Train loss : 0.00037792633520439267\n",
            "Train loss : 0.000403359008487314\n",
            "Train loss : 0.00027832025079987943\n",
            "Train loss : 0.0004674818192142993\n",
            "Train loss : 0.00046915095299482346\n",
            "Train loss : 0.0004311161465011537\n",
            "Train loss : 0.00020590917847584933\n",
            "Train loss : 0.00038503945688717067\n",
            "Train loss : 0.0005262444028630853\n",
            "Train loss : 0.0002586085465736687\n",
            "Train loss : 0.00029444144456647336\n",
            "Train loss : 0.0005109431222081184\n",
            "Train loss : 0.0002611345553305\n",
            "Train loss : 0.0005190575029700994\n",
            "Train loss : 0.00041440798668190837\n",
            "Train loss : 0.00032942634425126016\n",
            "Train loss : 0.0005757076432928443\n",
            "Train loss : 0.0003108429373241961\n",
            "Train loss : 0.00019820264424197376\n",
            "Train loss : 0.00041753717232495546\n",
            "Train loss : 0.000444311008322984\n",
            "Train loss : 0.00016982160741463304\n",
            "Train loss : 0.00039489020127803087\n",
            "Train loss : 0.00022901975899003446\n",
            "Train loss : 0.0005008841981180012\n",
            "Train loss : 0.0004911848227493465\n",
            "Train loss : 0.0005462536355480552\n",
            "Train loss : 0.0004736865230370313\n",
            "Train loss : 0.0002360517391934991\n",
            "Train loss : 0.00029552014893852174\n",
            "Train loss : 0.000708161445800215\n",
            "Train loss : 0.0005969048361293972\n",
            "Train loss : 0.00028682631091214716\n",
            "Train loss : 0.000493142637424171\n",
            "Train loss : 0.0002622566244099289\n",
            "Train loss : 0.0003427591873332858\n",
            "Train loss : 0.0004274238017387688\n",
            "Train loss : 0.0005697978194803\n",
            "Train loss : 0.00033854623325169086\n",
            "Train loss : 0.0005173922399990261\n",
            "Train loss : 0.0002732929715421051\n",
            "Train loss : 0.0003856903640553355\n",
            "Train loss : 0.00046722026308998466\n",
            "Train loss : 0.0003774551732931286\n",
            "Train loss : 0.00021843996364623308\n",
            "Train loss : 0.00047877070028334856\n",
            "Train loss : 0.000817540509160608\n",
            "Train loss : 0.0005400577210821211\n",
            "Train loss : 0.0002943297440651804\n",
            "Train loss : 0.0003402063448447734\n",
            "Train loss : 0.0004797145666088909\n",
            "Train loss : 0.0003816671669483185\n",
            "Train loss : 0.00023568081087432802\n",
            "Train loss : 0.00022175100457388908\n",
            "Train loss : 0.00036918744444847107\n",
            "Train loss : 0.0004606696602422744\n",
            "Train loss : 0.00025921579799614847\n",
            "Train loss : 0.00038316749851219356\n",
            "Train loss : 0.0005797822959721088\n",
            "Train loss : 0.00044929340947419405\n",
            "Train loss : 0.0005161541630513966\n",
            "Train loss : 0.00025756523245945573\n",
            "Train loss : 0.00026738757151179016\n",
            "Train loss : 0.00012795311340596527\n",
            "Train loss : 0.0003380937851034105\n",
            "Train loss : 0.0005812731105834246\n",
            "Train loss : 0.0002600018051452935\n",
            "Train loss : 0.0002218269364675507\n",
            "Train loss : 0.00038954944466240704\n",
            "Train loss : 0.0008815450128167868\n",
            "Train loss : 0.00019243365386500955\n",
            "Train loss : 0.00047269350034184754\n",
            "Train loss : 0.0002644200576469302\n",
            "Train loss : 0.00043628562707453966\n",
            "Train loss : 0.00026220851577818394\n",
            "Train loss : 0.0002881850814446807\n",
            "Train loss : 0.0003609434934332967\n",
            "Train loss : 0.0005402537644840777\n",
            "Train loss : 0.000368798355339095\n",
            "Train loss : 0.00026815899764187634\n",
            "Train loss : 0.0005580447614192963\n",
            "Train loss : 0.0004451730928849429\n",
            "Train loss : 0.0003686500422190875\n",
            "Train loss : 0.00033337456989102066\n",
            "Train loss : 0.00042150981607846916\n",
            "Train loss : 0.0003559040487743914\n",
            "Train loss : 0.0002521786082070321\n",
            "Train loss : 0.0002634674892760813\n",
            "Train loss : 0.0006340044201351702\n",
            "Train loss : 0.0003340734401717782\n",
            "Train loss : 0.00034023530315607786\n",
            "Train loss : 0.00033017914392985404\n",
            "Train loss : 0.000665463216137141\n",
            "Train loss : 0.0005273238057270646\n",
            "Train loss : 0.00023956195218488574\n",
            "Looked at 38400/60000 samples.\n",
            "Train loss : 0.0001288045896217227\n",
            "Train loss : 0.00032960312091745436\n",
            "Train loss : 0.0007290459470823407\n",
            "Train loss : 0.0003499679150991142\n",
            "Train loss : 0.0003019092255271971\n",
            "Train loss : 0.0003993151767645031\n",
            "Train loss : 0.00012358945969026536\n",
            "Train loss : 0.0004942750092595816\n",
            "Train loss : 0.00010802194447023794\n",
            "Train loss : 0.00017727191152516752\n",
            "Train loss : 0.0003590166161302477\n",
            "Train loss : 0.00035030889557674527\n",
            "Train loss : 0.00020468169532250613\n",
            "Train loss : 0.0001918971975101158\n",
            "Train loss : 0.0007469986448995769\n",
            "Train loss : 0.000322989682899788\n",
            "Train loss : 0.0001938055211212486\n",
            "Train loss : 0.0004988063010387123\n",
            "Train loss : 0.00044946634443476796\n",
            "Train loss : 0.00035119641688652337\n",
            "Train loss : 0.0003021273878403008\n",
            "Train loss : 0.0003813476942013949\n",
            "Train loss : 0.00042369539733044803\n",
            "Train loss : 0.00020319422765169293\n",
            "Train loss : 0.000256361294304952\n",
            "Train loss : 0.00021141281467862427\n",
            "Train loss : 0.000297647959087044\n",
            "Train loss : 0.0003683476825244725\n",
            "Train loss : 0.00035254054819233716\n",
            "Train loss : 0.00030806020367890596\n",
            "Train loss : 0.0006065929774194956\n",
            "Train loss : 0.00022362719755619764\n",
            "Train loss : 0.0002275768929393962\n",
            "Train loss : 0.0002847326686605811\n",
            "Train loss : 0.0001560283562866971\n",
            "Train loss : 0.0003545508370734751\n",
            "Train loss : 0.0005386399570852518\n",
            "Train loss : 0.00037012266693636775\n",
            "Train loss : 0.00048154135583899915\n",
            "Train loss : 0.00022784217435400933\n",
            "Train loss : 0.00044697863631881773\n",
            "Train loss : 0.00048135314136743546\n",
            "Train loss : 0.0005481282714754343\n",
            "Train loss : 0.00045602608588524163\n",
            "Train loss : 0.0002562026784289628\n",
            "Train loss : 0.0003468015929684043\n",
            "Train loss : 0.0003351623599883169\n",
            "Train loss : 0.0003117508895229548\n",
            "Train loss : 0.0004107177956029773\n",
            "Train loss : 0.0005016916547901928\n",
            "Train loss : 0.0005737491301260889\n",
            "Train loss : 0.00036448720493353903\n",
            "Train loss : 0.00015570540563203394\n",
            "Train loss : 0.0003641841176431626\n",
            "Train loss : 0.0002487762540113181\n",
            "Train loss : 0.0003945514908991754\n",
            "Train loss : 0.0002362476079724729\n",
            "Train loss : 0.00020501228573266417\n",
            "Train loss : 0.00012183529906906188\n",
            "Train loss : 0.00042662787018343806\n",
            "Train loss : 0.0004884513909928501\n",
            "Train loss : 0.0002984612947329879\n",
            "Train loss : 0.00044203244033269584\n",
            "Train loss : 0.0004900802159681916\n",
            "Train loss : 0.00018573933630250394\n",
            "Train loss : 0.00036803772673010826\n",
            "Train loss : 0.0002472632913850248\n",
            "Train loss : 0.00030825892463326454\n",
            "Train loss : 0.00031672860495746136\n",
            "Train loss : 0.0005112406215630472\n",
            "Train loss : 0.00022413430269807577\n",
            "Train loss : 0.0002649575471878052\n",
            "Train loss : 0.0003890148364007473\n",
            "Train loss : 0.00044189573964104056\n",
            "Train loss : 0.00040488832746632397\n",
            "Train loss : 0.0003142235509585589\n",
            "Train loss : 0.0006091379327699542\n",
            "Train loss : 0.00022177351638674736\n",
            "Train loss : 0.00010396985453553498\n",
            "Train loss : 0.00033390300814062357\n",
            "Train loss : 0.0001254884118679911\n",
            "Train loss : 0.00023031116870697588\n",
            "Train loss : 0.0002787838748190552\n",
            "Train loss : 0.0003385410236660391\n",
            "Train loss : 0.0002505583397578448\n",
            "Train loss : 0.00028330596978776157\n",
            "Train loss : 0.0002917068777605891\n",
            "Train loss : 0.00028185019618831575\n",
            "Train loss : 0.0006598256295546889\n",
            "Train loss : 0.0004482046642806381\n",
            "Train loss : 0.0003783355059567839\n",
            "Train loss : 0.00026506988797336817\n",
            "Train loss : 0.0003492447140160948\n",
            "Train loss : 0.00026449954020790756\n",
            "Train loss : 0.00021901607397012413\n",
            "Train loss : 0.0003820612619165331\n",
            "Train loss : 0.0003066622302867472\n",
            "Train loss : 0.00030383802368305624\n",
            "Train loss : 0.00026879229699261487\n",
            "Train loss : 0.0001992168981814757\n",
            "Train loss : 0.00046868648496456444\n",
            "Train loss : 0.00034836912527680397\n",
            "Train loss : 0.00028466657386161387\n",
            "Train loss : 0.00041641760617494583\n",
            "Train loss : 0.00021641839703079313\n",
            "Train loss : 0.0002550131466705352\n",
            "Train loss : 0.00032547226874157786\n",
            "Train loss : 0.0002932397183030844\n",
            "Train loss : 0.0006510370294563472\n",
            "Train loss : 0.00030268009868450463\n",
            "Train loss : 0.0007765197660773993\n",
            "Train loss : 0.00047101330710574985\n",
            "Train loss : 0.0007131619495339692\n",
            "Train loss : 0.00020031773601658642\n",
            "Train loss : 0.0003004636673722416\n",
            "Train loss : 0.00011210483353352174\n",
            "Train loss : 0.0003689728328026831\n",
            "Train loss : 0.0005343261291272938\n",
            "Train loss : 0.000277580606052652\n",
            "Train loss : 0.0005587776540778577\n",
            "Train loss : 0.0003418760315980762\n",
            "Train loss : 0.00032793282298371196\n",
            "Train loss : 0.00034162108204327524\n",
            "Train loss : 0.00033294898457825184\n",
            "Train loss : 0.0002199908922193572\n",
            "Train loss : 0.0006102286861278117\n",
            "Train loss : 0.00023179437266662717\n",
            "Train loss : 0.00021775435016024858\n",
            "Train loss : 0.0003093194973189384\n",
            "Train loss : 0.00034423128818161786\n",
            "Train loss : 0.0003489868831820786\n",
            "Train loss : 0.00021411583293229342\n",
            "Train loss : 0.00019035076547879726\n",
            "Train loss : 0.00019464104843791574\n",
            "Train loss : 0.0002506424207240343\n",
            "Train loss : 0.0003634327440522611\n",
            "Train loss : 0.00018801861733663827\n",
            "Train loss : 0.0002986593171954155\n",
            "Train loss : 0.00032964826095849276\n",
            "Train loss : 0.0002361697261221707\n",
            "Train loss : 0.00018949770310427994\n",
            "Train loss : 0.0005254251300357282\n",
            "Train loss : 0.00039970240322872996\n",
            "Train loss : 0.0003935999411623925\n",
            "Train loss : 0.00029956476646475494\n",
            "Train loss : 0.0002652664261404425\n",
            "Train loss : 0.000285800953861326\n",
            "Train loss : 0.0003191823197994381\n",
            "Train loss : 0.00045251395204104483\n",
            "Train loss : 0.00042403402039781213\n",
            "Train loss : 0.00034028891241177917\n",
            "Train loss : 0.0004161829419899732\n",
            "Train loss : 0.00023230734223034233\n",
            "Train loss : 0.00032108978484757245\n",
            "Train loss : 0.0003488163638394326\n",
            "Train loss : 0.00044018100015819073\n",
            "Train loss : 0.0003456573176663369\n",
            "Train loss : 0.0004395131836645305\n",
            "Train loss : 0.00044242432340979576\n",
            "Train loss : 0.00016994964971672744\n",
            "Train loss : 0.00010641950211720541\n",
            "Train loss : 0.00038282116292975843\n",
            "Train loss : 0.0003551160916686058\n",
            "Train loss : 0.00027743441751226783\n",
            "Train loss : 0.0006228576530702412\n",
            "Train loss : 0.0003916621208190918\n",
            "Train loss : 0.0003612575528677553\n",
            "Train loss : 0.00038368761306628585\n",
            "Train loss : 0.0006760520627722144\n",
            "Train loss : 0.0002903113199863583\n",
            "Train loss : 0.0002517120447009802\n",
            "Train loss : 0.0002670676331035793\n",
            "Train loss : 0.0004103374667465687\n",
            "Train loss : 0.0005092706414870918\n",
            "Train loss : 0.00019597564823925495\n",
            "Train loss : 0.00028903514612466097\n",
            "Train loss : 0.00017207744531333447\n",
            "Train loss : 0.00026303617050871253\n",
            "Train loss : 0.0002859386440832168\n",
            "Train loss : 0.00022289414482656866\n",
            "Train loss : 0.00016336528642568737\n",
            "Train loss : 0.00036875149817205966\n",
            "Train loss : 0.00012498498836066574\n",
            "Train loss : 0.0002626426285132766\n",
            "Train loss : 0.00035941152600571513\n",
            "Train loss : 0.0003644784155767411\n",
            "Train loss : 0.00043271537288092077\n",
            "Train loss : 0.00015128112863749266\n",
            "Train loss : 0.00011689269740600139\n",
            "Train loss : 0.00046378199476748705\n",
            "Train loss : 0.0005348692648112774\n",
            "Train loss : 0.00024850573390722275\n",
            "Train loss : 0.0003078650333918631\n",
            "Train loss : 0.0002863229892682284\n",
            "Train loss : 0.00033150569652207196\n",
            "Train loss : 0.00019714423979166895\n",
            "Train loss : 0.00035222535370849073\n",
            "Train loss : 0.00043198183993808925\n",
            "Train loss : 0.0003032588283531368\n",
            "Train loss : 0.0002978201664518565\n",
            "Train loss : 0.0002610885421745479\n",
            "Train loss : 0.00029702045139856637\n",
            "Train loss : 0.00011405049008317292\n",
            "Train loss : 0.0003251507587265223\n",
            "Train loss : 0.0001538478391012177\n",
            "Train loss : 0.0003351244959048927\n",
            "Train loss : 0.0003790663031395525\n",
            "Train loss : 0.00037118911859579384\n",
            "Train loss : 0.00035298694274388254\n",
            "Train loss : 0.0001602886477485299\n",
            "Train loss : 0.00028511471464298666\n",
            "Train loss : 0.00022774472017772496\n",
            "Train loss : 0.0003807483008131385\n",
            "Train loss : 0.00025198946241289377\n",
            "Train loss : 0.0002064636064460501\n",
            "Train loss : 0.0002444849815219641\n",
            "Train loss : 0.0004335304838605225\n",
            "Train loss : 0.00017865041445475072\n",
            "Train loss : 0.00024192406272049993\n",
            "Train loss : 0.0003443504392635077\n",
            "Train loss : 0.0003295595815870911\n",
            "Train loss : 0.00037432918907143176\n",
            "Train loss : 0.00033179475576616824\n",
            "Train loss : 0.00023102764680515975\n",
            "Train loss : 0.00017849591677077115\n",
            "Train loss : 0.00025193183682858944\n",
            "Train loss : 0.0003831162175629288\n",
            "Train loss : 0.00016011814295779914\n",
            "Train loss : 0.0002521478745620698\n",
            "Train loss : 0.0004771883541252464\n",
            "Train loss : 0.00033132024691440165\n",
            "Train loss : 0.00010849537648027763\n",
            "Train loss : 0.00018723424000199884\n",
            "Train loss : 0.00044043431989848614\n",
            "Train loss : 0.00019895349396392703\n",
            "Train loss : 0.0002235994761576876\n",
            "Train loss : 0.0003261478850618005\n",
            "Train loss : 0.0004519968933891505\n",
            "Train loss : 0.0003554757568053901\n",
            "Train loss : 0.000243768299696967\n",
            "Train loss : 0.00019161764066666365\n",
            "Train loss : 0.0005072957719676197\n",
            "Train loss : 0.0002761113573797047\n",
            "Train loss : 0.00032178047695197165\n",
            "Train loss : 0.00019980085198767483\n",
            "Train loss : 0.0001577469811309129\n",
            "Train loss : 0.0001581958931637928\n",
            "Train loss : 0.0002723355428315699\n",
            "Train loss : 0.0003118548484053463\n",
            "Train loss : 0.0003601052740123123\n",
            "Train loss : 0.00011468303273431957\n",
            "Train loss : 0.00016348971985280514\n",
            "Train loss : 0.00032811996061354876\n",
            "Train loss : 0.0004860847839154303\n",
            "Train loss : 0.0001335472334176302\n",
            "Train loss : 0.00021658405603375286\n",
            "Train loss : 0.0003444761096034199\n",
            "Train loss : 0.0002855717611964792\n",
            "Train loss : 0.0002747128310147673\n",
            "Train loss : 0.00020667785429395735\n",
            "Train loss : 0.00034577830228954554\n",
            "Train loss : 0.00022250354231800884\n",
            "Train loss : 0.00044293227256275713\n",
            "Train loss : 0.00026889852597378194\n",
            "Train loss : 0.00029914145125076175\n",
            "Train loss : 0.00012830182095058262\n",
            "Train loss : 0.0004245432501193136\n",
            "Train loss : 0.000529102748259902\n",
            "Train loss : 0.00022851148969493806\n",
            "Train loss : 0.0003129211545456201\n",
            "Train loss : 0.0003890510997734964\n",
            "Train loss : 0.00046537898015230894\n",
            "Train loss : 0.00043180465581826866\n",
            "Train loss : 0.00015770595928188413\n",
            "Train loss : 0.00022268921020440757\n",
            "Train loss : 0.0001999089290620759\n",
            "Train loss : 0.0002614294644445181\n",
            "Train loss : 0.00018648008699528873\n",
            "Train loss : 0.00034404531470499933\n",
            "Train loss : 0.00017436056805308908\n",
            "Train loss : 0.0003469607327133417\n",
            "Train loss : 0.0005712730926461518\n",
            "Train loss : 0.00044772890396416187\n",
            "Train loss : 0.0003772633499465883\n",
            "Train loss : 0.00020400408538989723\n",
            "Train loss : 0.00024098771973513067\n",
            "Train loss : 0.0004625653091352433\n",
            "Train loss : 0.0002412064786767587\n",
            "Train loss : 0.00037745796726085246\n",
            "Train loss : 0.0002817401837091893\n",
            "Train loss : 0.00030563081963919103\n",
            "Train loss : 0.0003979843750130385\n",
            "Train loss : 0.0003171856515109539\n",
            "Train loss : 0.0002914962824434042\n",
            "Train loss : 0.0006021360750310123\n",
            "Train loss : 0.00028458848828449845\n",
            "Train loss : 0.00031754738301970065\n",
            "Train loss : 0.00020324224897194654\n",
            "Train loss : 0.00016635979409329593\n",
            "Train loss : 0.00018107100913766772\n",
            "Train loss : 0.00024654189473949373\n",
            "Train loss : 0.0002695001312531531\n",
            "Train loss : 0.0002810383739415556\n",
            "Train loss : 0.00020040002709720284\n",
            "Train loss : 0.00020584107551258057\n",
            "Train loss : 0.00011411410378059372\n",
            "Train loss : 0.0001448331167921424\n",
            "Train loss : 0.00032099138479679823\n",
            "Train loss : 0.00016414547280874103\n",
            "Train loss : 0.00012831241474486887\n",
            "Train loss : 0.00033125156187452376\n",
            "Train loss : 0.00032579319668002427\n",
            "Train loss : 0.00011837805504910648\n",
            "Train loss : 0.0002814804611261934\n",
            "Train loss : 0.0004798417794518173\n",
            "Train loss : 0.00022574610193260014\n",
            "Train loss : 0.0002994363894686103\n",
            "Train loss : 0.0002670889371074736\n",
            "Train loss : 0.0005043483106419444\n",
            "Train loss : 0.00022402296599466354\n",
            "Train loss : 0.00019469250401016325\n",
            "Train loss : 0.00030593041446991265\n",
            "Train loss : 0.0004113644827157259\n",
            "Train loss : 0.0002147631166735664\n",
            "Train loss : 0.000592762662563473\n",
            "Train loss : 0.0004527284763753414\n",
            "Train loss : 0.0003145355440210551\n",
            "Train loss : 0.00031556139583699405\n",
            "Train loss : 0.00026760975015349686\n",
            "Train loss : 0.0001530024310341105\n",
            "Train loss : 0.00025036721490323544\n",
            "Train loss : 0.00026932303444482386\n",
            "Train loss : 0.00018628797261044383\n",
            "Train loss : 0.0002713367866817862\n",
            "Train loss : 0.00016389315715059638\n",
            "Train loss : 0.0003551364643499255\n",
            "Train loss : 0.00024099924485199153\n",
            "Train loss : 0.00033957994310185313\n",
            "Train loss : 0.0004694290109910071\n",
            "Train loss : 0.00029341602930799127\n",
            "Train loss : 0.00026736612198874354\n",
            "Train loss : 0.00037081382470205426\n",
            "Train loss : 0.0002981048892252147\n",
            "Train loss : 0.00024364469572901726\n",
            "Train loss : 0.00032182273571379483\n",
            "Train loss : 0.0002635983400978148\n",
            "Train loss : 0.00019483455980662256\n",
            "Train loss : 8.06550306151621e-05\n",
            "Train loss : 0.0001575087517267093\n",
            "Train loss : 0.000386251718737185\n",
            "Train loss : 0.00016609695740044117\n",
            "Train loss : 0.0002774928288999945\n",
            "Train loss : 0.00017179943097289652\n",
            "Train loss : 0.00015829999756533653\n",
            "Train loss : 0.00029483367688953876\n",
            "Train loss : 0.00024229948758147657\n",
            "Train loss : 0.00025916946469806135\n",
            "Train loss : 0.00029732007533311844\n",
            "Train loss : 0.0004678431723732501\n",
            "Train loss : 0.00017839531938079745\n",
            "Train loss : 0.00037478175363503397\n",
            "Train loss : 0.0002949756453745067\n",
            "Train loss : 0.00026132853236049414\n",
            "Train loss : 0.0003115785657428205\n",
            "Train loss : 0.00019416245049796999\n",
            "Train loss : 0.00034115422749891877\n",
            "Train loss : 0.0002859637897927314\n",
            "Train loss : 0.0002344618842471391\n",
            "Train loss : 0.00023428030544891953\n",
            "Train loss : 0.000179016831680201\n",
            "Train loss : 0.00017662713071331382\n",
            "Train loss : 0.00011651714885374531\n",
            "Train loss : 0.0003071652608923614\n",
            "Train loss : 0.0002559436543378979\n",
            "Train loss : 0.00021958842989988625\n",
            "Train loss : 0.00015537012950517237\n",
            "Train loss : 0.0002497591485735029\n",
            "Train loss : 0.00012862008588854223\n",
            "Train loss : 0.0003825776802841574\n",
            "Train loss : 0.0002658439625520259\n",
            "Train loss : 0.0001922968222061172\n",
            "Train loss : 0.0003194028395228088\n",
            "Train loss : 0.00022899876057635993\n",
            "Train loss : 0.0003812601207755506\n",
            "Train loss : 0.00014370771532412618\n",
            "Train loss : 0.00031377546838484704\n",
            "Train loss : 0.00032741567702032626\n",
            "Train loss : 0.00014787190593779087\n",
            "Train loss : 0.00014720496255904436\n",
            "Train loss : 0.000412039109505713\n",
            "Train loss : 0.00018537892901804298\n",
            "Train loss : 0.00048214022535830736\n",
            "Train loss : 0.00015582508058287203\n",
            "Train loss : 0.00021316533093340695\n",
            "Train loss : 0.00034237667568959296\n",
            "Train loss : 6.864075839985162e-05\n",
            "Train loss : 0.00012131254334235564\n",
            "Train loss : 0.0002726188686210662\n",
            "Train loss : 0.00020791725546587259\n",
            "Train loss : 0.00011657683353405446\n",
            "Looked at 51200/60000 samples.\n",
            "Train loss : 0.0002881422988139093\n",
            "Train loss : 0.00015123607590794563\n",
            "Train loss : 0.00023033801699057221\n",
            "Train loss : 0.00024542486062273383\n",
            "Train loss : 0.0005358107737265527\n",
            "Train loss : 0.000275621801847592\n",
            "Train loss : 0.00031570441205985844\n",
            "Train loss : 0.00021077139535918832\n",
            "Train loss : 0.0002467007434461266\n",
            "Train loss : 0.000194249878404662\n",
            "Train loss : 0.00017006357666105032\n",
            "Train loss : 0.0003608639817684889\n",
            "Train loss : 0.00048621208406984806\n",
            "Train loss : 0.0001668484474066645\n",
            "Train loss : 0.0002081295824609697\n",
            "Train loss : 0.00024466364993713796\n",
            "Train loss : 0.00027760895318351686\n",
            "Train loss : 0.0002960522542707622\n",
            "Train loss : 0.00021989026572555304\n",
            "Train loss : 0.0001459091145079583\n",
            "Train loss : 0.000236194406170398\n",
            "Train loss : 0.0005040345713496208\n",
            "Train loss : 0.00034548231633380055\n",
            "Train loss : 0.00029825576348230243\n",
            "Train loss : 0.00042407875298522413\n",
            "Train loss : 0.00011963322322117165\n",
            "Train loss : 0.00031360340653918684\n",
            "Train loss : 0.0003536772564984858\n",
            "Train loss : 0.0003344471042510122\n",
            "Train loss : 0.0003177015751134604\n",
            "Train loss : 0.00030489658820442855\n",
            "Train loss : 0.0002626828500069678\n",
            "Train loss : 0.00034318550024181604\n",
            "Train loss : 0.00042548502096906304\n",
            "Train loss : 0.00029897410422563553\n",
            "Train loss : 0.00018010340863838792\n",
            "Train loss : 0.0002611182280816138\n",
            "Train loss : 0.00023519861861132085\n",
            "Train loss : 0.0001569567248225212\n",
            "Train loss : 0.00020153398509137332\n",
            "Train loss : 0.00033986844937317073\n",
            "Train loss : 0.0003012603847309947\n",
            "Train loss : 0.00021552652469836175\n",
            "Train loss : 0.0003577075549401343\n",
            "Train loss : 0.0002962500730063766\n",
            "Train loss : 0.0002871483738999814\n",
            "Train loss : 0.0002454697387292981\n",
            "Train loss : 0.0001872247230494395\n",
            "Train loss : 0.00038097077049314976\n",
            "Train loss : 0.00014602892042603344\n",
            "Train loss : 7.503022789023817e-05\n",
            "Train loss : 0.0001864917139755562\n",
            "Train loss : 0.0002746436221059412\n",
            "Train loss : 0.000173948283190839\n",
            "Train loss : 0.0002173544344259426\n",
            "Train loss : 0.0001562072866363451\n",
            "Train loss : 0.00012945177149958909\n",
            "Train loss : 0.00018747102876659483\n",
            "Train loss : 0.0002609457587823272\n",
            "Train loss : 0.00032507514697499573\n",
            "Train loss : 0.00021111384558025748\n",
            "Train loss : 0.00044518712093122303\n",
            "Train loss : 0.00028511110576801\n",
            "Train loss : 0.00024034625675994903\n",
            "Train loss : 0.000289237272227183\n",
            "Train loss : 0.0002490219776518643\n",
            "Train loss : 0.00023171582142822444\n",
            "Train loss : 0.00035120133543387055\n",
            "Train loss : 7.095502951415256e-05\n",
            "Train loss : 0.0002721957571338862\n",
            "Train loss : 9.473463433096185e-05\n",
            "Train loss : 7.796972204232588e-05\n",
            "Train loss : 0.000245580798946321\n",
            "Train loss : 0.0002868494775611907\n",
            "Train loss : 9.460032742936164e-05\n",
            "Train loss : 0.00018099028966389596\n",
            "Train loss : 0.00020250314264558256\n",
            "Train loss : 0.0003408177581150085\n",
            "Train loss : 0.00038623629370704293\n",
            "Train loss : 0.0003750742762349546\n",
            "Train loss : 0.0003378553665243089\n",
            "Train loss : 0.0001715364633128047\n",
            "Train loss : 0.0001626667653908953\n",
            "Train loss : 0.00027353549376130104\n",
            "Train loss : 0.00030087720369920135\n",
            "Train loss : 0.0003012715606018901\n",
            "Train loss : 0.0003206921392120421\n",
            "Train loss : 0.00011559460108401254\n",
            "Train loss : 0.00019106976105831563\n",
            "Train loss : 0.00039791263407096267\n",
            "Train loss : 0.0003019389696419239\n",
            "Train loss : 0.00021944228501524776\n",
            "Train loss : 0.0005090649356134236\n",
            "Train loss : 0.0003881107841152698\n",
            "Train loss : 0.00020812613365706056\n",
            "Train loss : 0.0003238485660403967\n",
            "Train loss : 0.0003435807302594185\n",
            "Train loss : 0.0003032464883290231\n",
            "Train loss : 0.0002581680309958756\n",
            "Train loss : 0.00024225351808127016\n",
            "Train loss : 0.00012053696264047176\n",
            "Train loss : 0.00020908861188217998\n",
            "Train loss : 0.00022594956681132317\n",
            "Train loss : 0.00027729434077627957\n",
            "Train loss : 0.0002895882644224912\n",
            "Train loss : 0.00033944682218134403\n",
            "Train loss : 0.00017350648704450577\n",
            "Train loss : 0.00015421594434883446\n",
            "Train loss : 0.0003239678335376084\n",
            "Train loss : 0.0003236655902583152\n",
            "Train loss : 0.0004219989641569555\n",
            "Train loss : 0.000248808937612921\n",
            "Train loss : 0.0001847298553911969\n",
            "Train loss : 0.0001386754447594285\n",
            "Train loss : 0.00023562018759548664\n",
            "Train loss : 0.00015793157217558473\n",
            "Train loss : 0.00013493778533302248\n",
            "Train loss : 0.00021359731908887625\n",
            "Train loss : 0.00013417081208899617\n",
            "Train loss : 0.00015231900033541024\n",
            "Train loss : 0.00020989010226912796\n",
            "Train loss : 0.00014080309483688325\n",
            "Train loss : 0.00022704072762280703\n",
            "Train loss : 0.0001885225938167423\n",
            "Train loss : 8.311754208989441e-05\n",
            "Train loss : 0.0002593575918581337\n",
            "Train loss : 0.00013512861914932728\n",
            "Train loss : 0.00019228382734581828\n",
            "Train loss : 0.0002771894505713135\n",
            "Train loss : 0.00019261889974586666\n",
            "Train loss : 0.00023931042233016342\n",
            "Train loss : 0.0003815125674009323\n",
            "Train loss : 0.00021632037532981485\n",
            "Train loss : 0.00011467885633464903\n",
            "Train loss : 0.00029498085496015847\n",
            "Train loss : 0.00015167614037636667\n",
            "Train loss : 0.0002212950203102082\n",
            "Train loss : 0.0003022921155206859\n",
            "Train loss : 0.0003014287503901869\n",
            "Train loss : 0.00017569185001775622\n",
            "Train loss : 0.00029133327188901603\n",
            "Train loss : 0.00023102363047655672\n",
            "Train loss : 0.0002102949219988659\n",
            "Train loss : 0.00023636633704882115\n",
            "Train loss : 0.000304116663755849\n",
            "Train loss : 0.00019062210049014539\n",
            "Train loss : 0.00029655397520400584\n",
            "Train loss : 0.00025598579668439925\n",
            "Train loss : 0.0003172063734382391\n",
            "Train loss : 0.00034550734562799335\n",
            "Train loss : 0.0002900385297834873\n",
            "Train loss : 0.00027431565104052424\n",
            "Train loss : 0.00021014876256231219\n",
            "Train loss : 0.0003195611643604934\n",
            "Train loss : 0.0002355441974941641\n",
            "Train loss : 0.000332013878505677\n",
            "Train loss : 0.00012409561895765364\n",
            "Train loss : 0.00047133141197264194\n",
            "Train loss : 0.00026411956059746444\n",
            "Train loss : 0.00029970743344165385\n",
            "Train loss : 0.00020131035125814378\n",
            "Train loss : 0.0002459858951624483\n",
            "Train loss : 0.00021717415074817836\n",
            "Train loss : 0.00018506142077967525\n",
            "Train loss : 7.391118560917675e-05\n",
            "Train loss : 8.569745841668919e-05\n",
            "Train loss : 0.0002442663535475731\n",
            "Train loss : 0.00016475163283757865\n",
            "Train loss : 0.00021943013416603208\n",
            "Train loss : 0.00042080774437636137\n",
            "Train loss : 0.0001290242071263492\n",
            "Train loss : 0.0001972644531633705\n",
            "Train loss : 0.00019908517424482852\n",
            "Train loss : 0.00014142380678094923\n",
            "Train loss : 0.00020009682339150459\n",
            "Train loss : 0.00020753177523147315\n",
            "Train loss : 0.0002573856327217072\n",
            "Train loss : 0.00016290337953250855\n",
            "Train loss : 0.00017935567302629352\n",
            "Train loss : 0.0002845772251021117\n",
            "Train loss : 0.000300996151054278\n",
            "Train loss : 0.00015223666559904814\n",
            "Train loss : 0.0001321535964962095\n",
            "Train loss : 0.000186062854481861\n",
            "Train loss : 0.00023925877758301795\n",
            "Train loss : 0.0003891904198098928\n",
            "Train loss : 0.0001518405188107863\n",
            "Train loss : 9.989104728447273e-05\n",
            "Train loss : 0.0003149723052047193\n",
            "Train loss : 0.0002969148335978389\n",
            "Train loss : 0.00011676503345370293\n",
            "Train loss : 0.00018457419355399907\n",
            "Train loss : 0.00023079109087120742\n",
            "Train loss : 0.00020903139375150204\n",
            "Train loss : 0.00016761581355240196\n",
            "Train loss : 0.000369694025721401\n",
            "Train loss : 0.0002997956471517682\n",
            "Train loss : 0.0002535016101319343\n",
            "Train loss : 0.00015639732009731233\n",
            "Train loss : 9.544823842588812e-05\n",
            "Train loss : 0.00015118022565729916\n",
            "Train loss : 0.0003457230923231691\n",
            "Train loss : 0.00017082552949432284\n",
            "Train loss : 0.000286140653770417\n",
            "Train loss : 0.00018246450053993613\n",
            "Train loss : 0.0002919384860433638\n",
            "Train loss : 0.00020868035790044814\n",
            "Train loss : 0.00017502991249784827\n",
            "Train loss : 0.00028101252974011004\n",
            "Train loss : 0.0003160702472086996\n",
            "Train loss : 0.00031759959529154\n",
            "Train loss : 0.0003207815170753747\n",
            "Train loss : 0.00023340590996667743\n",
            "Train loss : 0.0001781142345862463\n",
            "Train loss : 0.0002706167579162866\n",
            "Train loss : 0.00016940048953983933\n",
            "Train loss : 0.00041656545363366604\n",
            "Train loss : 0.00022520427592098713\n",
            "Train loss : 0.00017462325922679156\n",
            "Train loss : 0.00031521209166385233\n",
            "Train loss : 0.0001702452718745917\n",
            "Train loss : 0.00022195512428879738\n",
            "Train loss : 0.00023327530652750283\n",
            "Train loss : 0.0002641148748807609\n",
            "Train loss : 0.0003955210850108415\n",
            "Train loss : 8.234192500822246e-05\n",
            "Train loss : 0.00013737237895838916\n",
            "Train loss : 0.0002026350557571277\n",
            "Train loss : 0.00033103619352914393\n",
            "Train loss : 0.0002756959293037653\n",
            "Train loss : 0.0001763891486916691\n",
            "Train loss : 0.00012610829435288906\n",
            "Train loss : 0.00034888292429968715\n",
            "Train loss : 0.00021555022976826876\n",
            "Train loss : 0.00038855941966176033\n",
            "Train loss : 0.00013025772932451218\n",
            "Train loss : 0.0004058033227920532\n",
            "Train loss : 0.00018895839457400143\n",
            "Train loss : 0.00011552657815627754\n",
            "Train loss : 0.00042152844252996147\n",
            "Train loss : 0.00013066185056231916\n",
            "Train loss : 0.00016150891315191984\n",
            "Train loss : 0.0001852490968303755\n",
            "Train loss : 0.00017189719073940068\n",
            "Train loss : 0.0001982133835554123\n",
            "Train loss : 0.0002789683057926595\n",
            "Train loss : 0.00028358891722746193\n",
            "Train loss : 0.0002871304168365896\n",
            "Train loss : 0.0002485346922185272\n",
            "Train loss : 0.00013887608656659722\n",
            "Train loss : 0.00027880026027560234\n",
            "Train loss : 0.00026106732548214495\n",
            "Train loss : 0.00029905117116868496\n",
            "Train loss : 0.00017052616749424487\n",
            "Train loss : 0.00039060437120497227\n",
            "Train loss : 0.00015991873806342483\n",
            "Train loss : 0.0003839839482679963\n",
            "Train loss : 0.00022345692559611052\n",
            "Train loss : 0.00025646897847764194\n",
            "Train loss : 0.0005035928916186094\n",
            "Train loss : 0.00029324422939680517\n",
            "Train loss : 0.0001527303975308314\n",
            "Train loss : 0.0002871383912861347\n",
            "Train loss : 0.000575993733946234\n",
            "Train loss : 0.0002510664635337889\n",
            "Train loss : 0.00019747621263377368\n",
            "Train loss : 0.00020349040278233588\n",
            "Train loss : 0.00019808609795290977\n",
            "Train loss : 0.00022420294408220798\n",
            "Train loss : 0.00024841431877575815\n",
            "Train loss : 0.0003116420120932162\n",
            "Train loss : 0.000281667395029217\n",
            "Train loss : 0.0001764158223522827\n",
            "Train loss : 0.0002894765348173678\n",
            "Train loss : 0.00014617714623454958\n",
            "Epoch : 1\n",
            "Test loss : 0.007319142576307058\n",
            "Test acc : 9.375\n",
            "Test loss : 0.007444972172379494\n",
            "Test acc : 18.75\n",
            "Test loss : 0.007525293156504631\n",
            "Test acc : 21.875\n",
            "Test loss : 0.00752651272341609\n",
            "Test acc : 31.25\n",
            "Test loss : 0.007272820919752121\n",
            "Test acc : 40.625\n",
            "Test loss : 0.007341706193983555\n",
            "Test acc : 56.25\n",
            "Test loss : 0.007378073874861002\n",
            "Test acc : 68.75\n",
            "Test loss : 0.00725072855129838\n",
            "Test acc : 81.25\n",
            "Test loss : 0.007652316242456436\n",
            "Test acc : 84.375\n",
            "Test loss : 0.007418801076710224\n",
            "Test acc : 96.875\n",
            "Test loss : 0.007325940299779177\n",
            "Test acc : 106.25\n",
            "Test loss : 0.007492990233004093\n",
            "Test acc : 118.75\n",
            "Test loss : 0.007134376093745232\n",
            "Test acc : 134.375\n",
            "Test loss : 0.007414682302623987\n",
            "Test acc : 153.125\n",
            "Test loss : 0.007573993876576424\n",
            "Test acc : 165.625\n",
            "Test loss : 0.007530008442699909\n",
            "Test acc : 171.875\n",
            "Test loss : 0.007586809806525707\n",
            "Test acc : 184.375\n",
            "Test loss : 0.007347517646849155\n",
            "Test acc : 193.75\n",
            "Test loss : 0.007401051931083202\n",
            "Test acc : 200.0\n",
            "Test loss : 0.007405619136989117\n",
            "Test acc : 206.25\n",
            "Test loss : 0.007250261027365923\n",
            "Test acc : 228.125\n",
            "Test loss : 0.007679341360926628\n",
            "Test acc : 243.75\n",
            "Test loss : 0.007349241059273481\n",
            "Test acc : 259.375\n",
            "Test loss : 0.007549960631877184\n",
            "Test acc : 265.625\n",
            "Test loss : 0.007378558162599802\n",
            "Test acc : 281.25\n",
            "Test loss : 0.007303732912987471\n",
            "Test acc : 306.25\n",
            "Test loss : 0.007433417718857527\n",
            "Test acc : 309.375\n",
            "Test loss : 0.007393495179712772\n",
            "Test acc : 318.75\n",
            "Test loss : 0.007453824859112501\n",
            "Test acc : 325.0\n",
            "Test loss : 0.007344197016209364\n",
            "Test acc : 343.75\n",
            "Test loss : 0.007451508194208145\n",
            "Test acc : 356.25\n",
            "Test loss : 0.007584382314234972\n",
            "Test acc : 362.5\n",
            "Test loss : 0.007258040364831686\n",
            "Test acc : 381.25\n",
            "Test loss : 0.00740512739866972\n",
            "Test acc : 393.75\n",
            "Test loss : 0.0073617915622889996\n",
            "Test acc : 403.125\n",
            "Test loss : 0.007387731224298477\n",
            "Test acc : 421.875\n",
            "Test loss : 0.00722085777670145\n",
            "Test acc : 431.25\n",
            "Test loss : 0.007301735691726208\n",
            "Test acc : 440.625\n",
            "Test loss : 0.0072183976881206036\n",
            "Test acc : 459.375\n",
            "Test loss : 0.007570626214146614\n",
            "Test acc : 471.875\n",
            "Test loss : 0.007465643808245659\n",
            "Test acc : 484.375\n",
            "Test loss : 0.00768063310533762\n",
            "Test acc : 500.0\n",
            "Test loss : 0.00729401595890522\n",
            "Test acc : 518.75\n",
            "Test loss : 0.007187616545706987\n",
            "Test acc : 540.625\n",
            "Test loss : 0.00732866395264864\n",
            "Test acc : 546.875\n",
            "Test loss : 0.007506913505494595\n",
            "Test acc : 556.25\n",
            "Test loss : 0.007279102690517902\n",
            "Test acc : 562.5\n",
            "Test loss : 0.0075258417055010796\n",
            "Test acc : 565.625\n",
            "Test loss : 0.0076474398374557495\n",
            "Test acc : 575.0\n",
            "Test loss : 0.0073445201851427555\n",
            "Test acc : 587.5\n",
            "Test loss : 0.007570585235953331\n",
            "Test acc : 593.75\n",
            "Test loss : 0.007311319932341576\n",
            "Test acc : 603.125\n",
            "Test loss : 0.0073540667071938515\n",
            "Test acc : 625.0\n",
            "Test loss : 0.007399949710816145\n",
            "Test acc : 634.375\n",
            "Test loss : 0.0072926986031234264\n",
            "Test acc : 643.75\n",
            "Test loss : 0.007382893934845924\n",
            "Test acc : 656.25\n",
            "Test loss : 0.007408333942294121\n",
            "Test acc : 668.75\n",
            "Test loss : 0.007603383157402277\n",
            "Test acc : 678.125\n",
            "Test loss : 0.007430148310959339\n",
            "Test acc : 687.5\n",
            "Test loss : 0.007443159818649292\n",
            "Test acc : 700.0\n",
            "Test loss : 0.0073358602821826935\n",
            "Test acc : 709.375\n",
            "Test loss : 0.007745567709207535\n",
            "Test acc : 718.75\n",
            "Test loss : 0.007518823258578777\n",
            "Test acc : 721.875\n",
            "Test loss : 0.0074107227846980095\n",
            "Test acc : 731.25\n",
            "Test loss : 0.007517639547586441\n",
            "Test acc : 740.625\n",
            "Test loss : 0.007293124683201313\n",
            "Test acc : 753.125\n",
            "Test loss : 0.007202120032161474\n",
            "Test acc : 775.0\n",
            "Test loss : 0.007556438446044922\n",
            "Test acc : 781.25\n",
            "Test loss : 0.007497278042137623\n",
            "Test acc : 787.5\n",
            "Test loss : 0.007474628277122974\n",
            "Test acc : 793.75\n",
            "Test loss : 0.007482025306671858\n",
            "Test acc : 806.25\n",
            "Test loss : 0.007403487805277109\n",
            "Test acc : 821.875\n",
            "Test loss : 0.007552063092589378\n",
            "Test acc : 837.5\n",
            "Test loss : 0.0076764971017837524\n",
            "Test acc : 843.75\n",
            "Test loss : 0.007469272240996361\n",
            "Test acc : 859.375\n",
            "Test loss : 0.007338934578001499\n",
            "Test acc : 881.25\n",
            "Test loss : 0.007575809024274349\n",
            "Test acc : 893.75\n",
            "Test loss : 0.007468816824257374\n",
            "Test acc : 903.125\n",
            "Test loss : 0.007593436632305384\n",
            "Test acc : 909.375\n",
            "Test loss : 0.0072798659093678\n",
            "Test acc : 925.0\n",
            "Test loss : 0.007422140333801508\n",
            "Test acc : 934.375\n",
            "Test loss : 0.007111228071153164\n",
            "Test acc : 956.25\n",
            "Test loss : 0.0074285236187279224\n",
            "Test acc : 968.75\n",
            "Test loss : 0.0073059131391346455\n",
            "Test acc : 987.5\n",
            "Test loss : 0.007396260742098093\n",
            "Test acc : 993.75\n",
            "Test loss : 0.007443184498697519\n",
            "Test acc : 1003.125\n",
            "Test loss : 0.007328942883759737\n",
            "Test acc : 1025.0\n",
            "Test loss : 0.007678337395191193\n",
            "Test acc : 1031.25\n",
            "Test loss : 0.00760155962780118\n",
            "Test acc : 1034.375\n",
            "Test loss : 0.007360846735537052\n",
            "Test acc : 1046.875\n",
            "Test loss : 0.007294501177966595\n",
            "Test acc : 1056.25\n",
            "Test loss : 0.00727534294128418\n",
            "Test acc : 1065.625\n",
            "Test loss : 0.007431583479046822\n",
            "Test acc : 1075.0\n",
            "Test loss : 0.007295174524188042\n",
            "Test acc : 1081.25\n",
            "Test loss : 0.007659889291971922\n",
            "Test acc : 1084.375\n",
            "Test loss : 0.007607427425682545\n",
            "Test acc : 1087.5\n",
            "Test loss : 0.007395966909825802\n",
            "Test acc : 1103.125\n",
            "Test loss : 0.007439987268298864\n",
            "Test acc : 1115.625\n",
            "Test loss : 0.007478254847228527\n",
            "Test acc : 1115.625\n",
            "Test loss : 0.0073531558737158775\n",
            "Test acc : 1125.0\n",
            "Test loss : 0.007409143727272749\n",
            "Test acc : 1131.25\n",
            "Test loss : 0.007379772607237101\n",
            "Test acc : 1143.75\n",
            "Test loss : 0.007288216147571802\n",
            "Test acc : 1156.25\n",
            "Test loss : 0.00767391175031662\n",
            "Test acc : 1168.75\n",
            "Test loss : 0.0076570212841033936\n",
            "Test acc : 1171.875\n",
            "Test loss : 0.007460782304406166\n",
            "Test acc : 1181.25\n",
            "Test loss : 0.007468746043741703\n",
            "Test acc : 1187.5\n",
            "Test loss : 0.007226433604955673\n",
            "Test acc : 1203.125\n",
            "Test loss : 0.007471099961549044\n",
            "Test acc : 1218.75\n",
            "Test loss : 0.0074538118205964565\n",
            "Test acc : 1225.0\n",
            "Test loss : 0.007421909365803003\n",
            "Test acc : 1231.25\n",
            "Test loss : 0.007408453617244959\n",
            "Test acc : 1231.25\n",
            "Test loss : 0.007501377258449793\n",
            "Test acc : 1240.625\n",
            "Test loss : 0.007332938257604837\n",
            "Test acc : 1259.375\n",
            "Test loss : 0.007483900524675846\n",
            "Test acc : 1265.625\n",
            "Test loss : 0.007448122370988131\n",
            "Test acc : 1275.0\n",
            "Test loss : 0.007369488477706909\n",
            "Test acc : 1284.375\n",
            "Test loss : 0.007587250787764788\n",
            "Test acc : 1290.625\n",
            "Test loss : 0.007568845991045237\n",
            "Test acc : 1296.875\n",
            "Test loss : 0.007534482050687075\n",
            "Test acc : 1306.25\n",
            "Test loss : 0.007430430967360735\n",
            "Test acc : 1312.5\n",
            "Test loss : 0.007313127163797617\n",
            "Test acc : 1315.625\n",
            "Test loss : 0.007682661060243845\n",
            "Test acc : 1315.625\n",
            "Test loss : 0.007431915961205959\n",
            "Test acc : 1328.125\n",
            "Test loss : 0.007360937539488077\n",
            "Test acc : 1337.5\n",
            "Test loss : 0.007504965178668499\n",
            "Test acc : 1353.125\n",
            "Test loss : 0.0073564243502914906\n",
            "Test acc : 1362.5\n",
            "Test loss : 0.007411884609609842\n",
            "Test acc : 1375.0\n",
            "Test loss : 0.007409369107335806\n",
            "Test acc : 1384.375\n",
            "Test loss : 0.007433450315147638\n",
            "Test acc : 1393.75\n",
            "Test loss : 0.007371791172772646\n",
            "Test acc : 1406.25\n",
            "Test loss : 0.007577334064990282\n",
            "Test acc : 1412.5\n",
            "Test loss : 0.007381855044513941\n",
            "Test acc : 1418.75\n",
            "Test loss : 0.007323172874748707\n",
            "Test acc : 1440.625\n",
            "Test loss : 0.007677766028791666\n",
            "Test acc : 1453.125\n",
            "Test loss : 0.007379299495369196\n",
            "Test acc : 1462.5\n",
            "Test loss : 0.007427283562719822\n",
            "Test acc : 1471.875\n",
            "Test loss : 0.007467766758054495\n",
            "Test acc : 1484.375\n",
            "Test loss : 0.00746166380122304\n",
            "Test acc : 1509.375\n",
            "Test loss : 0.007604554761201143\n",
            "Test acc : 1515.625\n",
            "Test loss : 0.007212955504655838\n",
            "Test acc : 1540.625\n",
            "Test loss : 0.007377536967396736\n",
            "Test acc : 1553.125\n",
            "Test loss : 0.007429931778460741\n",
            "Test acc : 1571.875\n",
            "Test loss : 0.007458633277565241\n",
            "Test acc : 1581.25\n",
            "Test loss : 0.007570772431790829\n",
            "Test acc : 1587.5\n",
            "Test loss : 0.007487169001251459\n",
            "Test acc : 1606.25\n",
            "Test loss : 0.007439847104251385\n",
            "Test acc : 1609.375\n",
            "Test loss : 0.007409464567899704\n",
            "Test acc : 1621.875\n",
            "Test loss : 0.0075912210159003735\n",
            "Test acc : 1637.5\n",
            "Test loss : 0.007187660317867994\n",
            "Test acc : 1656.25\n",
            "Test loss : 0.007584169041365385\n",
            "Test acc : 1659.375\n",
            "Test loss : 0.007361377589404583\n",
            "Test acc : 1671.875\n",
            "Test loss : 0.007442090660333633\n",
            "Test acc : 1678.125\n",
            "Test loss : 0.0074773491360247135\n",
            "Test acc : 1693.75\n",
            "Test loss : 0.007623401470482349\n",
            "Test acc : 1706.25\n",
            "Test loss : 0.007657334674149752\n",
            "Test acc : 1706.25\n",
            "Test loss : 0.007378706708550453\n",
            "Test acc : 1721.875\n",
            "Test loss : 0.007308389991521835\n",
            "Test acc : 1740.625\n",
            "Test loss : 0.007441795896738768\n",
            "Test acc : 1750.0\n",
            "Test loss : 0.007548256777226925\n",
            "Test acc : 1756.25\n",
            "Test loss : 0.00736412825062871\n",
            "Test acc : 1771.875\n",
            "Test loss : 0.007345712278038263\n",
            "Test acc : 1790.625\n",
            "Test loss : 0.007378934882581234\n",
            "Test acc : 1800.0\n",
            "Test loss : 0.00737244077026844\n",
            "Test acc : 1812.5\n",
            "Test loss : 0.007300748489797115\n",
            "Test acc : 1831.25\n",
            "Test loss : 0.007453511469066143\n",
            "Test acc : 1837.5\n",
            "Test loss : 0.0075087640434503555\n",
            "Test acc : 1846.875\n",
            "Test loss : 0.00740423146635294\n",
            "Test acc : 1862.5\n",
            "Test loss : 0.007605156395584345\n",
            "Test acc : 1878.125\n",
            "Test loss : 0.007486076559871435\n",
            "Test acc : 1884.375\n",
            "Test loss : 0.007305190432816744\n",
            "Test acc : 1896.875\n",
            "Test loss : 0.0070731136947870255\n",
            "Test acc : 1921.875\n",
            "Test loss : 0.007334108930081129\n",
            "Test acc : 1931.25\n",
            "Test loss : 0.007596463896334171\n",
            "Test acc : 1937.5\n",
            "Test loss : 0.007290873676538467\n",
            "Test acc : 1946.875\n",
            "Test loss : 0.007405395153909922\n",
            "Test acc : 1953.125\n",
            "Test loss : 0.007555884774774313\n",
            "Test acc : 1956.25\n",
            "Test loss : 0.00759274372830987\n",
            "Test acc : 1962.5\n",
            "Test loss : 0.007404418662190437\n",
            "Test acc : 1971.875\n",
            "Test loss : 0.007398455869406462\n",
            "Test acc : 1981.25\n",
            "Test loss : 0.007564317900687456\n",
            "Test acc : 1993.75\n",
            "Test loss : 0.007192626129835844\n",
            "Test acc : 2003.125\n",
            "Test loss : 0.007259955629706383\n",
            "Test acc : 2018.75\n",
            "Test loss : 0.007356029935181141\n",
            "Test acc : 2025.0\n",
            "Test loss : 0.007463433779776096\n",
            "Test acc : 2037.5\n",
            "Test loss : 0.007696542423218489\n",
            "Test acc : 2040.625\n",
            "Test loss : 0.007472024764865637\n",
            "Test acc : 2050.0\n",
            "Test loss : 0.007293884176760912\n",
            "Test acc : 2062.5\n",
            "Test loss : 0.0073058162815868855\n",
            "Test acc : 2068.75\n",
            "Test loss : 0.007716672495007515\n",
            "Test acc : 2068.75\n",
            "Test loss : 0.007481725886464119\n",
            "Test acc : 2071.875\n",
            "Test loss : 0.007268757093697786\n",
            "Test acc : 2081.25\n",
            "Test loss : 0.007322973106056452\n",
            "Test acc : 2100.0\n",
            "Test loss : 0.00767617579549551\n",
            "Test acc : 2103.125\n",
            "Test loss : 0.007425539195537567\n",
            "Test acc : 2121.875\n",
            "Test loss : 0.007242697756737471\n",
            "Test acc : 2143.75\n",
            "Test loss : 0.0073485723696649075\n",
            "Test acc : 2153.125\n",
            "Test loss : 0.00760929798707366\n",
            "Test acc : 2162.5\n",
            "Test loss : 0.007460783235728741\n",
            "Test acc : 2165.625\n",
            "Test loss : 0.007396992761641741\n",
            "Test acc : 2175.0\n",
            "Test loss : 0.007303369697183371\n",
            "Test acc : 2193.75\n",
            "Test loss : 0.007398608606308699\n",
            "Test acc : 2209.375\n",
            "Test loss : 0.007425854913890362\n",
            "Test acc : 2221.875\n",
            "Test loss : 0.0072680250741541386\n",
            "Test acc : 2228.125\n",
            "Test loss : 0.007413102313876152\n",
            "Test acc : 2243.75\n",
            "Test loss : 0.007387197110801935\n",
            "Test acc : 2250.0\n",
            "Test loss : 0.007423937786370516\n",
            "Test acc : 2265.625\n",
            "Test loss : 0.0077552879229187965\n",
            "Test acc : 2265.625\n",
            "Test loss : 0.007347553968429565\n",
            "Test acc : 2275.0\n",
            "Test loss : 0.007570980582386255\n",
            "Test acc : 2281.25\n",
            "Test loss : 0.007584987208247185\n",
            "Test acc : 2293.75\n",
            "Test loss : 0.007475751452147961\n",
            "Test acc : 2293.75\n",
            "Test loss : 0.007414642721414566\n",
            "Test acc : 2309.375\n",
            "Test loss : 0.007492566481232643\n",
            "Test acc : 2321.875\n",
            "Test loss : 0.007329570595175028\n",
            "Test acc : 2331.25\n",
            "Test loss : 0.007706950418651104\n",
            "Test acc : 2334.375\n",
            "Test loss : 0.007547914981842041\n",
            "Test acc : 2337.5\n",
            "Test loss : 0.007266555912792683\n",
            "Test acc : 2353.125\n",
            "Test loss : 0.007273718249052763\n",
            "Test acc : 2368.75\n",
            "Test loss : 0.007376234978437424\n",
            "Test acc : 2371.875\n",
            "Test loss : 0.007455111481249332\n",
            "Test acc : 2390.625\n",
            "Test loss : 0.007505044341087341\n",
            "Test acc : 2403.125\n",
            "Test loss : 0.0073562972247600555\n",
            "Test acc : 2409.375\n",
            "Test loss : 0.00753746647387743\n",
            "Test acc : 2415.625\n",
            "Test loss : 0.007373741362243891\n",
            "Test acc : 2421.875\n",
            "Test loss : 0.007330749183893204\n",
            "Test acc : 2428.125\n",
            "Test loss : 0.007403216790407896\n",
            "Test acc : 2434.375\n",
            "Test loss : 0.00769714405760169\n",
            "Test acc : 2440.625\n",
            "Test loss : 0.007133524399250746\n",
            "Test acc : 2468.75\n",
            "Test loss : 0.0076151625253260136\n",
            "Test acc : 2471.875\n",
            "Test loss : 0.007498652674257755\n",
            "Test acc : 2493.75\n",
            "Test loss : 0.007614111993461847\n",
            "Test acc : 2503.125\n",
            "Test loss : 0.007328998763114214\n",
            "Test acc : 2525.0\n",
            "Test loss : 0.007596056442707777\n",
            "Test acc : 2534.375\n",
            "Test loss : 0.007379553746432066\n",
            "Test acc : 2546.875\n",
            "Test loss : 0.007552156690508127\n",
            "Test acc : 2562.5\n",
            "Test loss : 0.007605183403939009\n",
            "Test acc : 2568.75\n",
            "Test loss : 0.007395651191473007\n",
            "Test acc : 2584.375\n",
            "Test loss : 0.007653020787984133\n",
            "Test acc : 2587.5\n",
            "Test loss : 0.007485178764909506\n",
            "Test acc : 2600.0\n",
            "Test loss : 0.007254099939018488\n",
            "Test acc : 2612.5\n",
            "Test loss : 0.0074019357562065125\n",
            "Test acc : 2628.125\n",
            "Test loss : 0.007353448309004307\n",
            "Test acc : 2631.25\n",
            "Test loss : 0.0073029217310249805\n",
            "Test acc : 2637.5\n",
            "Test loss : 0.007629196625202894\n",
            "Test acc : 2643.75\n",
            "Test loss : 0.007404195610433817\n",
            "Test acc : 2653.125\n",
            "Test loss : 0.007417604792863131\n",
            "Test acc : 2656.25\n",
            "Test loss : 0.007333540823310614\n",
            "Test acc : 2671.875\n",
            "Test loss : 0.007200109306722879\n",
            "Test acc : 2690.625\n",
            "Test loss : 0.007445324677973986\n",
            "Test acc : 2703.125\n",
            "Test loss : 0.00740595068782568\n",
            "Test acc : 2703.125\n",
            "Test loss : 0.00750848138704896\n",
            "Test acc : 2715.625\n",
            "Test loss : 0.007655723951756954\n",
            "Test acc : 2731.25\n",
            "Test loss : 0.007355198729783297\n",
            "Test acc : 2750.0\n",
            "Test loss : 0.007401267532259226\n",
            "Test acc : 2756.25\n",
            "Test loss : 0.007376599125564098\n",
            "Test acc : 2768.75\n",
            "Test loss : 0.007371092680841684\n",
            "Test acc : 2771.875\n",
            "Test loss : 0.007235590368509293\n",
            "Test acc : 2787.5\n",
            "Test loss : 0.007346493192017078\n",
            "Test acc : 2793.75\n",
            "Test loss : 0.007303704973310232\n",
            "Test acc : 2812.5\n",
            "Test loss : 0.007533635012805462\n",
            "Test acc : 2821.875\n",
            "Test loss : 0.007346208207309246\n",
            "Test acc : 2828.125\n",
            "Test loss : 0.00750221312046051\n",
            "Test acc : 2843.75\n",
            "Test loss : 0.007482846267521381\n",
            "Test acc : 2853.125\n",
            "Test loss : 0.00754398200660944\n",
            "Test acc : 2865.625\n",
            "Test loss : 0.007312828209251165\n",
            "Test acc : 2881.25\n",
            "Test loss : 0.007089817430824041\n",
            "Test acc : 2896.875\n",
            "Test loss : 0.0075436849147081375\n",
            "Test acc : 2903.125\n",
            "Test loss : 0.007553300354629755\n",
            "Test acc : 2915.625\n",
            "Test loss : 0.007335198111832142\n",
            "Test acc : 2925.0\n",
            "Test loss : 0.0073556494899094105\n",
            "Test acc : 2931.25\n",
            "Test loss : 0.007415637373924255\n",
            "Test acc : 2946.875\n",
            "Test loss : 0.007426118478178978\n",
            "Test acc : 2956.25\n",
            "Test loss : 0.007392565719783306\n",
            "Test acc : 2962.5\n",
            "Test loss : 0.007405739743262529\n",
            "Test acc : 2965.625\n",
            "Test loss : 0.007268758025020361\n",
            "Test acc : 2984.375\n",
            "Test loss : 0.007440072484314442\n",
            "Test acc : 2993.75\n",
            "Test loss : 0.00738753005862236\n",
            "Test acc : 3006.25\n",
            "Test loss : 0.007516409270465374\n",
            "Test acc : 3018.75\n",
            "Test loss : 0.007345364894717932\n",
            "Test acc : 3037.5\n",
            "Test loss : 0.007532777264714241\n",
            "Test acc : 3046.875\n",
            "Test loss : 0.0075017306953668594\n",
            "Test acc : 3056.25\n",
            "Test loss : 0.007463560439646244\n",
            "Test acc : 3065.625\n",
            "Test loss : 0.007408566307276487\n",
            "Test acc : 3081.25\n",
            "Test loss : 0.007445263676345348\n",
            "Test acc : 3087.5\n",
            "Test loss : 0.007303743623197079\n",
            "Test acc : 3103.125\n",
            "Test loss : 0.007554533425718546\n",
            "Test acc : 3112.5\n",
            "Test loss : 0.007467860821634531\n",
            "Test acc : 3128.125\n",
            "Test loss : 0.007638915441930294\n",
            "Test acc : 3140.625\n",
            "Test loss : 0.0074743772856891155\n",
            "Test acc : 3146.875\n",
            "Test loss : 0.007421574089676142\n",
            "Test acc : 3165.625\n",
            "Test loss : 0.00755599420517683\n",
            "Test acc : 3175.0\n",
            "Test loss : 0.007253597490489483\n",
            "Test acc : 3200.0\n",
            "Test loss : 0.007556100841611624\n",
            "Test acc : 3206.25\n",
            "Test loss : 0.0074975742027163506\n",
            "Test acc : 3212.5\n",
            "Test loss : 0.007597215007990599\n",
            "Test acc : 3212.5\n",
            "Test loss : 0.00733166141435504\n",
            "Test acc : 3228.125\n",
            "Test loss : 0.007459175772964954\n",
            "Test acc : 3234.375\n",
            "Test loss : 0.007412469480186701\n",
            "Test acc : 3243.75\n",
            "Test loss : 0.007471026852726936\n",
            "Test acc : 3253.125\n",
            "Test loss : 0.007338140159845352\n",
            "Test acc : 3262.5\n",
            "Test loss : 0.007272417191416025\n",
            "Test acc : 3271.875\n",
            "Test loss : 0.007556391414254904\n",
            "Test acc : 3281.25\n",
            "Test loss : 0.0072556608356535435\n",
            "Test acc : 3300.0\n",
            "Test loss : 0.007453979458659887\n",
            "Test acc : 3318.75\n",
            "Test loss : 0.007320080883800983\n",
            "Test acc : 3331.25\n",
            "Test loss : 0.007676729932427406\n",
            "Test acc : 3337.5\n",
            "Test loss : 0.007338725496083498\n",
            "Test acc : 3343.75\n",
            "Test loss : 0.007488212548196316\n",
            "Test acc : 3353.125\n",
            "Test loss : 0.0073072942905128\n",
            "Test acc : 3371.875\n",
            "Test loss : 0.007679843343794346\n",
            "Test acc : 3378.125\n",
            "Test loss : 0.00743069825693965\n",
            "Test acc : 3390.625\n",
            "Test loss : 0.007558837067335844\n",
            "Test acc : 3396.875\n",
            "Looked at 0/60000 samples.\n",
            "Train loss : 0.5171526670455933\n",
            "Train loss : 0.44079330563545227\n",
            "Train loss : 0.2266443371772766\n",
            "Train loss : 0.18227910995483398\n",
            "Train loss : 0.1124018207192421\n",
            "Train loss : 0.07897987216711044\n",
            "Train loss : 0.09639419615268707\n",
            "Train loss : 0.05036505311727524\n",
            "Train loss : 0.0472988560795784\n",
            "Train loss : 0.04976457357406616\n",
            "Train loss : 0.05036475881934166\n",
            "Train loss : 0.073782779276371\n",
            "Train loss : 0.019569693133234978\n",
            "Train loss : 0.03829539939761162\n",
            "Train loss : 0.02598167210817337\n",
            "Train loss : 0.0322311669588089\n",
            "Train loss : 0.012912844307720661\n",
            "Train loss : 0.019534213468432426\n",
            "Train loss : 0.0101728904992342\n",
            "Train loss : 0.039442770183086395\n",
            "Train loss : 0.027191363275051117\n",
            "Train loss : 0.01565181463956833\n",
            "Train loss : 0.027575233951210976\n",
            "Train loss : 0.014538577757775784\n",
            "Train loss : 0.0179398525506258\n",
            "Train loss : 0.021464847028255463\n",
            "Train loss : 0.011678873561322689\n",
            "Train loss : 0.013037079013884068\n",
            "Train loss : 0.018124395981431007\n",
            "Train loss : 0.01823604106903076\n",
            "Train loss : 0.015033971518278122\n",
            "Train loss : 0.010537895374000072\n",
            "Train loss : 0.011463381350040436\n",
            "Train loss : 0.014354661107063293\n",
            "Train loss : 0.01841014251112938\n",
            "Train loss : 0.010682360269129276\n",
            "Train loss : 0.013495681807398796\n",
            "Train loss : 0.014431236311793327\n",
            "Train loss : 0.009201576933264732\n",
            "Train loss : 0.023404881358146667\n",
            "Train loss : 0.006948459427803755\n",
            "Train loss : 0.007064358331263065\n",
            "Train loss : 0.01205974817276001\n",
            "Train loss : 0.00949039589613676\n",
            "Train loss : 0.015021038241684437\n",
            "Train loss : 0.007033388130366802\n",
            "Train loss : 0.011294648051261902\n",
            "Train loss : 0.005581410136073828\n",
            "Train loss : 0.009455644525587559\n",
            "Train loss : 0.005375118926167488\n",
            "Train loss : 0.009406084194779396\n",
            "Train loss : 0.016086982563138008\n",
            "Train loss : 0.0068860831670463085\n",
            "Train loss : 0.009242132306098938\n",
            "Train loss : 0.006176746916025877\n",
            "Train loss : 0.00773165887221694\n",
            "Train loss : 0.008183599449694157\n",
            "Train loss : 0.0088210329413414\n",
            "Train loss : 0.013046422973275185\n",
            "Train loss : 0.007729159202426672\n",
            "Train loss : 0.004779827781021595\n",
            "Train loss : 0.004685571882873774\n",
            "Train loss : 0.010690929368138313\n",
            "Train loss : 0.003988558892160654\n",
            "Train loss : 0.007948891259729862\n",
            "Train loss : 0.007823034189641476\n",
            "Train loss : 0.0068459827452898026\n",
            "Train loss : 0.007254343014210463\n",
            "Train loss : 0.005775298923254013\n",
            "Train loss : 0.0043668425641953945\n",
            "Train loss : 0.003912195563316345\n",
            "Train loss : 0.0024298573844134808\n",
            "Train loss : 0.005252411589026451\n",
            "Train loss : 0.004462997894734144\n",
            "Train loss : 0.005573092959821224\n",
            "Train loss : 0.003505082568153739\n",
            "Train loss : 0.0028140107169747353\n",
            "Train loss : 0.00574103556573391\n",
            "Train loss : 0.007305874023586512\n",
            "Train loss : 0.005847921594977379\n",
            "Train loss : 0.003728229785338044\n",
            "Train loss : 0.002978183561936021\n",
            "Train loss : 0.003952250815927982\n",
            "Train loss : 0.0018934932304546237\n",
            "Train loss : 0.0028278559911996126\n",
            "Train loss : 0.007170727010816336\n",
            "Train loss : 0.004239080939441919\n",
            "Train loss : 0.00507398322224617\n",
            "Train loss : 0.002005727030336857\n",
            "Train loss : 0.005009602755308151\n",
            "Train loss : 0.0017787065589800477\n",
            "Train loss : 0.0020670986268669367\n",
            "Train loss : 0.005026556551456451\n",
            "Train loss : 0.004438919015228748\n",
            "Train loss : 0.007203422021120787\n",
            "Train loss : 0.006839785724878311\n",
            "Train loss : 0.003915767185389996\n",
            "Train loss : 0.005200852639973164\n",
            "Train loss : 0.0029080708045512438\n",
            "Train loss : 0.004952755756676197\n",
            "Train loss : 0.005497816018760204\n",
            "Train loss : 0.006558239925652742\n",
            "Train loss : 0.0043820226565003395\n",
            "Train loss : 0.004073196090757847\n",
            "Train loss : 0.004266155883669853\n",
            "Train loss : 0.006441126111894846\n",
            "Train loss : 0.005913341883569956\n",
            "Train loss : 0.003917309921234846\n",
            "Train loss : 0.0026756050065159798\n",
            "Train loss : 0.002082968130707741\n",
            "Train loss : 0.0033756422344595194\n",
            "Train loss : 0.003156881546601653\n",
            "Train loss : 0.0018974753329530358\n",
            "Train loss : 0.004153636284172535\n",
            "Train loss : 0.001427477109245956\n",
            "Train loss : 0.0019185504643246531\n",
            "Train loss : 0.0030220127664506435\n",
            "Train loss : 0.00483796326443553\n",
            "Train loss : 0.005100660026073456\n",
            "Train loss : 0.004510681144893169\n",
            "Train loss : 0.003506117034703493\n",
            "Train loss : 0.0034810344222933054\n",
            "Train loss : 0.0015405204612761736\n",
            "Train loss : 0.003996954299509525\n",
            "Train loss : 0.004175292793661356\n",
            "Train loss : 0.0028370569925755262\n",
            "Train loss : 0.00201665167696774\n",
            "Train loss : 0.0028116710018366575\n",
            "Train loss : 0.0036944577004760504\n",
            "Train loss : 0.005372398067265749\n",
            "Train loss : 0.0032854434102773666\n",
            "Train loss : 0.0045111412182450294\n",
            "Train loss : 0.002982416655868292\n",
            "Train loss : 0.0018909958889707923\n",
            "Train loss : 0.006809450685977936\n",
            "Train loss : 0.0013820998137816787\n",
            "Train loss : 0.003999675158411264\n",
            "Train loss : 0.0028284715954214334\n",
            "Train loss : 0.0015436969697475433\n",
            "Train loss : 0.003753470489755273\n",
            "Train loss : 0.0032479758374392986\n",
            "Train loss : 0.0011849325383082032\n",
            "Train loss : 0.0016570956213399768\n",
            "Train loss : 0.0033074195962399244\n",
            "Train loss : 0.0028778754640370607\n",
            "Train loss : 0.0015965872444212437\n",
            "Train loss : 0.004086925648152828\n",
            "Train loss : 0.002377029974013567\n",
            "Train loss : 0.0025273000355809927\n",
            "Train loss : 0.0015343434643000364\n",
            "Train loss : 0.001803109422326088\n",
            "Train loss : 0.003133053658530116\n",
            "Train loss : 0.00307834823615849\n",
            "Train loss : 0.002643726533278823\n",
            "Train loss : 0.0025581514928489923\n",
            "Train loss : 0.0030902321450412273\n",
            "Train loss : 0.0025238252710551023\n",
            "Train loss : 0.0027374126948416233\n",
            "Train loss : 0.0023383325897157192\n",
            "Train loss : 0.002942079911008477\n",
            "Train loss : 0.002538884524255991\n",
            "Train loss : 0.0019168155267834663\n",
            "Train loss : 0.0026223373133689165\n",
            "Train loss : 0.0031913809943944216\n",
            "Train loss : 0.0028460181783884764\n",
            "Train loss : 0.0014745705993846059\n",
            "Train loss : 0.004068650770932436\n",
            "Train loss : 0.002504049800336361\n",
            "Train loss : 0.0019953609444200993\n",
            "Train loss : 0.001775204436853528\n",
            "Train loss : 0.004863174166530371\n",
            "Train loss : 0.0014617525739595294\n",
            "Train loss : 0.002312547294422984\n",
            "Train loss : 0.0023159997072070837\n",
            "Train loss : 0.001956482185050845\n",
            "Train loss : 0.0016588788712397218\n",
            "Train loss : 0.002126304665580392\n",
            "Train loss : 0.0023817503824830055\n",
            "Train loss : 0.0013961781514808536\n",
            "Train loss : 0.001903718221001327\n",
            "Train loss : 0.002549258293583989\n",
            "Train loss : 0.002243571914732456\n",
            "Train loss : 0.0014355283929035068\n",
            "Train loss : 0.00286288745701313\n",
            "Train loss : 0.0021528182551264763\n",
            "Train loss : 0.0011593721574172378\n",
            "Train loss : 0.0018381195841357112\n",
            "Train loss : 0.0016780906589701772\n",
            "Train loss : 0.0033392668701708317\n",
            "Train loss : 0.0010140622034668922\n",
            "Train loss : 0.0013786709168925881\n",
            "Train loss : 0.001486765337176621\n",
            "Train loss : 0.0020639572758227587\n",
            "Train loss : 0.0024002946447581053\n",
            "Train loss : 0.0022754648234695196\n",
            "Train loss : 0.003708995645865798\n",
            "Train loss : 0.0013474071165546775\n",
            "Train loss : 0.002203937154263258\n",
            "Train loss : 0.001927106874063611\n",
            "Train loss : 0.0026529747992753983\n",
            "Train loss : 0.0034308203030377626\n",
            "Train loss : 0.0017151362262666225\n",
            "Train loss : 0.002646246924996376\n",
            "Train loss : 0.002253183862194419\n",
            "Train loss : 0.0019221993861719966\n",
            "Train loss : 0.0019369375659152865\n",
            "Train loss : 0.0017196668777614832\n",
            "Train loss : 0.0014167941408231854\n",
            "Train loss : 0.0019167199498042464\n",
            "Train loss : 0.002059540245682001\n",
            "Train loss : 0.0018049896461889148\n",
            "Train loss : 0.0017942535923793912\n",
            "Train loss : 0.002809288678690791\n",
            "Train loss : 0.0012159913312643766\n",
            "Train loss : 0.002906693145632744\n",
            "Train loss : 0.0020899733062833548\n",
            "Train loss : 0.002467162674292922\n",
            "Train loss : 0.0017028769943863153\n",
            "Train loss : 0.0010862513445317745\n",
            "Train loss : 0.001844333135522902\n",
            "Train loss : 0.001546245999634266\n",
            "Train loss : 0.0018524941988289356\n",
            "Train loss : 0.0013946390245109797\n",
            "Train loss : 0.0011378302006050944\n",
            "Train loss : 0.0009199313935823739\n",
            "Train loss : 0.0021413140930235386\n",
            "Train loss : 0.0008536746026948094\n",
            "Train loss : 0.002171840751543641\n",
            "Train loss : 0.002086387248709798\n",
            "Train loss : 0.0019401957979425788\n",
            "Train loss : 0.0014832401648163795\n",
            "Train loss : 0.0013738666893914342\n",
            "Train loss : 0.001816882286220789\n",
            "Train loss : 0.0015420964919030666\n",
            "Train loss : 0.0008205694030039012\n",
            "Train loss : 0.0015195965534076095\n",
            "Train loss : 0.0012972832191735506\n",
            "Train loss : 0.0014408290153369308\n",
            "Train loss : 0.003234121948480606\n",
            "Train loss : 0.0013278914848342538\n",
            "Train loss : 0.0018984652124345303\n",
            "Train loss : 0.0013217246159911156\n",
            "Train loss : 0.0011735984589904547\n",
            "Train loss : 0.002860044362023473\n",
            "Train loss : 0.001411748817190528\n",
            "Train loss : 0.001479923608712852\n",
            "Train loss : 0.0018570753745734692\n",
            "Train loss : 0.003815981326624751\n",
            "Train loss : 0.0016152207972481847\n",
            "Train loss : 0.0014207290951162577\n",
            "Train loss : 0.002348553156480193\n",
            "Train loss : 0.0009488501236774027\n",
            "Train loss : 0.002119689015671611\n",
            "Train loss : 0.002056283177807927\n",
            "Train loss : 0.0021568271331489086\n",
            "Train loss : 0.0015219793422147632\n",
            "Train loss : 0.0017804170493036509\n",
            "Train loss : 0.0009368733153678477\n",
            "Train loss : 0.00144671811722219\n",
            "Train loss : 0.0015849641058593988\n",
            "Train loss : 0.0013933454174548388\n",
            "Train loss : 0.0014242875622585416\n",
            "Train loss : 0.0012547147925943136\n",
            "Train loss : 0.001308630220592022\n",
            "Train loss : 0.001385548384860158\n",
            "Train loss : 0.0018175728619098663\n",
            "Train loss : 0.002096777781844139\n",
            "Train loss : 0.0012016549007967114\n",
            "Train loss : 0.0017420353833585978\n",
            "Train loss : 0.0011941002449020743\n",
            "Train loss : 0.0009795809164643288\n",
            "Train loss : 0.0013988669961690903\n",
            "Train loss : 0.0014564399607479572\n",
            "Train loss : 0.0017047410365194082\n",
            "Train loss : 0.0010761438170447946\n",
            "Train loss : 0.0017083213897421956\n",
            "Train loss : 0.0022978356573730707\n",
            "Train loss : 0.0015115159330889583\n",
            "Train loss : 0.0014236714923754334\n",
            "Train loss : 0.0008890149765647948\n",
            "Train loss : 0.001525005092844367\n",
            "Train loss : 0.0014376587932929397\n",
            "Train loss : 0.0022927282843738794\n",
            "Train loss : 0.0011929103638976812\n",
            "Train loss : 0.0014034874038770795\n",
            "Train loss : 0.001127193565480411\n",
            "Train loss : 0.0010235282825306058\n",
            "Train loss : 0.0014837906928732991\n",
            "Train loss : 0.001077395980246365\n",
            "Train loss : 0.0018930711084976792\n",
            "Train loss : 0.0011336569441482425\n",
            "Train loss : 0.0009828009642660618\n",
            "Train loss : 0.0018059418071061373\n",
            "Train loss : 0.0022022973280400038\n",
            "Train loss : 0.0020599416457116604\n",
            "Train loss : 0.0009124751086346805\n",
            "Train loss : 0.0010589823359623551\n",
            "Train loss : 0.0011891224421560764\n",
            "Train loss : 0.001137447077780962\n",
            "Train loss : 0.00143790477886796\n",
            "Train loss : 0.0009295687777921557\n",
            "Train loss : 0.0018207576358690858\n",
            "Train loss : 0.0008545145974494517\n",
            "Train loss : 0.0017059643287211657\n",
            "Train loss : 0.0016167061403393745\n",
            "Train loss : 0.0012411287752911448\n",
            "Train loss : 0.002161859767511487\n",
            "Train loss : 0.0013195897918194532\n",
            "Train loss : 0.0019398140721023083\n",
            "Train loss : 0.0008666955400258303\n",
            "Train loss : 0.001553247100673616\n",
            "Train loss : 0.0010048284893855453\n",
            "Train loss : 0.0012558192247524858\n",
            "Train loss : 0.0009430330828763545\n",
            "Train loss : 0.0008088068570941687\n",
            "Train loss : 0.0013227763120085\n",
            "Train loss : 0.0010191596811637282\n",
            "Train loss : 0.0007299702847376466\n",
            "Train loss : 0.0009999562753364444\n",
            "Train loss : 0.0014553521759808064\n",
            "Train loss : 0.0013583570253103971\n",
            "Train loss : 0.0012862627627328038\n",
            "Train loss : 0.0011086786398664117\n",
            "Train loss : 0.0015522124012932181\n",
            "Train loss : 0.0008785678073763847\n",
            "Train loss : 0.0013481533387675881\n",
            "Train loss : 0.0015055991243571043\n",
            "Train loss : 0.0009460616274736822\n",
            "Train loss : 0.0010724050225690007\n",
            "Train loss : 0.0009951734682545066\n",
            "Train loss : 0.0013680447591468692\n",
            "Train loss : 0.0016998393693938851\n",
            "Train loss : 0.001724077737890184\n",
            "Train loss : 0.0017522580455988646\n",
            "Train loss : 0.0011199766304343939\n",
            "Train loss : 0.001202164450660348\n",
            "Train loss : 0.0005586739862337708\n",
            "Train loss : 0.0009898830903694034\n",
            "Train loss : 0.0011668946826830506\n",
            "Train loss : 0.000583999149966985\n",
            "Train loss : 0.0013559832004830241\n",
            "Train loss : 0.001035282970406115\n",
            "Train loss : 0.0011783929076045752\n",
            "Train loss : 0.0011405878467485309\n",
            "Train loss : 0.0008324181544594467\n",
            "Train loss : 0.0007480200147256255\n",
            "Train loss : 0.0006854448583908379\n",
            "Train loss : 0.002069368027150631\n",
            "Train loss : 0.0023562763817608356\n",
            "Train loss : 0.0005398524226620793\n",
            "Train loss : 0.0018764793640002608\n",
            "Train loss : 0.0010354231344535947\n",
            "Train loss : 0.0015151695115491748\n",
            "Train loss : 0.0007858995231799781\n",
            "Train loss : 0.00030341357341967523\n",
            "Train loss : 0.002124879276379943\n",
            "Train loss : 0.0009141957270912826\n",
            "Train loss : 0.0014891880564391613\n",
            "Train loss : 0.001839109812863171\n",
            "Train loss : 0.0010277716210111976\n",
            "Train loss : 0.0016895417356863618\n",
            "Train loss : 0.001536528579890728\n",
            "Train loss : 0.0011834601173177361\n",
            "Train loss : 0.0006587956449948251\n",
            "Train loss : 0.0013224211288616061\n",
            "Train loss : 0.0006382258725352585\n",
            "Train loss : 0.0004615384677890688\n",
            "Train loss : 0.0006186917307786644\n",
            "Train loss : 0.001873692381195724\n",
            "Train loss : 0.0013166341232135892\n",
            "Train loss : 0.0006680250517092645\n",
            "Train loss : 0.0012470020446926355\n",
            "Train loss : 0.0008795568719506264\n",
            "Train loss : 0.0008192196255549788\n",
            "Train loss : 0.0011643790639936924\n",
            "Train loss : 0.0011727465316653252\n",
            "Train loss : 0.0020498360972851515\n",
            "Train loss : 0.0015682278899475932\n",
            "Train loss : 0.0013668014435097575\n",
            "Train loss : 0.0015312460018321872\n",
            "Train loss : 0.0006723713013343513\n",
            "Train loss : 0.0012292020255699754\n",
            "Train loss : 0.001354868640191853\n",
            "Train loss : 0.0009519600425846875\n",
            "Train loss : 0.0016269477782770991\n",
            "Train loss : 0.001307363505475223\n",
            "Train loss : 0.0007486703689210117\n",
            "Train loss : 0.0010046897223219275\n",
            "Train loss : 0.0006175608141347766\n",
            "Train loss : 0.00145945162512362\n",
            "Train loss : 0.0014902249677106738\n",
            "Train loss : 0.0003466212365310639\n",
            "Train loss : 0.0008345003589056432\n",
            "Train loss : 0.0011254810960963368\n",
            "Train loss : 0.002219969406723976\n",
            "Train loss : 0.0007152912439778447\n",
            "Train loss : 0.0010655629448592663\n",
            "Train loss : 0.0008369380375370383\n",
            "Train loss : 0.0007923805387690663\n",
            "Train loss : 0.00096593500347808\n",
            "Looked at 12800/60000 samples.\n",
            "Train loss : 0.0009499049629084766\n",
            "Train loss : 0.0010832971893250942\n",
            "Train loss : 0.0006716741481795907\n",
            "Train loss : 0.0007594907656311989\n",
            "Train loss : 0.0010295407846570015\n",
            "Train loss : 0.0009274187032133341\n",
            "Train loss : 0.0007948616403155029\n",
            "Train loss : 0.000935352232772857\n",
            "Train loss : 0.0009052378009073436\n",
            "Train loss : 0.0008168126223608851\n",
            "Train loss : 0.0008374048629775643\n",
            "Train loss : 0.0007872584974393249\n",
            "Train loss : 0.0006219184142537415\n",
            "Train loss : 0.0019151209853589535\n",
            "Train loss : 0.0009486177004873753\n",
            "Train loss : 0.0007871018024161458\n",
            "Train loss : 0.001104332972317934\n",
            "Train loss : 0.0015717007918283343\n",
            "Train loss : 0.0006266327109187841\n",
            "Train loss : 0.00036835530772805214\n",
            "Train loss : 0.0005996894324198365\n",
            "Train loss : 0.0012977891601622105\n",
            "Train loss : 0.0006244069081731141\n",
            "Train loss : 0.0007039593765512109\n",
            "Train loss : 0.0006999592878855765\n",
            "Train loss : 0.000296877435175702\n",
            "Train loss : 0.000692245434038341\n",
            "Train loss : 0.0009695581393316388\n",
            "Train loss : 0.0007504739332944155\n",
            "Train loss : 0.0008172717643901706\n",
            "Train loss : 0.0011685187928378582\n",
            "Train loss : 0.00047173071652650833\n",
            "Train loss : 0.00043279945384711027\n",
            "Train loss : 0.0006442330195568502\n",
            "Train loss : 0.0008982567233033478\n",
            "Train loss : 0.0010183022823184729\n",
            "Train loss : 0.0014308695681393147\n",
            "Train loss : 0.0010118631180375814\n",
            "Train loss : 0.0008900651591829956\n",
            "Train loss : 0.0008886507712304592\n",
            "Train loss : 0.0009129789541475475\n",
            "Train loss : 0.00124239397700876\n",
            "Train loss : 0.0009776869555935264\n",
            "Train loss : 0.0015131828840821981\n",
            "Train loss : 0.0021597356535494328\n",
            "Train loss : 0.0013924973318353295\n",
            "Train loss : 0.0007793010445311666\n",
            "Train loss : 0.0007608350133523345\n",
            "Train loss : 0.0016631359467282891\n",
            "Train loss : 0.0008972791256383061\n",
            "Train loss : 0.0007757757557556033\n",
            "Train loss : 0.0009562612394802272\n",
            "Train loss : 0.001305373734794557\n",
            "Train loss : 0.0006039207219146192\n",
            "Train loss : 0.0006062050815671682\n",
            "Train loss : 0.000797880464233458\n",
            "Train loss : 0.0005685854703187943\n",
            "Train loss : 0.0008476942894048989\n",
            "Train loss : 0.0008891046745702624\n",
            "Train loss : 0.0011300055775791407\n",
            "Train loss : 0.0012713924515992403\n",
            "Train loss : 0.0006217695772647858\n",
            "Train loss : 0.00043989220284856856\n",
            "Train loss : 0.0006847661570645869\n",
            "Train loss : 0.00091414904454723\n",
            "Train loss : 0.0007789181545376778\n",
            "Train loss : 0.0007790285744704306\n",
            "Train loss : 0.0004826377844437957\n",
            "Train loss : 0.0008131091599352658\n",
            "Train loss : 0.0011486297007650137\n",
            "Train loss : 0.0005938287940807641\n",
            "Train loss : 0.0008516082307323813\n",
            "Train loss : 0.0005318405455909669\n",
            "Train loss : 0.0009137672022916377\n",
            "Train loss : 0.0006387417670339346\n",
            "Train loss : 0.0014384493697434664\n",
            "Train loss : 0.0005812757299281657\n",
            "Train loss : 0.0011742253554984927\n",
            "Train loss : 0.0011171094374731183\n",
            "Train loss : 0.0008428466389887035\n",
            "Train loss : 0.0007472858997061849\n",
            "Train loss : 0.000629193615168333\n",
            "Train loss : 0.0005156695260666311\n",
            "Train loss : 0.0012443945743143559\n",
            "Train loss : 0.0009511641692370176\n",
            "Train loss : 0.0005940566188655794\n",
            "Train loss : 0.001227817963808775\n",
            "Train loss : 0.0015624642837792635\n",
            "Train loss : 0.000593257718719542\n",
            "Train loss : 0.0008995154057629406\n",
            "Train loss : 0.0008837719797156751\n",
            "Train loss : 0.0006478042341768742\n",
            "Train loss : 0.0008259591995738447\n",
            "Train loss : 0.0005378699279390275\n",
            "Train loss : 0.0010240800911560655\n",
            "Train loss : 0.0008184222388081253\n",
            "Train loss : 0.0007372476393356919\n",
            "Train loss : 0.0005240050377324224\n",
            "Train loss : 0.0003307324950583279\n",
            "Train loss : 0.0009808724280446768\n",
            "Train loss : 0.0006885132752358913\n",
            "Train loss : 0.0006842773873358965\n",
            "Train loss : 0.0004693368391599506\n",
            "Train loss : 0.0016138179926201701\n",
            "Train loss : 0.0005212864489294589\n",
            "Train loss : 0.0017469532322138548\n",
            "Train loss : 0.0007196579244919121\n",
            "Train loss : 0.0003721242246683687\n",
            "Train loss : 0.0007265390013344586\n",
            "Train loss : 0.0014780687633901834\n",
            "Train loss : 0.0010576104978099465\n",
            "Train loss : 0.0008824394317343831\n",
            "Train loss : 0.00045774420141242445\n",
            "Train loss : 0.00041440760833211243\n",
            "Train loss : 0.0006658808561041951\n",
            "Train loss : 0.0018516967538744211\n",
            "Train loss : 0.0007235214579850435\n",
            "Train loss : 0.0007070789579302073\n",
            "Train loss : 0.0015183176146820188\n",
            "Train loss : 0.0010744165629148483\n",
            "Train loss : 0.0008597484556958079\n",
            "Train loss : 0.0003921240568161011\n",
            "Train loss : 0.0006530605023726821\n",
            "Train loss : 0.0015035206452012062\n",
            "Train loss : 0.0005559409037232399\n",
            "Train loss : 0.0011433541076257825\n",
            "Train loss : 0.0006119636818766594\n",
            "Train loss : 0.0005551985814236104\n",
            "Train loss : 0.000761112489271909\n",
            "Train loss : 0.0006928101647645235\n",
            "Train loss : 0.001053942134603858\n",
            "Train loss : 0.0007246118620969355\n",
            "Train loss : 0.0009401167626492679\n",
            "Train loss : 0.0010184576967731118\n",
            "Train loss : 0.00045860791578888893\n",
            "Train loss : 0.0005303799407556653\n",
            "Train loss : 0.0009800626430660486\n",
            "Train loss : 0.00051931879715994\n",
            "Train loss : 0.0008642304455861449\n",
            "Train loss : 0.0008199968142434955\n",
            "Train loss : 0.0007036353345029056\n",
            "Train loss : 0.000605378532782197\n",
            "Train loss : 0.0006562607013620436\n",
            "Train loss : 0.0012451435904949903\n",
            "Train loss : 0.001018173643387854\n",
            "Train loss : 0.0007336207781918347\n",
            "Train loss : 0.0009315618663094938\n",
            "Train loss : 0.0007678953115828335\n",
            "Train loss : 0.0013693623477593064\n",
            "Train loss : 0.0008009762386791408\n",
            "Train loss : 0.0006881534936837852\n",
            "Train loss : 0.0003834521339740604\n",
            "Train loss : 0.0009141778573393822\n",
            "Train loss : 0.000509269128087908\n",
            "Train loss : 0.0004575163475237787\n",
            "Train loss : 0.0005641643074341118\n",
            "Train loss : 0.0005225875647738576\n",
            "Train loss : 0.0009242512169294059\n",
            "Train loss : 0.0005784752429462969\n",
            "Train loss : 0.0016120730433613062\n",
            "Train loss : 0.0005154160317033529\n",
            "Train loss : 0.0006267749704420567\n",
            "Train loss : 0.0006311162142083049\n",
            "Train loss : 0.000841984641738236\n",
            "Train loss : 0.0008458516094833612\n",
            "Train loss : 0.0005432645557448268\n",
            "Train loss : 0.0007148750009946525\n",
            "Train loss : 0.00042208426748402417\n",
            "Train loss : 0.00042557617416605353\n",
            "Train loss : 0.0011236203135922551\n",
            "Train loss : 0.0009678474743850529\n",
            "Train loss : 0.0006403828738257289\n",
            "Train loss : 0.0008447582949884236\n",
            "Train loss : 0.0007088013808242977\n",
            "Train loss : 0.000861925829667598\n",
            "Train loss : 0.0006410345667973161\n",
            "Train loss : 0.0016518966294825077\n",
            "Train loss : 0.0008374786120839417\n",
            "Train loss : 0.000827510841190815\n",
            "Train loss : 0.0008893687045201659\n",
            "Train loss : 0.0009324838756583631\n",
            "Train loss : 0.000558731728233397\n",
            "Train loss : 0.0007717018597759306\n",
            "Train loss : 0.00028334377566352487\n",
            "Train loss : 0.001054667867720127\n",
            "Train loss : 0.0010489841224625707\n",
            "Train loss : 0.001300534582696855\n",
            "Train loss : 0.0008928932948037982\n",
            "Train loss : 0.0007597350631840527\n",
            "Train loss : 0.0005690287216566503\n",
            "Train loss : 0.00045764335663989186\n",
            "Train loss : 0.0014350574929267168\n",
            "Train loss : 0.0004842930065933615\n",
            "Train loss : 0.0005306311068125069\n",
            "Train loss : 0.00046328245662152767\n",
            "Train loss : 0.0009871175279840827\n",
            "Train loss : 0.0005588355707004666\n",
            "Train loss : 0.0014803885715082288\n",
            "Train loss : 0.0010746193584054708\n",
            "Train loss : 0.0006000919966027141\n",
            "Train loss : 0.0006139828474260867\n",
            "Train loss : 0.0007954579778015614\n",
            "Train loss : 0.0012358701787889004\n",
            "Train loss : 0.0008967345347627997\n",
            "Train loss : 0.0010533151216804981\n",
            "Train loss : 0.0010664775036275387\n",
            "Train loss : 0.0014728017849847674\n",
            "Train loss : 0.0009575891308486462\n",
            "Train loss : 0.0012722766259685159\n",
            "Train loss : 0.0007444123621098697\n",
            "Train loss : 0.0004879834596067667\n",
            "Train loss : 0.0004582369583658874\n",
            "Train loss : 0.00043510246905498207\n",
            "Train loss : 0.0006506124045699835\n",
            "Train loss : 0.0005194832338020205\n",
            "Train loss : 0.0007142999675124884\n",
            "Train loss : 0.0002854754857253283\n",
            "Train loss : 0.0005249198293313384\n",
            "Train loss : 0.0009676361223682761\n",
            "Train loss : 0.0007598457741551101\n",
            "Train loss : 0.000664323044475168\n",
            "Train loss : 0.001043060328811407\n",
            "Train loss : 0.0008626110502518713\n",
            "Train loss : 0.0010513501474633813\n",
            "Train loss : 0.0005110978381708264\n",
            "Train loss : 0.0006236907211132348\n",
            "Train loss : 0.0006695000338368118\n",
            "Train loss : 0.000724026293028146\n",
            "Train loss : 0.00048577680718153715\n",
            "Train loss : 0.0010983855463564396\n",
            "Train loss : 0.0007577131618745625\n",
            "Train loss : 0.0003571506531443447\n",
            "Train loss : 0.00047600961988791823\n",
            "Train loss : 0.0007111788727343082\n",
            "Train loss : 0.0012354127829894423\n",
            "Train loss : 0.0009741886169649661\n",
            "Train loss : 0.0006224745884537697\n",
            "Train loss : 0.000697594543453306\n",
            "Train loss : 0.0008453932241536677\n",
            "Train loss : 0.000508764642290771\n",
            "Train loss : 0.0004859054461121559\n",
            "Train loss : 0.0005585389444604516\n",
            "Train loss : 0.000705406884662807\n",
            "Train loss : 0.0004592815530486405\n",
            "Train loss : 0.00074990518623963\n",
            "Train loss : 0.0008318167529068887\n",
            "Train loss : 0.00040371823706664145\n",
            "Train loss : 0.0002636149583850056\n",
            "Train loss : 0.0007320911390706897\n",
            "Train loss : 0.00043716724030673504\n",
            "Train loss : 0.00048758252523839474\n",
            "Train loss : 0.0006832462386228144\n",
            "Train loss : 0.0005326068494468927\n",
            "Train loss : 0.0006281817913986742\n",
            "Train loss : 0.0004174990172032267\n",
            "Train loss : 0.0006663429667241871\n",
            "Train loss : 0.0006307358271442354\n",
            "Train loss : 0.000564992253202945\n",
            "Train loss : 0.0006497431895695627\n",
            "Train loss : 0.0006499464507214725\n",
            "Train loss : 0.00034983200021088123\n",
            "Train loss : 0.0005491427145898342\n",
            "Train loss : 0.001074740313924849\n",
            "Train loss : 0.0005242343177087605\n",
            "Train loss : 0.0007367115467786789\n",
            "Train loss : 0.0006797678070142865\n",
            "Train loss : 0.00039929785998538136\n",
            "Train loss : 0.0009979134192690253\n",
            "Train loss : 0.0010058560874313116\n",
            "Train loss : 0.0003618685877881944\n",
            "Train loss : 0.00038251475780270994\n",
            "Train loss : 0.0004013815487269312\n",
            "Train loss : 0.000430428481195122\n",
            "Train loss : 0.0004964683321304619\n",
            "Train loss : 0.0006822309806011617\n",
            "Train loss : 0.000381978607038036\n",
            "Train loss : 0.0006839660345576704\n",
            "Train loss : 0.0008727000094950199\n",
            "Train loss : 0.0004923754022456706\n",
            "Train loss : 0.0003505725762806833\n",
            "Train loss : 0.0004117885255254805\n",
            "Train loss : 0.0006670248694717884\n",
            "Train loss : 0.0004198392271064222\n",
            "Train loss : 0.000567332492209971\n",
            "Train loss : 0.0007917791372165084\n",
            "Train loss : 0.000504780444316566\n",
            "Train loss : 0.0005082371644675732\n",
            "Train loss : 0.0009987151715904474\n",
            "Train loss : 0.000784652482252568\n",
            "Train loss : 0.0008112380746752024\n",
            "Train loss : 0.0006009590579196811\n",
            "Train loss : 0.0011614818358793855\n",
            "Train loss : 0.0011678938753902912\n",
            "Train loss : 0.0007002577185630798\n",
            "Train loss : 0.0003943668561987579\n",
            "Train loss : 0.0005167640629224479\n",
            "Train loss : 0.0006832327926531434\n",
            "Train loss : 0.0007049917476251721\n",
            "Train loss : 0.00027635664446279407\n",
            "Train loss : 0.0005182402092032135\n",
            "Train loss : 0.0005194006953388453\n",
            "Train loss : 0.0005079839611425996\n",
            "Train loss : 0.0007617191877216101\n",
            "Train loss : 0.00030914117814972997\n",
            "Train loss : 0.0004903030348941684\n",
            "Train loss : 0.0012801405973732471\n",
            "Train loss : 0.0002451476757414639\n",
            "Train loss : 0.0004793920670635998\n",
            "Train loss : 0.0004041437932755798\n",
            "Train loss : 0.0006127909873612225\n",
            "Train loss : 0.0005333655280992389\n",
            "Train loss : 0.0004706391482613981\n",
            "Train loss : 0.00026406574761494994\n",
            "Train loss : 0.0005146796465851367\n",
            "Train loss : 0.0007927898550406098\n",
            "Train loss : 0.0004537872737273574\n",
            "Train loss : 0.0005814601317979395\n",
            "Train loss : 0.0007847048691473901\n",
            "Train loss : 0.0008442577673122287\n",
            "Train loss : 0.0005451972829177976\n",
            "Train loss : 0.0003626146644819528\n",
            "Train loss : 0.0004935716860927641\n",
            "Train loss : 0.0005036892252974212\n",
            "Train loss : 0.0004033720470033586\n",
            "Train loss : 0.0007064799428917468\n",
            "Train loss : 0.0003989066171925515\n",
            "Train loss : 0.0006611326825805008\n",
            "Train loss : 0.0004469959530979395\n",
            "Train loss : 0.0004472395230550319\n",
            "Train loss : 0.0004550962767098099\n",
            "Train loss : 0.0007707730401307344\n",
            "Train loss : 0.0006513544940389693\n",
            "Train loss : 0.00036577414721250534\n",
            "Train loss : 0.000825865485239774\n",
            "Train loss : 0.00047988470760174096\n",
            "Train loss : 0.00025174629990942776\n",
            "Train loss : 0.0005614178371615708\n",
            "Train loss : 0.0006246650009416044\n",
            "Train loss : 0.0007127904682420194\n",
            "Train loss : 0.0004098614735994488\n",
            "Train loss : 0.0005158393760211766\n",
            "Train loss : 0.0005284565268084407\n",
            "Train loss : 0.0006807207246311009\n",
            "Train loss : 0.0008585267933085561\n",
            "Train loss : 0.0006467430503107607\n",
            "Train loss : 0.0006372174248099327\n",
            "Train loss : 0.00035081960959360003\n",
            "Train loss : 0.0006402311264537275\n",
            "Train loss : 0.0007746436749584973\n",
            "Train loss : 0.000778847374022007\n",
            "Train loss : 0.0006886383634991944\n",
            "Train loss : 0.0004701928119175136\n",
            "Train loss : 0.0004256164247635752\n",
            "Train loss : 0.0007541022496297956\n",
            "Train loss : 0.0004773133550770581\n",
            "Train loss : 0.0007869374821893871\n",
            "Train loss : 0.0003634498280007392\n",
            "Train loss : 0.0006017308915033937\n",
            "Train loss : 0.001156967948190868\n",
            "Train loss : 0.0007802066975273192\n",
            "Train loss : 0.0006889373762533069\n",
            "Train loss : 0.0006066127098165452\n",
            "Train loss : 0.0005937889218330383\n",
            "Train loss : 0.0005417178035713732\n",
            "Train loss : 0.0004462387878447771\n",
            "Train loss : 0.0004814727290067822\n",
            "Train loss : 0.0001930440339492634\n",
            "Train loss : 0.000635493837762624\n",
            "Train loss : 0.0004904543166048825\n",
            "Train loss : 0.000629536691121757\n",
            "Train loss : 0.0010319033171981573\n",
            "Train loss : 0.0005509552429430187\n",
            "Train loss : 0.0006865702453069389\n",
            "Train loss : 0.0004704830644186586\n",
            "Train loss : 0.0005702183116227388\n",
            "Train loss : 0.0007103138486854732\n",
            "Train loss : 0.0003763107524719089\n",
            "Train loss : 0.0006730906898155808\n",
            "Train loss : 0.0005030442844145\n",
            "Train loss : 0.0005916822701692581\n",
            "Train loss : 0.0007393797277472913\n",
            "Train loss : 0.0006871095974929631\n",
            "Train loss : 0.00045935448724776506\n",
            "Train loss : 0.0010121758095920086\n",
            "Train loss : 0.0006640056963078678\n",
            "Train loss : 0.0006528545054607093\n",
            "Train loss : 0.0003988823445979506\n",
            "Train loss : 0.0008038159576244652\n",
            "Train loss : 0.0005697656306438148\n",
            "Train loss : 0.0004600310930982232\n",
            "Train loss : 0.00046498895972035825\n",
            "Train loss : 0.00028075871523469687\n",
            "Train loss : 0.00023329874966293573\n",
            "Train loss : 0.0005024622660130262\n",
            "Train loss : 0.00048583015450276434\n",
            "Train loss : 0.0003194884629920125\n",
            "Train loss : 0.000514698913320899\n",
            "Train loss : 0.0009188369731418788\n",
            "Train loss : 0.0005524045554921031\n",
            "Train loss : 0.0003529530658852309\n",
            "Looked at 25600/60000 samples.\n",
            "Train loss : 0.0006007464835420251\n",
            "Train loss : 0.0004229684127494693\n",
            "Train loss : 0.00038296933053061366\n",
            "Train loss : 0.0004338613071013242\n",
            "Train loss : 0.000599937979131937\n",
            "Train loss : 0.00026233421522192657\n",
            "Train loss : 0.0004451817076187581\n",
            "Train loss : 0.00044815748697146773\n",
            "Train loss : 0.0005591725348494947\n",
            "Train loss : 0.0007211365737020969\n",
            "Train loss : 0.0009977574227377772\n",
            "Train loss : 0.00047421370982192457\n",
            "Train loss : 0.0004563945112749934\n",
            "Train loss : 0.00038939446676522493\n",
            "Train loss : 0.0004063428204972297\n",
            "Train loss : 0.00044923764653503895\n",
            "Train loss : 0.00029909872682765126\n",
            "Train loss : 0.0006483872421085835\n",
            "Train loss : 0.00022178566723596305\n",
            "Train loss : 0.0004555433988571167\n",
            "Train loss : 0.0006693499744869769\n",
            "Train loss : 0.0006663721869699657\n",
            "Train loss : 0.000516011961735785\n",
            "Train loss : 0.00040961470222100616\n",
            "Train loss : 0.0006231583538465202\n",
            "Train loss : 0.0004506344557739794\n",
            "Train loss : 0.00016961153596639633\n",
            "Train loss : 0.00033852827618829906\n",
            "Train loss : 0.0006421981379389763\n",
            "Train loss : 0.000691177905537188\n",
            "Train loss : 0.000751894258428365\n",
            "Train loss : 0.0002958184340968728\n",
            "Train loss : 0.0004125763662159443\n",
            "Train loss : 0.0004920352948829532\n",
            "Train loss : 0.0009105086792260408\n",
            "Train loss : 0.0003904604527633637\n",
            "Train loss : 0.0005110680358484387\n",
            "Train loss : 0.00044080096995458007\n",
            "Train loss : 0.00045548504567705095\n",
            "Train loss : 0.00030055982642807066\n",
            "Train loss : 0.000700006727129221\n",
            "Train loss : 0.0003572448040358722\n",
            "Train loss : 0.0006689925794489682\n",
            "Train loss : 0.00032284538610838354\n",
            "Train loss : 0.0004974286421202123\n",
            "Train loss : 0.0003193141601514071\n",
            "Train loss : 0.0007195421494543552\n",
            "Train loss : 0.0005365651450119913\n",
            "Train loss : 0.0005084069562144578\n",
            "Train loss : 0.0006135944277048111\n",
            "Train loss : 0.000339075515512377\n",
            "Train loss : 0.0005717517924495041\n",
            "Train loss : 0.0006730848108418286\n",
            "Train loss : 0.00042754944297485054\n",
            "Train loss : 0.00031826531630940735\n",
            "Train loss : 0.0003831761423498392\n",
            "Train loss : 0.000973574526142329\n",
            "Train loss : 0.00035669890348799527\n",
            "Train loss : 0.0006896337727084756\n",
            "Train loss : 0.0006650089053437114\n",
            "Train loss : 0.00027607273659668863\n",
            "Train loss : 0.0006363778957165778\n",
            "Train loss : 0.000395143375499174\n",
            "Train loss : 0.0010180848184973001\n",
            "Train loss : 0.00045151132508181036\n",
            "Train loss : 0.00035712626413442194\n",
            "Train loss : 0.0007492522709071636\n",
            "Train loss : 0.0003445427864789963\n",
            "Train loss : 0.0005528701585717499\n",
            "Train loss : 0.0006629268173128366\n",
            "Train loss : 0.0004113730101380497\n",
            "Train loss : 0.0005887220613658428\n",
            "Train loss : 0.0005317542236298323\n",
            "Train loss : 0.0004191632615402341\n",
            "Train loss : 0.00034231762401759624\n",
            "Train loss : 0.0003309772873762995\n",
            "Train loss : 0.00040429324144497514\n",
            "Train loss : 0.0005553621449507773\n",
            "Train loss : 0.0004229697515256703\n",
            "Train loss : 0.0006470595835708082\n",
            "Train loss : 0.0005128320190124214\n",
            "Train loss : 0.00031905219657346606\n",
            "Train loss : 0.0004929821006953716\n",
            "Train loss : 0.0007322899764403701\n",
            "Train loss : 0.00035810613189823925\n",
            "Train loss : 0.00028416342684067786\n",
            "Train loss : 0.00039756292244419456\n",
            "Train loss : 0.00031372622470371425\n",
            "Train loss : 0.0005574091919697821\n",
            "Train loss : 0.00035745714558288455\n",
            "Train loss : 0.00036834145430475473\n",
            "Train loss : 0.00028566591208800673\n",
            "Train loss : 0.0003156156453769654\n",
            "Train loss : 0.0006242352537810802\n",
            "Train loss : 0.000472110667033121\n",
            "Train loss : 0.0007476619211956859\n",
            "Train loss : 0.00046444006147794425\n",
            "Train loss : 0.0009180568740703166\n",
            "Train loss : 0.0006668046698905528\n",
            "Train loss : 0.0003314800269436091\n",
            "Train loss : 0.0004422127385623753\n",
            "Train loss : 0.00031333419610746205\n",
            "Train loss : 0.0007698761182837188\n",
            "Train loss : 0.0004920100327581167\n",
            "Train loss : 0.00039900161209516227\n",
            "Train loss : 0.0005014933412894607\n",
            "Train loss : 0.00032048849971033633\n",
            "Train loss : 0.00037980228080414236\n",
            "Train loss : 0.0005680739413946867\n",
            "Train loss : 0.0005442397086881101\n",
            "Train loss : 0.0003237909695599228\n",
            "Train loss : 0.0007760663866065443\n",
            "Train loss : 0.0006134096765890718\n",
            "Train loss : 0.0008325978997163475\n",
            "Train loss : 0.0003767533926293254\n",
            "Train loss : 0.0001980057859327644\n",
            "Train loss : 0.00038636766839772463\n",
            "Train loss : 0.000625459011644125\n",
            "Train loss : 0.0005318002076819539\n",
            "Train loss : 0.00025824285694397986\n",
            "Train loss : 0.0007371532265096903\n",
            "Train loss : 0.0004020419146399945\n",
            "Train loss : 0.0006455139955505729\n",
            "Train loss : 0.0005090768099762499\n",
            "Train loss : 0.0001802251790650189\n",
            "Train loss : 0.0002701517951209098\n",
            "Train loss : 0.000654621166177094\n",
            "Train loss : 0.0004471438587643206\n",
            "Train loss : 0.00035460101207718253\n",
            "Train loss : 0.0005067046149633825\n",
            "Train loss : 0.0007170045282691717\n",
            "Train loss : 0.00035183323780074716\n",
            "Train loss : 0.0002811465528793633\n",
            "Train loss : 0.0004638806567527354\n",
            "Train loss : 0.0005604572361335158\n",
            "Train loss : 0.0004013095167465508\n",
            "Train loss : 0.0004540523805189878\n",
            "Train loss : 0.0002668578235898167\n",
            "Train loss : 0.000202914176043123\n",
            "Train loss : 0.0005704190698452294\n",
            "Train loss : 0.00032786710653454065\n",
            "Train loss : 0.000301496620522812\n",
            "Train loss : 0.000674137962050736\n",
            "Train loss : 0.00037462348700501025\n",
            "Train loss : 0.0005309616099111736\n",
            "Train loss : 0.0007998886867426336\n",
            "Train loss : 0.0004394185671117157\n",
            "Train loss : 0.00031866462086327374\n",
            "Train loss : 0.0009461005101911724\n",
            "Train loss : 0.00048471757327206433\n",
            "Train loss : 0.0004976209602318704\n",
            "Train loss : 0.0003488639777060598\n",
            "Train loss : 0.0005388064892031252\n",
            "Train loss : 0.0005594868562184274\n",
            "Train loss : 0.0005050816689617932\n",
            "Train loss : 0.0004215965454932302\n",
            "Train loss : 0.000383500155294314\n",
            "Train loss : 0.00021118191943969578\n",
            "Train loss : 0.0005402821698226035\n",
            "Train loss : 0.00043504920904524624\n",
            "Train loss : 0.00037799033452756703\n",
            "Train loss : 0.00042359536746516824\n",
            "Train loss : 0.0003469551738817245\n",
            "Train loss : 0.0004454297013580799\n",
            "Train loss : 0.00034999920171685517\n",
            "Train loss : 0.0005261388141661882\n",
            "Train loss : 0.0002730405831243843\n",
            "Train loss : 0.00044417017488740385\n",
            "Train loss : 0.00041667016921564937\n",
            "Train loss : 0.00029898903449065983\n",
            "Train loss : 0.00022022316989023238\n",
            "Train loss : 0.00019804017210844904\n",
            "Train loss : 0.0005862191319465637\n",
            "Train loss : 0.0003248238645028323\n",
            "Train loss : 0.00036775873741135\n",
            "Train loss : 0.0004476896720007062\n",
            "Train loss : 0.00020368561672512442\n",
            "Train loss : 0.0005122518050484359\n",
            "Train loss : 0.0004021075728815049\n",
            "Train loss : 0.0005444854032248259\n",
            "Train loss : 0.00029030730365775526\n",
            "Train loss : 0.0005920034600421786\n",
            "Train loss : 0.0004647250461857766\n",
            "Train loss : 0.0005416895728558302\n",
            "Train loss : 0.000682688201777637\n",
            "Train loss : 0.00025089786504395306\n",
            "Train loss : 0.0004284152528271079\n",
            "Train loss : 0.00048782050726003945\n",
            "Train loss : 0.0003637016925495118\n",
            "Train loss : 0.0002899157116189599\n",
            "Train loss : 0.00014217333227861673\n",
            "Train loss : 0.00030531108495779335\n",
            "Train loss : 0.00028381714946590364\n",
            "Train loss : 0.00024153164122253656\n",
            "Train loss : 0.000743187905754894\n",
            "Train loss : 0.00041337849688716233\n",
            "Train loss : 0.0005347694386728108\n",
            "Train loss : 0.0005936762900091708\n",
            "Train loss : 0.0004221971321385354\n",
            "Train loss : 0.0008454974158667028\n",
            "Train loss : 0.00047020078636705875\n",
            "Train loss : 0.00015278268256224692\n",
            "Train loss : 0.0004234748485032469\n",
            "Train loss : 0.0005768411210738122\n",
            "Train loss : 0.0002343322557862848\n",
            "Train loss : 0.0004352251999080181\n",
            "Train loss : 0.000506308046169579\n",
            "Train loss : 0.00041273096576333046\n",
            "Train loss : 0.000631259405054152\n",
            "Train loss : 0.00036683454527519643\n",
            "Train loss : 0.0005888763698749244\n",
            "Train loss : 0.0002694691065698862\n",
            "Train loss : 0.00029220309806987643\n",
            "Train loss : 0.00025461780023761094\n",
            "Train loss : 0.0004366156936157495\n",
            "Train loss : 0.00046787632163614035\n",
            "Train loss : 0.0005095575470477343\n",
            "Train loss : 0.0005230450187809765\n",
            "Train loss : 0.00026828687987290323\n",
            "Train loss : 0.0003723346453625709\n",
            "Train loss : 0.0007180757238529623\n",
            "Train loss : 0.00039515344542451203\n",
            "Train loss : 0.0004581055254675448\n",
            "Train loss : 0.0003979802713729441\n",
            "Train loss : 0.0003991082194261253\n",
            "Train loss : 0.0003890806110575795\n",
            "Train loss : 0.00029543490381911397\n",
            "Train loss : 0.00040353002259507775\n",
            "Train loss : 0.0004654273798223585\n",
            "Train loss : 0.00024785043206065893\n",
            "Train loss : 0.000243232017965056\n",
            "Train loss : 0.00033552112290635705\n",
            "Train loss : 0.00025119143538177013\n",
            "Train loss : 0.0007447613752447069\n",
            "Train loss : 0.0003493386902846396\n",
            "Train loss : 0.0006336492951959372\n",
            "Train loss : 0.0003408447664696723\n",
            "Train loss : 0.00041802250780165195\n",
            "Train loss : 0.00033000894472934306\n",
            "Train loss : 0.0007156896172091365\n",
            "Train loss : 0.0003272745234426111\n",
            "Train loss : 0.00015054878895170987\n",
            "Train loss : 0.0007233623182401061\n",
            "Train loss : 0.000298451806884259\n",
            "Train loss : 0.00022929585247766227\n",
            "Train loss : 0.00025878800079226494\n",
            "Train loss : 0.00047525326954200864\n",
            "Train loss : 0.0003406610921956599\n",
            "Train loss : 0.00036684886435978115\n",
            "Train loss : 0.00039508010377176106\n",
            "Train loss : 0.0005333708832040429\n",
            "Train loss : 0.0005482590640895069\n",
            "Train loss : 0.0007830934482626617\n",
            "Train loss : 0.00047734248801134527\n",
            "Train loss : 0.0006008793716318905\n",
            "Train loss : 0.00035684453905560076\n",
            "Train loss : 0.00038452347507700324\n",
            "Train loss : 0.0005427274736575782\n",
            "Train loss : 0.0003010566288139671\n",
            "Train loss : 0.0002886494330596179\n",
            "Train loss : 0.00048184182378463447\n",
            "Train loss : 0.0004019589687231928\n",
            "Train loss : 0.0005276638548821211\n",
            "Train loss : 0.00044774854904972017\n",
            "Train loss : 0.00022570131113752723\n",
            "Train loss : 0.0002559772692620754\n",
            "Train loss : 0.0005040555261075497\n",
            "Train loss : 0.0005931579507887363\n",
            "Train loss : 0.0005893462803214788\n",
            "Train loss : 0.0003908171784132719\n",
            "Train loss : 0.0003285664424765855\n",
            "Train loss : 0.0002122166333720088\n",
            "Train loss : 0.0004873497528024018\n",
            "Train loss : 0.00043132214341312647\n",
            "Train loss : 0.0004039242339786142\n",
            "Train loss : 0.0003670297737699002\n",
            "Train loss : 0.0003880821168422699\n",
            "Train loss : 0.00019317673286423087\n",
            "Train loss : 0.00036393158370628953\n",
            "Train loss : 0.00026602804427966475\n",
            "Train loss : 0.00037246185820549726\n",
            "Train loss : 0.00035770240356214345\n",
            "Train loss : 0.0004314941761549562\n",
            "Train loss : 0.0006111837574280798\n",
            "Train loss : 0.0006677852943539619\n",
            "Train loss : 0.0004728128551505506\n",
            "Train loss : 0.00031602606759406626\n",
            "Train loss : 0.0004026357491966337\n",
            "Train loss : 0.0002733890141826123\n",
            "Train loss : 0.00045964139280840755\n",
            "Train loss : 0.00023709845845587552\n",
            "Train loss : 0.0001418005849700421\n",
            "Train loss : 0.00023046793648973107\n",
            "Train loss : 0.00041952304309234023\n",
            "Train loss : 0.00036167650250718\n",
            "Train loss : 0.00046302727423608303\n",
            "Train loss : 0.0002867310249712318\n",
            "Train loss : 0.0003609642735682428\n",
            "Train loss : 0.0006624130182899535\n",
            "Train loss : 0.00044559751404449344\n",
            "Train loss : 0.0004520131042227149\n",
            "Train loss : 0.000316195422783494\n",
            "Train loss : 0.0003863674064632505\n",
            "Train loss : 0.00024580402532592416\n",
            "Train loss : 0.00029797261231578887\n",
            "Train loss : 0.0006404681480489671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_results = eval_model(model=model_1,data_loader=test_dataloader,loss_fn=loss,accuracy_fn=accuracy_fn)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "gmUNfJtWI5he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_results = eval_model(model=model_0,data_loader=test_dataloader,loss_fn=loss,accuracy_fn=accuracy_fn)\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "eoa_oZQ9IsdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FaishonMNISTModelV2(nn.Module):\n",
        "  def __init__(self,input_shape:int,hidden_units:int,output_shape:int):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "    )\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,\n",
        "                  kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,\n",
        "                  kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "    )"
      ],
      "metadata": {
        "id": "zAdtcTLuJ-gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.classifier= nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(in_features=hidden_units*0,out_features=output_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = conv_block_1(x)\n",
        "      print(x.shape)\n",
        "      x = conv_block_2(x)\n",
        "      print(x.shape)\n",
        "      x = classifier(x)\n",
        "      print(x.shape)\n",
        "      return x\n",
        ")"
      ],
      "metadata": {
        "id": "P9_hdZoSMbaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNPMIVNPPSBL",
        "outputId": "a77dace9-453d-48ac-ce3a-5348a0f51310"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ogVgcJ24PaeM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}